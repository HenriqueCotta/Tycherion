Tycherion
├── >>> .env <<<
├── >>> .env.example <<<
├── .git (ignored)
├── .gitignore (ignored)
├── .venv (ignored)
├── .vscode
│   └── >>> settings.json <<<
├── README.md (ignored)
├── configs
│   └── >>> demo.yaml <<<
├── pj_output.txt (ignored)
├── >>> pyproject.toml <<<
├── >>> requirements.txt <<<
├── scripts
│   └── >>> run_demo.py <<<
├── src
│   └── tycherion
│       ├── adapters
│       │   ├── mt5
│       │   │   ├── __pycache__ (ignored)
│       │   │   ├── >>> account_mt5.py <<<
│       │   │   ├── >>> market_data_mt5.py <<<
│       │   │   ├── >>> trading_mt5.py <<<
│       │   │   └── >>> universe_mt5.py <<<
│       │   └── telemetry
│       │       ├── >>> __init__.py <<<
│       │       ├── __pycache__ (ignored)
│       │       ├── >>> console.py <<<
│       │       ├── >>> db_journal.py <<<
│       │       ├── >>> memory.py <<<
│       │       └── >>> mongo_journal.py <<<
│       ├── application
│       │   ├── pipeline
│       │   │   ├── >>> __init__.py <<<
│       │   │   ├── __pycache__ (ignored)
│       │   │   ├── >>> config.py <<<
│       │   │   ├── >>> result.py <<<
│       │   │   └── >>> service.py <<<
│       │   ├── plugins
│       │   │   ├── __pycache__ (ignored)
│       │   │   └── >>> registry.py <<<
│       │   ├── runmodes
│       │   │   ├── __pycache__ (ignored)
│       │   │   └── >>> live_multimodel.py <<<
│       │   ├── services
│       │   │   ├── __pycache__ (ignored)
│       │   │   ├── >>> coverage_selector.py <<<
│       │   │   ├── >>> ensemble.py <<<
│       │   │   ├── >>> order_planner.py <<<
│       │   │   └── >>> sizer.py <<<
│       │   └── telemetry
│       │       ├── >>> __init__.py <<<
│       │       ├── __pycache__ (ignored)
│       │       ├── >>> event_factory.py <<<
│       │       ├── >>> hub.py <<<
│       │       ├── >>> ids.py <<<
│       │       ├── >>> provider.py <<<
│       │       └── >>> trace.py <<<
│       ├── bootstrap
│       │   ├── __pycache__ (ignored)
│       │   └── >>> main.py <<<
│       ├── domain
│       │   ├── >>> __init__.py <<<
│       │   ├── __pycache__ (ignored)
│       │   ├── market
│       │   │   ├── >>> __init__.py <<<
│       │   │   └── >>> entities.py <<<
│       │   ├── portfolio
│       │   │   ├── >>> __init__.py <<<
│       │   │   ├── __pycache__ (ignored)
│       │   │   ├── allocators
│       │   │   │   ├── >>> __init__.py <<<
│       │   │   │   ├── __pycache__ (ignored)
│       │   │   │   ├── >>> base.py <<<
│       │   │   │   ├── >>> equal_weight.py <<<
│       │   │   │   └── >>> proportional.py <<<
│       │   │   ├── balancers
│       │   │   │   ├── >>> __init__.py <<<
│       │   │   │   ├── __pycache__ (ignored)
│       │   │   │   ├── >>> base.py <<<
│       │   │   │   └── >>> threshold.py <<<
│       │   │   └── >>> entities.py <<<
│       │   └── signals
│       │       ├── >>> __init__.py <<<
│       │       ├── __pycache__ (ignored)
│       │       ├── >>> entities.py <<<
│       │       ├── indicators
│       │       │   ├── >>> __init__.py <<<
│       │       │   ├── __pycache__ (ignored)
│       │       │   ├── >>> base.py <<<
│       │       │   ├── >>> stretch_zscore.py <<<
│       │       │   ├── >>> trend_donchian.py <<<
│       │       │   └── >>> volatility_atr.py <<<
│       │       └── models
│       │           ├── >>> __init__.py <<<
│       │           ├── __pycache__ (ignored)
│       │           ├── >>> base.py <<<
│       │           ├── >>> mean_reversion.py <<<
│       │           └── >>> trend_following.py <<<
│       ├── ports
│       │   ├── __pycache__ (ignored)
│       │   ├── >>> account.py <<<
│       │   ├── >>> market_data.py <<<
│       │   ├── >>> telemetry.py <<<
│       │   ├── >>> trading.py <<<
│       │   └── >>> universe.py <<<
│       └── shared
│           ├── __pycache__ (ignored)
│           ├── >>> config.py <<<
│           └── >>> decorators.py <<<
├── tests
│   └── >>> test_telemetry.py <<<
└── tycherion_guidelines.md (ignored)



--- .env:START ---
# Fixed identity for this machine/instance (used by telemetry runner_id)
TYCHERION_RUNNER_ID="dev-runner-01"

MT5_TERMINAL_PATH="C:\\Program Files\\MetaTrader 5 Terminal\\terminal64.exe"

# DEMO
MT5_SERVER="Rico-DEMO"
MT5_LOGIN="3008317111"
MT5_PASSWORD="BusterBD001."

# # PROD
# MT5_SERVER="Rico-PROD"
# MT5_LOGIN="3008317111"
# MT5_PASSWORD="BusterBD001."
--- .env:END ---

--- .env.example:START ---
# Fixed identity for this machine/instance (used by telemetry runner_id)
TYCHERION_RUNNER_ID="dev-runner-01"

# Opcional: se quiser logar via código em vez do Terminal já autenticado
MT5_TERMINAL_PATH=
MT5_SERVER=
MT5_LOGIN=
MT5_PASSWORD=

--- .env.example:END ---

--- pyproject.toml:START ---
[build-system]
requires = ["setuptools>=68", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "tycherion"
version = "0.1.0"
requires-python = ">=3.10"
dependencies = [
  "MetaTrader5>=5.0",
  "pandas>=2.2",
  "pyyaml>=6.0",
  "python-dotenv>=1.0",
  "pydantic>=2.8",
  "typing-extensions>=4.12"
]

[tool.setuptools]
package-dir = {"" = "src"}

[tool.setuptools.packages.find]
where = ["src"]

[tool.mypy]
python_version = "3.10"
strict = true

[tool.ruff]
line-length = 100

--- pyproject.toml:END ---

--- requirements.txt:START ---
MetaTrader5>=5.0
pandas>=2.2
pyyaml>=6.0
python-dotenv>=1.0
pydantic>=2.8
typing-extensions>=4.12

--- requirements.txt:END ---

--- .vscode\settings.json:START ---
{
    "python.analysis.extraPaths": [
        "./src"
    ]
}
--- .vscode\settings.json:END ---

--- configs\demo.yaml:START ---
timeframe: "H1"

lookback_days: 15

trading:
  dry_run: true
  require_demo: true
  deviation_points: 10
  volume_mode: "min"
  fixed_volume: 100.0

risk:
  risk_per_trade_pct: 0.5
  max_daily_loss_pct: 2.0

mt5:
  terminal_path: null
  server: null
  login: null
  password: null

application:
  run_mode:
    name: "live_multimodel"
  playbook: "default"
  schedule:
    run_forever: true
    interval_seconds: 60

  coverage:
    source: "static"
    symbols: ["PETR4", "VALE3", "WIN$", "WDO$"]
    pattern: null

  models:
    pipeline:
      - "trend_following"
      - "mean_reversion"

  portfolio:
    allocator: "proportional"
    balancer: "threshold"
    threshold_weight: 0.25

telemetry:
  db_enabled: false
  db_min_level: "INFO"
  db_channels: ["audit", "ops"]        # adicione "debug" se quiser persistir debug
  db_batch_size: 50
  # db_dsn: "postgresql://user:pass@host:5432/dbname"  # opcional

  mongo_enabled: false
  mongo_min_level: "INFO"
  mongo_channels: ["audit", "ops"]
  mongo_batch_size: 200
  # mongo_uri: "mongodb://user:pass@host:27017/?authSource=admin"  # opcional
  # mongo_db: "tycherion"
  # mongo_collection: "execution_journal_events"

  console_enabled: true
  console_min_level: "INFO"
  console_channels: ["ops"]            # adicione "debug" para ver debug no terminal

--- configs\demo.yaml:END ---

--- scripts\run_demo.py:START ---
import sys, pathlib
ROOT = pathlib.Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT / "src"))
from tycherion.bootstrap.main import run_app
if __name__ == "__main__":
    run_app(config_path=str(ROOT / "configs" / "demo.yaml"))

--- scripts\run_demo.py:END ---

--- src\tycherion\adapters\mt5\account_mt5.py:START ---
from __future__ import annotations

import MetaTrader5 as mt5

from tycherion.ports.account import AccountPort
from tycherion.domain.portfolio.entities import Position


class MT5Account(AccountPort):
    def is_demo(self) -> bool:
        ai = mt5.account_info()
        return bool(ai and ai.trade_mode == mt5.ACCOUNT_TRADE_MODE_DEMO)

    def balance(self) -> float:
        ai = mt5.account_info()
        return float(getattr(ai, "balance", 0.0) or 0.0)

    def equity(self) -> float:
        ai = mt5.account_info()
        return float(getattr(ai, "equity", 0.0) or 0.0)

    def positions(self) -> list[Position]:
        poss = mt5.positions_get()
        out: list[Position] = []
        if poss:
            for p in poss:
                out.append(
                    Position(
                        symbol=p.symbol,
                        quantity=float(getattr(p, "volume", 0.0) or 0.0),
                        price=float(getattr(p, "price_open", 0.0) or 0.0),
                    )
                )
        return out

--- src\tycherion\adapters\mt5\account_mt5.py:END ---

--- src\tycherion\adapters\mt5\market_data_mt5.py:START ---
from __future__ import annotations
from datetime import datetime, timezone
from typing import Dict
import pandas as pd
import MetaTrader5 as mt5
from tycherion.ports.market_data import MarketDataPort

_TF_MAP: Dict[str, int] = {
    "M1": mt5.TIMEFRAME_M1,
    "M5": mt5.TIMEFRAME_M5,
    "M15": mt5.TIMEFRAME_M15,
    "M30": mt5.TIMEFRAME_M30,
    "H1": mt5.TIMEFRAME_H1,
    "H4": mt5.TIMEFRAME_H4,
    "D1": mt5.TIMEFRAME_D1,
}

class MT5MarketData(MarketDataPort):
    def get_bars(self, symbol: str, timeframe: str, start: datetime, end: datetime) -> pd.DataFrame:
        tf = _TF_MAP.get(timeframe.upper())
        if tf is None:
            raise ValueError(f"Unsupported timeframe: {timeframe}")
        rates = mt5.copy_rates_range(
            symbol, tf,
            start.astimezone(timezone.utc),
            end.astimezone(timezone.utc)
        )
        if rates is None or len(rates) == 0:
            return pd.DataFrame(columns=["time","open","high","low","close","tick_volume","spread","real_volume"])
        df = pd.DataFrame(rates)
        df["time"] = pd.to_datetime(df["time"], unit="s", utc=True)
        return df
--- src\tycherion\adapters\mt5\market_data_mt5.py:END ---

--- src\tycherion\adapters\mt5\trading_mt5.py:START ---
from __future__ import annotations
from dataclasses import dataclass
from typing import Optional
import MetaTrader5 as mt5
from tycherion.ports.trading import TradingPort, TradeResult
from tycherion.shared.decorators import demo_only, logged
from tycherion.application.services.sizer import symbol_min_volume, volume_from_weight

@dataclass
class MT5Trader(TradingPort):
    dry_run: bool = True
    require_demo: bool = True
    deviation_points: int = 10
    volume_mode: str = "min"
    fixed_volume: float = 0.01

    def _resolve_volume(self, symbol: str, volume: Optional[float]) -> float:
        if volume is not None:
            return float(volume)
        return volume_from_weight(symbol, 1.0, self.volume_mode, self.fixed_volume)

    @logged
    @demo_only
    def market_buy(self, symbol: str, volume: Optional[float] = None) -> TradeResult:
        if self.dry_run:
            return TradeResult(True, 0, None, "DRY_RUN: buy skipped")
        if not mt5.symbol_select(symbol, True):
            return TradeResult(False, -1, None, f"symbol_select failed: {symbol}")
        tick = mt5.symbol_info_tick(symbol)
        if not tick:
            return TradeResult(False, -2, None, "missing tick")
        vol = self._resolve_volume(symbol, volume)
        if vol < symbol_min_volume(symbol):
            vol = symbol_min_volume(symbol)
        request = {
            "action": mt5.TRADE_ACTION_DEAL,
            "symbol": symbol,
            "type": mt5.ORDER_TYPE_BUY,
            "volume": vol,
            "price": tick.ask,
            "deviation": self.deviation_points,
            "type_time": mt5.ORDER_TIME_GTC,
            "type_filling": mt5.ORDER_FILLING_RETURN,
            "magic": 401,
            "comment": "tycherion-buy",
        }
        check = mt5.order_check(request)
        if not check or check.retcode != mt5.TRADE_RETCODE_DONE:
            return TradeResult(False, getattr(check, "retcode", -3), None, f"order_check failed: {check}")
        res = mt5.order_send(request)
        ok = bool(res and res.retcode in (mt5.TRADE_RETCODE_DONE, mt5.TRADE_RETCODE_PLACED))
        return TradeResult(ok, getattr(res, "retcode", -4), getattr(res, "order", None), str(res))

    @logged
    @demo_only
    def market_sell(self, symbol: str, volume: Optional[float] = None) -> TradeResult:
        if self.dry_run:
            return TradeResult(True, 0, None, "DRY_RUN: sell skipped")
        if not mt5.symbol_select(symbol, True):
            return TradeResult(False, -1, None, f"symbol_select failed: {symbol}")
        tick = mt5.symbol_info_tick(symbol)
        if not tick:
            return TradeResult(False, -2, None, "missing tick")
        vol = self._resolve_volume(symbol, volume)
        if vol < symbol_min_volume(symbol):
            vol = symbol_min_volume(symbol)
        request = {
            "action": mt5.TRADE_ACTION_DEAL,
            "symbol": symbol,
            "type": mt5.ORDER_TYPE_SELL,
            "volume": vol,
            "price": tick.bid,
            "deviation": self.deviation_points,
            "type_time": mt5.ORDER_TIME_GTC,
            "type_filling": mt5.ORDER_FILLING_RETURN,
            "magic": 401,
            "comment": "tycherion-sell",
        }
        check = mt5.order_check(request)
        if not check or check.retcode != mt5.TRADE_RETCODE_DONE:
            return TradeResult(False, getattr(check, "retcode", -3), None, f"order_check failed: {check}")
        res = mt5.order_send(request)
        ok = bool(res and res.retcode in (mt5.TRADE_RETCODE_DONE, mt5.TRADE_RETCODE_PLACED))
        return TradeResult(ok, getattr(res, "retcode", -4), getattr(res, "order", None), str(res))

--- src\tycherion\adapters\mt5\trading_mt5.py:END ---

--- src\tycherion\adapters\mt5\universe_mt5.py:START ---
from __future__ import annotations
import MetaTrader5 as mt5
from typing import List
from tycherion.ports.universe import UniversePort

class MT5Universe(UniversePort):
    def visible_symbols(self) -> List[str]:
        syms = mt5.symbols_get()
        return [s.name for s in syms if getattr(s, "visible", False)]

    def by_pattern(self, pattern: str) -> List[str]:
        syms = mt5.symbols_get(pattern)
        return [s.name for s in syms]

--- src\tycherion\adapters\mt5\universe_mt5.py:END ---

--- src\tycherion\adapters\telemetry\console.py:START ---
from __future__ import annotations

import sys
from dataclasses import dataclass, field
from typing import Any

from tycherion.ports.telemetry import TelemetryEvent, TelemetryLevel, TelemetrySink


def _short(v: Any, limit: int = 80) -> str:
    s = str(v)
    return s if len(s) <= limit else (s[: limit - 1] + "…")


def _summarize(event: TelemetryEvent) -> str:
    attributes = dict(event.attributes or {})
    data = dict(event.data or {})

    parts: list[str] = []
    for k in ("component", "stage", "symbol", "model"):
        if k in attributes and attributes[k] not in (None, ""):
            parts.append(f"{k}={_short(attributes[k], 40)}")

    for k in (
        "dropped_count",
        "passed_count",
        "symbols_count",
        "duration_ms",
        "threshold",
        "score",
        "side",
        "weight",
        "confidence",
        "reason",
    ):
        if k in data:
            parts.append(f"{k}={_short(data[k], 40)}")

    if not parts and data:
        parts.append(f"data_keys={list(data.keys())[:8]}")

    return " ".join(parts)


@dataclass(slots=True)
class ConsoleTelemetrySink(TelemetrySink):
    """Human-friendly console output.

    Default should be disabled via config. When enabled, prints one line per event.
    """

    enabled_flag: bool = False
    channels: set[str] = field(default_factory=lambda: {"ops"})
    min_level: TelemetryLevel = TelemetryLevel.INFO
    stream: Any = sys.stdout

    def enabled(self, channel: str, level: TelemetryLevel, name: str | None = None) -> bool:
        _ = name
        if not self.enabled_flag:
            return False
        if channel not in self.channels:
            return False
        return TelemetryLevel.coerce(level).rank() >= TelemetryLevel.coerce(self.min_level).rank()

    def emit(self, event: TelemetryEvent) -> None:
        # Keep this stable and easy to grep
        trace_short = _short(event.trace_id, 18)
        msg = (
            f"[{event.level.value}]"
            f"[{event.channel}]"
            f"[runner={_short(event.runner_id, 18)}]"
            f"[trace={trace_short}]"
            f"[event_seq={event.event_seq}]"
        )

        if event.span_id:
            msg += f"[span={_short(event.span_id, 8)}]"
        if event.parent_span_id:
            msg += f"[parent={_short(event.parent_span_id, 8)}]"

        msg += f" {event.name}"
        summary = _summarize(event)
        if summary:
            msg = f"{msg} {summary}"
        try:
            self.stream.write(msg + "\n")
            self.stream.flush()
        except Exception:
            return

--- src\tycherion\adapters\telemetry\console.py:END ---

--- src\tycherion\adapters\telemetry\db_journal.py:START ---
from __future__ import annotations

import json
from dataclasses import dataclass, field

from tycherion.ports.telemetry import TelemetryEvent, TelemetryLevel, TelemetrySink


# Idempotent-ish DDL:
# - Create table if missing
# - Add new columns for newer schema if table existed from older versions
# - Create a unique index for idempotent dedupe (trace_id, event_seq)
_DDL = """
CREATE TABLE IF NOT EXISTS execution_journal_events (
  id BIGSERIAL PRIMARY KEY,
  runner_id TEXT NULL,
  trace_id TEXT NOT NULL,
  event_seq BIGINT NULL,
  ts_utc TEXT NOT NULL,
  mono_ns BIGINT NULL,
  span_id TEXT NULL,
  parent_span_id TEXT NULL,
  name TEXT NOT NULL,
  level TEXT NOT NULL,
  channel TEXT NOT NULL,
  attributes_json TEXT NOT NULL,
  data_json TEXT NOT NULL,
  schema_version INTEGER NOT NULL
);

ALTER TABLE execution_journal_events ADD COLUMN IF NOT EXISTS runner_id TEXT NULL;
ALTER TABLE execution_journal_events ADD COLUMN IF NOT EXISTS event_seq BIGINT NULL;
ALTER TABLE execution_journal_events ADD COLUMN IF NOT EXISTS mono_ns BIGINT NULL;

CREATE INDEX IF NOT EXISTS idx_eje_trace_ts ON execution_journal_events(trace_id, ts_utc);
CREATE INDEX IF NOT EXISTS idx_eje_span ON execution_journal_events(span_id);
CREATE INDEX IF NOT EXISTS idx_eje_name ON execution_journal_events(name);
CREATE INDEX IF NOT EXISTS idx_eje_channel ON execution_journal_events(channel);
CREATE INDEX IF NOT EXISTS idx_eje_runner ON execution_journal_events(runner_id);

-- Dedupe / idempotency
CREATE UNIQUE INDEX IF NOT EXISTS uq_eje_trace_eventseq ON execution_journal_events(trace_id, event_seq);
"""


def _connect(dsn: str):
    """Lazy import of a Postgres driver.

    We intentionally do NOT add dependencies to Tycherion. The sink will work
    if the runtime environment provides a PostgreSQL DB-API driver.
    """

    try:
        import psycopg  # type: ignore

        return psycopg.connect(dsn)
    except Exception:
        pass

    try:
        import psycopg2  # type: ignore

        return psycopg2.connect(dsn)
    except Exception as e:
        raise RuntimeError(
            "PostgreSQL telemetry sink requires a driver (psycopg or psycopg2) to be installed"
        ) from e


@dataclass(slots=True)
class DbExecutionJournalSink(TelemetrySink):
    """Append-only execution journal persisted in PostgreSQL.

    Dedupe strategy:
    - table has a surrogate PK (BIGSERIAL)
    - for idempotency, we also have a unique index on (trace_id, event_seq)
    - inserts use ON CONFLICT DO NOTHING when supported
    """

    dsn: str
    enabled_flag: bool = True
    channels: set[str] = field(default_factory=lambda: {"audit", "ops"})
    min_level: TelemetryLevel = TelemetryLevel.INFO
    batch_size: int = 100

    _conn: object | None = field(default=None, init=False, repr=False)
    _buffer: list[
        tuple[
            str | None,
            str,
            int | None,
            str,
            int | None,
            str | None,
            str | None,
            str,
            str,
            str,
            str,
            str,
            int,
        ]
    ] = field(default_factory=list, init=False, repr=False)
    _supports_on_conflict: bool = field(default=True, init=False, repr=False)

    def _ensure_conn(self):
        if self._conn is not None:
            return self._conn
        conn = _connect(self.dsn)
        try:
            cur = conn.cursor()
            cur.execute(_DDL)
            conn.commit()
        except Exception:
            try:
                conn.close()
            except Exception:
                pass
            raise
        self._conn = conn
        return conn

    def enabled(self, channel: str, level: TelemetryLevel, name: str | None = None) -> bool:
        _ = name
        if not self.enabled_flag:
            return False
        if channel not in self.channels:
            return False
        return TelemetryLevel.coerce(level).rank() >= TelemetryLevel.coerce(self.min_level).rank()

    def emit(self, event: TelemetryEvent) -> None:
        if not self.enabled(event.channel, event.level, event.name):
            return
        try:
            attributes_json = json.dumps(
                dict(event.attributes or {}), separators=(",", ":"), ensure_ascii=False
            )
            data_json = json.dumps(dict(event.data or {}), separators=(",", ":"), ensure_ascii=False)
            row = (
                str(event.runner_id) if event.runner_id else None,
                str(event.trace_id),
                int(event.event_seq) if event.event_seq is not None else None,
                event.ts_utc.isoformat(),
                int(event.mono_ns) if event.mono_ns is not None else None,
                str(event.span_id) if event.span_id else None,
                str(event.parent_span_id) if event.parent_span_id else None,
                str(event.name),
                str(event.level.value),
                str(event.channel),
                attributes_json,
                data_json,
                int(event.schema_version),
            )
            self._buffer.append(row)
            if len(self._buffer) >= max(1, int(self.batch_size)):
                self.flush()
        except Exception:
            return

    def flush(self) -> None:
        if not self._buffer:
            return
        try:
            conn = self._ensure_conn()
        except Exception:
            self._buffer.clear()
            return

        rows = list(self._buffer)
        self._buffer.clear()

        def _exec(sql: str) -> None:
            cur = conn.cursor()
            cur.executemany(sql, rows)
            conn.commit()

        try:
            if self._supports_on_conflict:
                _exec(
                    """
                    INSERT INTO execution_journal_events
                      (runner_id, trace_id, event_seq, ts_utc, mono_ns, span_id, parent_span_id, name, level, channel, attributes_json, data_json, schema_version)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (trace_id, event_seq) DO NOTHING
                    """
                )
            else:
                _exec(
                    """
                    INSERT INTO execution_journal_events
                      (runner_id, trace_id, event_seq, ts_utc, mono_ns, span_id, parent_span_id, name, level, channel, attributes_json, data_json, schema_version)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """
                )
        except Exception:
            # If ON CONFLICT fails (missing unique index/constraint), fallback once.
            try:
                conn.rollback()
            except Exception:
                pass

            if self._supports_on_conflict:
                self._supports_on_conflict = False
                try:
                    _exec(
                        """
                        INSERT INTO execution_journal_events
                          (runner_id, trace_id, event_seq, ts_utc, mono_ns, span_id, parent_span_id, name, level, channel, attributes_json, data_json, schema_version)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        """
                    )
                    return
                except Exception:
                    try:
                        conn.rollback()
                    except Exception:
                        pass
            return

    def close(self) -> None:
        try:
            self.flush()
        finally:
            if self._conn is not None:
                try:
                    self._conn.close()
                except Exception:
                    pass
                self._conn = None

--- src\tycherion\adapters\telemetry\db_journal.py:END ---

--- src\tycherion\adapters\telemetry\memory.py:START ---
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Iterable

from tycherion.ports.telemetry import TelemetryEvent, TelemetryLevel, TelemetrySink


@dataclass(slots=True)
class InMemoryTelemetrySink(TelemetrySink):
    """Test-friendly telemetry sink.

    Stores events in memory so tests can assert on them without stdout/DB.
    """

    enabled_flag: bool = True
    channels: set[str] = field(default_factory=lambda: {"audit", "ops"})
    min_level: TelemetryLevel = TelemetryLevel.INFO
    events: list[TelemetryEvent] = field(default_factory=list)

    def enabled(self, channel: str, level: TelemetryLevel, name: str | None = None) -> bool:
        _ = name
        if not self.enabled_flag:
            return False
        if channel not in self.channels:
            return False
        return TelemetryLevel.coerce(level).rank() >= TelemetryLevel.coerce(self.min_level).rank()

    def emit(self, event: TelemetryEvent) -> None:
        self.events.append(event)

--- src\tycherion\adapters\telemetry\memory.py:END ---

--- src\tycherion\adapters\telemetry\mongo_journal.py:START ---
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any

from tycherion.ports.telemetry import TelemetryEvent, TelemetryLevel, TelemetrySink


def _mongo_client(uri: str):
    try:
        from pymongo import MongoClient  # type: ignore
    except Exception as e:
        raise RuntimeError(
            "MongoDB telemetry sink requires `pymongo` to be installed in the runtime environment"
        ) from e
    return MongoClient(uri)


@dataclass(slots=True)
class MongoExecutionJournalSink(TelemetrySink):
    """Append-only execution journal persisted in MongoDB.

    This sink is meant for operational health/audit data:
    - errors, spans, lifecycle, audits
    - queryable by trace_id, runner_id, event_seq, time

    Dedupe strategy:
    - unique index on (trace_id, event_seq)
    - inserts are best-effort, duplicates are ignored
    """

    uri: str
    db_name: str = "tycherion"
    collection_name: str = "execution_journal_events"
    enabled_flag: bool = True
    channels: set[str] = field(default_factory=lambda: {"audit", "ops"})
    min_level: TelemetryLevel = TelemetryLevel.INFO
    batch_size: int = 200

    _client: Any | None = field(default=None, init=False, repr=False)
    _collection: Any | None = field(default=None, init=False, repr=False)
    _buffer: list[dict[str, Any]] = field(default_factory=list, init=False, repr=False)

    def _ensure_collection(self):
        if self._collection is not None:
            return self._collection

        client = _mongo_client(self.uri)
        db = client[self.db_name]
        col = db[self.collection_name]

        # Best-effort index creation (ignore permissions/duplicate issues)
        try:
            col.create_index([("trace_id", 1), ("event_seq", 1)], unique=True, name="uq_trace_eventseq")
            col.create_index([("ts_utc", 1)], name="idx_ts")
            col.create_index([("runner_id", 1)], name="idx_runner")
            col.create_index([("channel", 1)], name="idx_channel")
            col.create_index([("level", 1)], name="idx_level")
        except Exception:
            pass

        self._client = client
        self._collection = col
        return col

    def enabled(self, channel: str, level: TelemetryLevel, name: str | None = None) -> bool:
        _ = name
        if not self.enabled_flag:
            return False
        if channel not in self.channels:
            return False
        return TelemetryLevel.coerce(level).rank() >= TelemetryLevel.coerce(self.min_level).rank()

    def emit(self, event: TelemetryEvent) -> None:
        if not self.enabled(event.channel, event.level, event.name):
            return

        try:
            doc: dict[str, Any] = {
                "schema_version": int(event.schema_version),
                "runner_id": str(event.runner_id),
                "trace_id": str(event.trace_id),
                "event_seq": int(event.event_seq),
                "ts_utc": event.ts_utc,  # pymongo stores datetime natively
                "mono_ns": int(event.mono_ns) if event.mono_ns is not None else None,
                "span_id": str(event.span_id) if event.span_id else None,
                "parent_span_id": str(event.parent_span_id) if event.parent_span_id else None,
                "name": str(event.name),
                "level": str(event.level.value),
                "channel": str(event.channel),
                "attributes": dict(event.attributes or {}),
                "data": dict(event.data or {}),
            }
            self._buffer.append(doc)
            if len(self._buffer) >= max(1, int(self.batch_size)):
                self.flush()
        except Exception:
            return

    def flush(self) -> None:
        if not self._buffer:
            return

        try:
            col = self._ensure_collection()
        except Exception:
            self._buffer.clear()
            return

        docs = list(self._buffer)
        self._buffer.clear()

        try:
            # ordered=False keeps going after dup errors
            col.insert_many(docs, ordered=False)
        except Exception:
            # Ignore duplicate key and other errors (best-effort journal)
            return

    def close(self) -> None:
        try:
            self.flush()
        finally:
            try:
                if self._client is not None:
                    self._client.close()
            except Exception:
                pass
            self._client = None
            self._collection = None

--- src\tycherion\adapters\telemetry\mongo_journal.py:END ---

--- src\tycherion\adapters\telemetry\__init__.py:START ---
from .console import ConsoleTelemetrySink
from .db_journal import DbExecutionJournalSink
from .mongo_journal import MongoExecutionJournalSink
from .memory import InMemoryTelemetrySink

__all__ = [
    "ConsoleTelemetrySink",
    "DbExecutionJournalSink",
    "MongoExecutionJournalSink",
    "InMemoryTelemetrySink",
]

--- src\tycherion\adapters\telemetry\__init__.py:END ---

--- src\tycherion\application\pipeline\config.py:START ---
from __future__ import annotations

from dataclasses import dataclass
from typing import Iterable, List

from tycherion.shared.config import AppConfig, PipelineStageCfg


@dataclass(frozen=True, slots=True)
class PipelineStageConfig:
    """Configuration of a single pipeline stage (application-level, YAML-agnostic)."""

    name: str
    drop_threshold: float | None = None


@dataclass(frozen=True, slots=True)
class PipelineConfig:
    """Internal normalized pipeline configuration.

    This object is the only thing the pipeline execution should consume.
    It is intentionally decoupled from YAML and Pydantic.
    """

    stages: List[PipelineStageConfig]


def build_pipeline_config(cfg: AppConfig) -> PipelineConfig:
    """Build a PipelineConfig from the current AppConfig.

    The AppConfig is created by YAML/adapters, but the rest of the application
    should not read YAML-derived structures directly.
    """
    stages_in: Iterable[PipelineStageCfg] = cfg.application.models.pipeline or []
    stages: list[PipelineStageConfig] = []
    for st in stages_in:
        stages.append(
            PipelineStageConfig(
                name=str(st.name),
                drop_threshold=(float(st.drop_threshold) if st.drop_threshold is not None else None),
            )
        )
    if not stages:
        raise RuntimeError(
            "No model pipeline configured. Please set application.models.pipeline in your YAML."
        )
    return PipelineConfig(stages=stages)

--- src\tycherion\application\pipeline\config.py:END ---

--- src\tycherion\application\pipeline\result.py:START ---
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict

from tycherion.domain.portfolio.entities import SignalsBySymbol
from tycherion.domain.signals.entities import SymbolState

from .config import PipelineConfig


@dataclass(frozen=True, slots=True)
class PipelineRunResult:
    pipeline_config: PipelineConfig
    states_by_symbol: Dict[str, SymbolState]
    signals_by_symbol: SignalsBySymbol
    stage_stats: Dict[str, int]

--- src\tycherion\application\pipeline\result.py:END ---

--- src\tycherion\application\pipeline\service.py:START ---
from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import Callable, Dict, Mapping, Optional, Tuple

import pandas as pd

from tycherion.domain.portfolio.entities import PortfolioSnapshot, Signal, SignalsBySymbol
from tycherion.domain.signals.entities import (
    IndicatorOutput,
    ModelDecision,
    ModelStageResult,
    SymbolState,
)
from tycherion.domain.signals.models.base import SignalModel
from tycherion.domain.signals.indicators.base import BaseIndicator
from tycherion.ports.market_data import MarketDataPort
from tycherion.application.telemetry import TraceTelemetry
from tycherion.ports.telemetry import TelemetryLevel

from .config import PipelineConfig, PipelineStageConfig
from .result import PipelineRunResult

@dataclass(slots=True)
class ModelPipelineService:
    """Façade that runs the ordered per-symbol model pipeline."""

    market_data: MarketDataPort
    model_registry: Mapping[str, SignalModel]
    indicator_picker: Callable[[str, Optional[str]], BaseIndicator]
    timeframe: str
    lookback_days: int
    playbook: str | None = None

    def run(
        self,
        universe_symbols: list[str],
        portfolio_snapshot: PortfolioSnapshot,
        pipeline_config: PipelineConfig,
        tracer: TraceTelemetry,
    ) -> PipelineRunResult:
        t = tracer.child({"component": "pipeline"})
        held_symbols = set(portfolio_snapshot.positions.keys())

        with t.span(
            "pipeline",
            channel="ops",
            level=TelemetryLevel.INFO,
            data={
                "symbols_count": int(len(universe_symbols)),
                "stages": [st.name for st in pipeline_config.stages],
            },
        ):
            # 1) Init per-symbol state
            states: Dict[str, SymbolState] = {
                sym: SymbolState(symbol=sym, is_held=(sym in held_symbols))
                for sym in universe_symbols
            }

            # 2) Resolve models
            resolved = self._resolve_models(pipeline_config)

            # 3) Determine indicator needs once for the whole pipeline
            needed_keys: set[str] = set()
            for _, model in resolved:
                try:
                    needed_keys.update(model.requires() or set())
                except Exception:
                    # a model might not implement requires()
                    pass

            # 4) Time window for analysis
            end = datetime.now(timezone.utc)
            start = end - timedelta(days=int(self.lookback_days))

            stage_stats: Dict[str, int] = {st.name: 0 for st in pipeline_config.stages}
            stage_passed: Dict[str, int] = {st.name: 0 for st in pipeline_config.stages}

            for st in pipeline_config.stages:
                t.emit(
                    name="pipeline.stage_started",
                    channel="ops",
                    level=TelemetryLevel.INFO,
                    attributes={"stage": st.name},
                    data={"threshold": st.drop_threshold},
                )

            for symbol, state in states.items():
                if not state.alive and not state.is_held:
                    continue

                df = self._safe_get_bars(symbol, start, end, state, t)
                if df is None or df.empty:
                    if not state.is_held:
                        t.emit(
                            name="pipeline.symbol_dropped",
                            channel="audit",
                            level=TelemetryLevel.WARN,
                            attributes={"symbol": symbol},
                            data={"reason": "no_market_data"},
                        )
                        state.alive = False
                    continue

                if t.enabled("debug", TelemetryLevel.DEBUG):
                    try:
                        t.emit(
                            name="market_data.sample",
                            channel="debug",
                            level=TelemetryLevel.DEBUG,
                            attributes={"symbol": symbol},
                            data={
                                "rows": int(len(df)),
                                "columns": list(df.columns)[:20],
                                "head": df.head(2).to_dict(orient="list"),
                                "tail": df.tail(2).to_dict(orient="list"),
                            },
                        )
                    except Exception:
                        pass

                bundle = self._compute_indicators(df, needed_keys, state, t)

                # Pipeline execution per stage
                for stage_cfg, model in resolved:
                    if not state.alive and not state.is_held:
                        break

                    stage_passed[stage_cfg.name] = int(stage_passed.get(stage_cfg.name, 0)) + 1
                    score = self._run_stage(symbol, stage_cfg, model, bundle, state, t)

                    # Drop policy
                    if stage_cfg.drop_threshold is not None and score < float(stage_cfg.drop_threshold):
                        if state.is_held:
                            state.notes[f"below_threshold_{stage_cfg.name}"] = 1.0
                            continue
                        state.alive = False
                        state.notes[f"dropped_by_{stage_cfg.name}"] = 1.0
                        stage_stats[stage_cfg.name] = int(stage_stats.get(stage_cfg.name, 0)) + 1
                        t.emit(
                            name="pipeline.symbol_dropped",
                            channel="audit",
                            level=TelemetryLevel.INFO,
                            attributes={"symbol": symbol, "stage": stage_cfg.name},
                            data={
                                "score": float(score),
                                "threshold": float(stage_cfg.drop_threshold),
                                "reason": "below_threshold",
                            },
                        )
                        break

                # Final signal fields (simple v1 rule: last stage score)
                last_score = float(state.pipeline_results[-1].score) if state.pipeline_results else 0.0
                state.alpha_score = last_score
                state.notes["final_confidence"] = abs(last_score)

            # 5) Convert states into SignalsBySymbol
            signals: SignalsBySymbol = {}
            for symbol, state in states.items():
                if not state.alive and not state.is_held:
                    continue
                signed = float(state.alpha_score)
                confidence = float(state.notes.get("final_confidence", abs(signed)))
                signals[symbol] = Signal(symbol=symbol, signed=signed, confidence=confidence)
                t.emit(
                    name="pipeline.signal_emitted",
                    channel="audit",
                    level=TelemetryLevel.INFO,
                    attributes={"symbol": symbol},
                    data={"signed": signed, "confidence": confidence},
                )

            for st in pipeline_config.stages:
                dropped = int(stage_stats.get(st.name, 0))
                passed = int(stage_passed.get(st.name, 0))
                t.emit(
                    name="pipeline.stage_completed",
                    channel="ops",
                    level=TelemetryLevel.INFO,
                    attributes={"stage": st.name},
                    data={
                        "passed_count": passed,
                        "dropped_count": dropped,
                        "threshold": st.drop_threshold,
                    },
                )

            t.emit(
                name="pipeline.summary",
                channel="ops",
                level=TelemetryLevel.INFO,
                data={
                    "signals_count": int(len(signals)),
                    "alive_count": int(sum(1 for s in states.values() if s.alive or s.is_held)),
                },
            )

            return PipelineRunResult(
                pipeline_config=pipeline_config,
                states_by_symbol=states,
                signals_by_symbol=signals,
                stage_stats=stage_stats,
            )

    def _resolve_models(self, pipeline_config: PipelineConfig) -> list[Tuple[PipelineStageConfig, SignalModel]]:
        pipeline: list[Tuple[PipelineStageConfig, SignalModel]] = []
        for stage in pipeline_config.stages:
            name = stage.name
            model = self.model_registry.get(name)
            if model is None:
                available = ", ".join(sorted(self.model_registry.keys()))
                raise RuntimeError(f"Model not found: {name!r}. Available models: {available}")
            pipeline.append((stage, model))
        return pipeline

    def _safe_get_bars(
        self,
        symbol: str,
        start: datetime,
        end: datetime,
        state: SymbolState,
        t: TraceTelemetry,
    ) -> pd.DataFrame | None:
        try:
            return self.market_data.get_bars(symbol, self.timeframe, start, end)
        except Exception as e:
            state.notes["data_error"] = 1.0
            t.emit(
                name="error.exception",
                channel="ops",
                level=TelemetryLevel.ERROR,
                attributes={"symbol": symbol},
                data={"exception_type": type(e).__name__, "message": str(e), "stage": "get_bars"},
            )
            return None

    def _compute_indicators(
        self,
        df: pd.DataFrame,
        needed_keys: set[str],
        state: SymbolState,
        t: TraceTelemetry,
    ) -> Dict[str, IndicatorOutput]:
        bundle: Dict[str, IndicatorOutput] = {}
        for key in needed_keys:
            try:
                ind = self.indicator_picker(key, self.playbook)
                bundle[key] = ind.compute(df.copy())
            except Exception as e:
                state.notes[f"indicator_error_{key}"] = 1.0
                t.emit(
                    name="error.exception",
                    channel="ops",
                    level=TelemetryLevel.ERROR,
                    data={
                        "exception_type": type(e).__name__,
                        "message": str(e),
                        "stage": "indicator",
                        "indicator": key,
                    },
                )
                bundle[key] = IndicatorOutput(score=0.0, features={})
        return bundle

    def _run_stage(
        self,
        symbol: str,
        stage_cfg: PipelineStageConfig,
        model: SignalModel,
        indicators: Dict[str, IndicatorOutput],
        state: SymbolState,
        t: TraceTelemetry,
    ) -> float:
        stage_name = stage_cfg.name
        try:
            if t.enabled("debug", TelemetryLevel.DEBUG):
                try:
                    t.emit(
                        name="model.input_snapshot",
                        channel="debug",
                        level=TelemetryLevel.DEBUG,
                        attributes={"symbol": symbol, "stage": stage_name, "model": stage_name},
                        data={
                            "indicator_keys": list(indicators.keys())[:30],
                            "features_keys": {
                                k: list(v.features.keys())[:20]
                                for k, v in indicators.items()
                                if getattr(v, "features", None)
                            },
                        },
                    )
                except Exception:
                    pass
            decision = model.decide(indicators)
        except Exception as e:
            state.notes[f"model_error_{stage_name}"] = 1.0
            t.emit(
                name="error.exception",
                channel="ops",
                level=TelemetryLevel.ERROR,
                attributes={"symbol": symbol, "stage": stage_name, "model": stage_name},
                data={"exception_type": type(e).__name__, "message": str(e), "stage": "model"},
            )
            decision = ModelDecision(side="HOLD", weight=0.0, confidence=0.0)

        score = self._decision_to_score(decision)
        state.pipeline_results.append(ModelStageResult(model_name=stage_name, score=score))

        t.emit(
            name="model.decided",
            channel="audit",
            level=TelemetryLevel.INFO,
            attributes={"symbol": symbol, "stage": stage_name, "model": stage_name},
            data={
                "score": float(score),
                "side": decision.side,
                "weight": float(decision.weight or 0.0),
                "confidence": float(decision.confidence or 0.0),
            },
        )
        return score

    @staticmethod
    def _decision_to_score(d: ModelDecision) -> float:
        """Map a ModelDecision into a numeric score in [-1, 1]."""
        side = (d.side or "HOLD").upper()
        w = float(d.weight or 0.0)
        w = max(0.0, min(1.0, w))
        if side == "BUY":
            s = w
        elif side == "SELL":
            s = -w
        else:
            s = 0.0
        return max(-1.0, min(1.0, s))


--- src\tycherion\application\pipeline\service.py:END ---

--- src\tycherion\application\pipeline\__init__.py:START ---

--- src\tycherion\application\pipeline\__init__.py:END ---

--- src\tycherion\application\plugins\registry.py:START ---
from __future__ import annotations

from typing import Dict, List, Iterable

from tycherion.application.telemetry import TraceTelemetry
from tycherion.ports.telemetry import TelemetryLevel

from tycherion.domain.signals.indicators.base import BaseIndicator
from tycherion.domain.signals.models.base import SignalModel
from tycherion.domain.portfolio.allocators.base import BaseAllocator
from tycherion.domain.portfolio.balancers.base import BaseBalancer

INDICATORS: Dict[str, List[BaseIndicator]] = {}
MODELS: Dict[str, SignalModel] = {}
ALLOCATORS: Dict[str, BaseAllocator] = {}
BALANCERS: Dict[str, BaseBalancer] = {}
DEFAULT_METHOD: Dict[str, str] = {}

def register_indicator(*, key: str, method: str, tags: set[str]):
    """
    Register an indicator implementation for a given logical key (e.g. "trend")
    and method (e.g. "donchian_50_50").
    """
    def deco(cls):
        inst = cls()
        inst.key = key
        inst.method = method
        inst.tags = tags
        INDICATORS.setdefault(key, []).append(inst)
        return cls
    return deco


def register_model(*, name: str, tags: set[str]):
    """
    Register a per-symbol signal model.
    """
    def deco(cls):
        inst = cls()
        inst.name = name
        inst.tags = tags
        MODELS[name] = inst
        return cls
    return deco


def register_allocator(*, name: str, tags: set[str]):
    """
    Register a portfolio allocator strategy.
    """
    def deco(cls):
        inst = cls()
        inst.name = name
        inst.tags = tags
        ALLOCATORS[name] = inst
        return cls
    return deco


def register_balancer(*, name: str, tags: set[str]):
    """
    Register a portfolio balancer / rebalancer strategy.
    """
    def deco(cls):
        inst = cls()
        inst.name = name
        inst.tags = tags
        BALANCERS[name] = inst
        return cls
    return deco


def set_default_indicator_method(key: str, method: str) -> None:
    DEFAULT_METHOD[key] = method


def pick_indicator_for(key: str, playbook: str | None = None) -> BaseIndicator:
    """
    Pick an indicator instance for a given key and (optionally) playbook.
    Preference order:
    - indicators whose tags contain the playbook name
    - indicators whose tags contain "default"
    - otherwise, the first registered
    If DEFAULT_METHOD[key] is set, prefer that method among candidates.
    """
    candidates: Iterable[BaseIndicator] = INDICATORS.get(key, [])
    candidates = list(candidates)
    if not candidates:
        raise KeyError(f"No indicators registered for key={key!r}")

    # filter by tags / playbook
    if playbook:
        tagged = [
            ind for ind in candidates
            if playbook in getattr(ind, "tags", set())
        ]
        if tagged:
            candidates = tagged

    # then prefer "default"
    defaults = [
        ind for ind in candidates
        if "default" in getattr(ind, "tags", set())
    ]
    if defaults:
        candidates = defaults

    # lastly, prefer DEFAULT_METHOD if configured
    method = DEFAULT_METHOD.get(key)
    if method:
        for ind in candidates:
            if getattr(ind, "method", None) == method:
                return ind

    return candidates[0]


def auto_discover(tracer: TraceTelemetry | None) -> None:
    """
    Import all plugin modules so that their decorators run and fill the
    registries above. This is called once during application startup.
    """
    import importlib
    import pkgutil
    
    t = tracer.child({"component": "plugins"}) if tracer else None

    def _emit(name: str, *, level: TelemetryLevel, channel: str, data: dict) -> None:
        if t is None:
            return
        t.emit(name=name, level=level, channel=channel, attributes=None, data=data)

    bases = (
        "tycherion.domain.signals.indicators",
        "tycherion.domain.signals.models",
        "tycherion.domain.portfolio.allocators",
        "tycherion.domain.portfolio.balancers",
    )

    for base in bases:
        try:
            pkg = importlib.import_module(base)
        except Exception as e:
            _emit(
                "plugins.base_import_failed",
                level=TelemetryLevel.WARN,
                channel="ops",
                data={"base": base, "error": str(e)},
            )
            continue

        pkg_path = getattr(pkg, "__path__", None)
        if not pkg_path:
            continue

        for mod in pkgutil.walk_packages(pkg_path, pkg.__name__ + "."):
            try:
                importlib.import_module(mod.name)
            except Exception as e:
                _emit(
                    "plugins.module_import_failed",
                    level=TelemetryLevel.WARN,
                    channel="ops",
                    data={"module": mod.name, "error": str(e)},
                )

    _emit(
        "plugins.discovered",
        level=TelemetryLevel.INFO,
        channel="ops",
        data={
            "indicators_count": int(sum(len(v) for v in INDICATORS.values())),
            "models_count": int(len(MODELS)),
            "allocators_count": int(len(ALLOCATORS)),
            "balancers_count": int(len(BALANCERS)),
        },
    )

--- src\tycherion\application\plugins\registry.py:END ---

--- src\tycherion\application\runmodes\live_multimodel.py:START ---
from __future__ import annotations

import time
from typing import Dict

from tycherion.shared.config import AppConfig
from tycherion.ports.trading import TradingPort
from tycherion.ports.account import AccountPort
from tycherion.ports.universe import UniversePort

from tycherion.application.plugins.registry import (
    ALLOCATORS,
    BALANCERS,
)
from tycherion.application.services.coverage_selector import build_coverage
from tycherion.application.services.order_planner import build_orders
from tycherion.domain.portfolio.entities import (
    PortfolioSnapshot,
    Position,
)

from tycherion.application.pipeline.config import build_pipeline_config
from tycherion.application.pipeline.service import ModelPipelineService
from tycherion.application.telemetry import TelemetryProvider, TraceTelemetry, stable_config_hash
from tycherion.ports.telemetry import TelemetryLevel


def _build_portfolio_snapshot(account: AccountPort) -> PortfolioSnapshot:
    equity = float(account.equity())
    positions: Dict[str, Position] = {}
    for p in account.positions():
        positions[p.symbol] = p
    return PortfolioSnapshot(equity=equity, positions=positions)


def _fallback_tracer() -> TraceTelemetry:
    # Used when telemetry is disabled/unconfigured.
    return TraceTelemetry(port=None, runner_id="runner-unknown", trace_id="no-trace", base_attributes=None)


def run_live_multimodel(
    cfg: AppConfig,
    trader: TradingPort,
    account: AccountPort,
    universe: UniversePort,
    pipeline_service: ModelPipelineService,
    telemetry_provider: TelemetryProvider | None = None,
    config_path: str | None = None,
) -> None:
    """Live runmode that delegates per-symbol pipeline execution to ModelPipelineService."""

    allocator = ALLOCATORS.get(cfg.application.portfolio.allocator)
    if not allocator:
        raise RuntimeError(f"Allocator not found: {cfg.application.portfolio.allocator!r}")

    balancer = BALANCERS.get(cfg.application.portfolio.balancer)
    if not balancer:
        raise RuntimeError(f"Balancer not found: {cfg.application.portfolio.balancer!r}")

    pipeline_config = build_pipeline_config(cfg)

    def step_once() -> None:
        t = (
            telemetry_provider.new_trace(base_attributes={"component": "runmode"})
            if telemetry_provider
            else _fallback_tracer()
        )

        try:
            cfg_hash = stable_config_hash(cfg.model_dump())
        except Exception:
            cfg_hash = ""

        with t.span(
            "run",
            channel="ops",
            level=TelemetryLevel.INFO,
            attributes={"run_mode": "live_multimodel"},
            data={
                "timeframe": cfg.timeframe,
                "lookback_days": int(cfg.lookback_days),
                "pipeline_stages": [st.name for st in pipeline_config.stages],
                "config_hash": cfg_hash,
                "config_path": config_path,
            },
        ):

            # 1) Structural universe from coverage + ensure held symbols are included
            with t.span("coverage.fetch", channel="ops", level=TelemetryLevel.INFO):
                coverage = build_coverage(cfg, pipeline_service.market_data, universe)
                portfolio = _build_portfolio_snapshot(account)
                held_symbols = set(portfolio.positions.keys())
                universe_symbols = sorted(set(coverage) | held_symbols)

                t.emit(
                    name="coverage.summary",
                    channel="ops",
                    level=TelemetryLevel.INFO,
                    data={
                        "symbols_count": int(len(universe_symbols)),
                        "symbols_sample": universe_symbols[: min(10, len(universe_symbols))],
                    },
                )

            # 2) Run pipeline (single entrypoint)
            result = pipeline_service.run(
                universe_symbols=universe_symbols,
                portfolio_snapshot=portfolio,
                pipeline_config=pipeline_config,
                tracer=t,
            )

            t.emit(
                name="pipeline.run_summary",
                channel="ops",
                level=TelemetryLevel.INFO,
                data={"stage_stats": dict(result.stage_stats or {})},
            )

            # 3) Allocation -> target weights
            with t.span("allocator", channel="ops", level=TelemetryLevel.INFO):
                target_alloc = allocator.allocate(result.signals_by_symbol)

            # 4) Balancing -> rebalance plan
            with t.span("balancer", channel="ops", level=TelemetryLevel.INFO):
                plan = balancer.plan(
                    portfolio=portfolio,
                    target=target_alloc,
                    threshold=cfg.application.portfolio.threshold_weight,
                )
                t.emit(
                    name="rebalance.plan_built",
                    channel="ops",
                    level=TelemetryLevel.INFO,
                    data={"instructions_count": int(len(plan))},
                )

            # 5) Orders -> execution
            with t.span("execution", channel="ops", level=TelemetryLevel.INFO):
                orders = build_orders(portfolio, plan, cfg.trading)
                t.emit(
                    name="orders.built",
                    channel="ops",
                    level=TelemetryLevel.INFO,
                    data={"orders_count": int(len(orders))},
                )

                for od in orders:
                    if od.side.upper() == "BUY":
                        res = trader.market_buy(od.symbol, volume=od.volume)
                    else:
                        res = trader.market_sell(od.symbol, volume=od.volume)
                    t.emit(
                        name="trade.executed",
                        channel="ops",
                        level=TelemetryLevel.INFO,
                        attributes={"symbol": od.symbol},
                        data={"side": od.side, "volume": float(od.volume), "result": str(res)},
                    )

        if telemetry_provider:
            telemetry_provider.flush()

    if cfg.application.schedule.run_forever:
        while True:
            try:
                step_once()
                time.sleep(max(1, cfg.application.schedule.interval_seconds))
            except KeyboardInterrupt:
                t = (
                    telemetry_provider.new_trace(base_attributes={"component": "runmode"})
                    if telemetry_provider
                    else _fallback_tracer()
                )
                t.emit(
                    name="run.stopped",
                    channel="ops",
                    level=TelemetryLevel.INFO,
                    attributes={"run_mode": "live_multimodel"},
                    data={"reason": "KeyboardInterrupt"},
                )
                if telemetry_provider:
                    telemetry_provider.flush()
                break
            except Exception as e:
                t = (
                    telemetry_provider.new_trace(base_attributes={"component": "runmode"})
                    if telemetry_provider
                    else _fallback_tracer()
                )
                t.emit(
                    name="error.exception",
                    channel="ops",
                    level=TelemetryLevel.ERROR,
                    attributes={"run_mode": "live_multimodel"},
                    data={"exception_type": type(e).__name__, "message": str(e)},
                )
                if telemetry_provider:
                    telemetry_provider.flush()
                time.sleep(3)
    else:
        step_once()

--- src\tycherion\application\runmodes\live_multimodel.py:END ---

--- src\tycherion\application\services\coverage_selector.py:START ---
from __future__ import annotations

from tycherion.shared.config import AppConfig
from tycherion.ports.market_data import MarketDataPort
from tycherion.ports.universe import UniversePort


def _build_base_coverage(cfg: AppConfig, universe: UniversePort) -> list[str]:
    """Build the *structural* universe of symbols.

    Coverage is intentionally dumb. It only answers: *which* symbols should be
    considered, based on the configured source. Any kind of "smart filtering"
    (liquidity, regimes, sanity checks, alpha, etc.) must live in the model
    pipeline, not here.
    """
    src = (cfg.application.coverage.source or "").lower()
    if src == "static":
        # Remove duplicates while preserving order
        return list(dict.fromkeys(cfg.application.coverage.symbols or []))
    if src == "market_watch":
        return universe.visible_symbols()
    if src == "pattern":
        patt = cfg.application.coverage.pattern or "*"
        return universe.by_pattern(patt)
    return universe.visible_symbols()


def build_coverage(cfg: AppConfig, data: MarketDataPort, universe: UniversePort) -> list[str]:
    """Build the list of symbols to analyse in this run.

    NOTE: `data` is kept in the signature for backward compatibility, but is
    intentionally unused. The universe thinning that previously depended on
    recent `tick_volume` (coverage.top_n) is deprecated and removed.
    """
    _ = data  # explicit unused
    return _build_base_coverage(cfg, universe)

--- src\tycherion\application\services\coverage_selector.py:END ---

--- src\tycherion\application\services\ensemble.py:START ---
# application/services/ensemble.py (versão nova)

from __future__ import annotations

from typing import List
from tycherion.domain.signals.entities import ModelDecision, AggregatedDecision


def combine(decisions: List[ModelDecision]) -> AggregatedDecision:
    """
    Combina uma lista de ModelDecision em uma decisão agregada única.
    """
    if not decisions:
        return AggregatedDecision(
            side="HOLD",
            weight=0.0,
            confidence=0.0,
            signed=0.0,
        )

    num, den = 0.0, 0.0
    for d in decisions:
        side = (d.side or "HOLD").upper()
        w = float(d.weight)
        c = float(d.confidence if d.confidence is not None else 0.5)
        c = max(0.0, min(1.0, c))

        if side == "BUY":
            signed = w
        elif side == "SELL":
            signed = -w
        else:
            signed = 0.0

        num += signed * c
        den += c

    if den <= 0:
        return AggregatedDecision(
            side="HOLD",
            weight=0.0,
            confidence=0.0,
            signed=0.0,
        )

    s = num / den
    side = "BUY" if s > 0.1 else ("SELL" if s < -0.1 else "HOLD")
    weight = min(1.0, abs(s))
    confidence = min(1.0, den / max(1, len(decisions)))

    return AggregatedDecision(
        side=side,
        weight=weight,
        confidence=confidence,
        signed=s,
    )


--- src\tycherion\application\services\ensemble.py:END ---

--- src\tycherion\application\services\order_planner.py:START ---
from __future__ import annotations

from dataclasses import dataclass
from typing import List

from tycherion.domain.portfolio.entities import PortfolioSnapshot, RebalanceInstruction
from tycherion.shared.config import Trading


@dataclass
class SuggestedOrder:
    symbol: str
    side: str   # "BUY" | "SELL"
    volume: float


def build_orders(
    portfolio: PortfolioSnapshot,
    plan: List[RebalanceInstruction],
    trading_cfg: Trading,
) -> List[SuggestedOrder]:
    """
    Convert domain-level rebalance instructions (expressed in weights) into
    concrete order suggestions with broker volumes. This is the point where
    we cross from the pure portfolio domain into broker-specific constraints.
    """
    # Lazy import to avoid circular deps
    from tycherion.application.services.sizer import (
        volume_from_weight,
        symbol_min_volume,
    )

    orders: List[SuggestedOrder] = []
    for instr in plan:
        # For now we scale volumes solely by absolute delta_weight. In the
        # future this can incorporate volatility, risk, etc.
        w = abs(float(instr.delta_weight))
        if w <= 0.0:
            continue

        vol = volume_from_weight(
            instr.symbol,
            w,
            trading_cfg.volume_mode,
            trading_cfg.fixed_volume,
        )
        min_vol = symbol_min_volume(instr.symbol)
        vol = max(vol, min_vol)
        if vol <= 0.0:
            continue

        orders.append(
            SuggestedOrder(
                symbol=instr.symbol,
                side=instr.side,
                volume=vol,
            )
        )
    return orders


--- src\tycherion\application\services\order_planner.py:END ---

--- src\tycherion\application\services\sizer.py:START ---
from __future__ import annotations
import MetaTrader5 as mt5

def symbol_min_volume(symbol: str) -> float:
    info = mt5.symbol_info(symbol)
    if not info:
        return 0.0
    v = max(info.volume_min, info.volume_step)
    steps = round(v / info.volume_step)
    return steps * info.volume_step

def volume_from_weight(symbol: str, weight: float, mode: str, fixed_volume: float) -> float:
    weight = max(0.0, min(1.0, float(weight)))
    if weight < 1e-6:
        return 0.0
    if mode == 'fixed':
        return float(fixed_volume) * weight
    return symbol_min_volume(symbol)


--- src\tycherion\application\services\sizer.py:END ---

--- src\tycherion\application\telemetry\event_factory.py:START ---
from __future__ import annotations

from datetime import datetime, timezone
from typing import Any, Mapping

from tycherion.ports.telemetry import TelemetryEvent, TelemetryLevel


def now_utc() -> datetime:
    return datetime.now(timezone.utc)


def make_event(
    *,
    schema_version: int,
    runner_id: str,
    trace_id: str,
    event_seq: int,
    ts_utc: datetime | None = None,
    mono_ns: int | None = None,
    span_id: str | None = None,
    parent_span_id: str | None = None,
    name: str,
    level: str | TelemetryLevel,
    channel: str,
    attributes: Mapping[str, Any] | None = None,
    data: Mapping[str, Any] | None = None,
) -> TelemetryEvent:
    return TelemetryEvent(
        schema_version=int(schema_version),
        runner_id=str(runner_id),
        trace_id=str(trace_id),
        event_seq=int(event_seq),
        ts_utc=(ts_utc or now_utc()),
        mono_ns=(int(mono_ns) if mono_ns is not None else None),
        span_id=(str(span_id) if span_id is not None else None),
        parent_span_id=(str(parent_span_id) if parent_span_id is not None else None),
        name=str(name),
        level=TelemetryLevel.coerce(level),
        channel=str(channel),
        attributes=(dict(attributes or {}) if attributes is not None else None),
        data=dict(data or {}),
    )

--- src\tycherion\application\telemetry\event_factory.py:END ---

--- src\tycherion\application\telemetry\hub.py:START ---
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Mapping

from tycherion.ports.telemetry import TelemetryEvent, TelemetryLevel, TelemetryPort, TelemetrySink


@dataclass(slots=True)
class TelemetryHub(TelemetryPort):
    """Fan-out hub.

    The hub is application-layer infrastructure: it receives canonical
    TelemetryEvent envelopes and forwards them to sinks. Sinks can filter
    independently.
    """

    sinks: list[TelemetrySink]
    base_attributes: Mapping[str, Any] | None = None

    def emit(self, event: TelemetryEvent) -> None:
        attributes = dict(self.base_attributes or {})
        if event.attributes:
            attributes.update(dict(event.attributes))
        merged = TelemetryEvent(
            schema_version=event.schema_version,
            runner_id=event.runner_id,
            trace_id=event.trace_id,
            event_seq=event.event_seq,
            ts_utc=event.ts_utc,
            mono_ns=event.mono_ns,
            span_id=event.span_id,
            parent_span_id=event.parent_span_id,
            name=event.name,
            level=event.level,
            channel=event.channel,
            attributes=attributes if attributes else None,
            data=dict(event.data or {}),
        )

        for s in list(self.sinks):
            try:
                if not s.enabled(merged.channel, merged.level, merged.name):
                    continue
                s.emit(merged)
            except Exception:
                continue

    def enabled(self, channel: str, level: str | TelemetryLevel) -> bool:
        lv = TelemetryLevel.coerce(level)
        for s in list(self.sinks):
            try:
                if s.enabled(str(channel), lv, None):
                    return True
            except Exception:
                continue
        return False

    def child(self, attributes: Mapping[str, Any]) -> TelemetryPort:
        merged = dict(self.base_attributes or {})
        merged.update(dict(attributes or {}))
        return TelemetryHub(sinks=self.sinks, base_attributes=merged)

    def flush(self) -> None:
        for s in list(self.sinks):
            flush = getattr(s, "flush", None)
            if callable(flush):
                try:
                    flush()
                except Exception:
                    continue

    def close(self) -> None:
        for s in list(self.sinks):
            close = getattr(s, "close", None)
            if callable(close):
                try:
                    close()
                except Exception:
                    continue

--- src\tycherion\application\telemetry\hub.py:END ---

--- src\tycherion\application\telemetry\ids.py:START ---
from __future__ import annotations

import re
from datetime import datetime, timezone


_RUNNER_SAFE_RE = re.compile(r"[^a-zA-Z0-9._-]+")


def sanitize_runner_id(runner_id: str) -> str:
    """Make runner_id safe to embed into trace_id strings.

    We keep this intentionally conservative: only alnum, dot, underscore, dash.
    """

    runner_id = (runner_id or "").strip()
    if not runner_id:
        return "runner-unknown"
    runner_id = _RUNNER_SAFE_RE.sub("_", runner_id)
    return runner_id[:80]  # keep IDs compact


def format_ts_compact(ts_utc: datetime) -> str:
    """UTC timestamp as YYYYMMDDHHMMSSffffff (microseconds)."""

    if ts_utc.tzinfo is None:
        ts_utc = ts_utc.replace(tzinfo=timezone.utc)
    ts_utc = ts_utc.astimezone(timezone.utc)
    return ts_utc.strftime("%Y%m%d%H%M%S%f")


def make_trace_id(runner_id: str, ts_utc: datetime, trace_seq: int) -> str:
    runner_id = sanitize_runner_id(runner_id)
    return f"{runner_id}-{format_ts_compact(ts_utc)}-{trace_seq:04x}"


def make_span_id(span_seq: int) -> str:
    return f"{span_seq:016x}"

--- src\tycherion\application\telemetry\ids.py:END ---

--- src\tycherion\application\telemetry\provider.py:START ---
from __future__ import annotations

import itertools
from dataclasses import dataclass
from typing import Any, Mapping

from tycherion.application.telemetry.event_factory import now_utc
from tycherion.application.telemetry.ids import make_trace_id, sanitize_runner_id
from tycherion.application.telemetry.trace import TraceTelemetry
from tycherion.ports.telemetry import TelemetryPort


@dataclass(slots=True)
class TelemetryProvider:
    """Factory for per-run TraceTelemetry objects.

    Centralising trace creation prevents each runmode from inventing its own trace_id
    and keeps ID format consistent across the system.
    """

    runner_id: str
    hub: TelemetryPort
    _trace_counter: Any = None

    def __post_init__(self) -> None:
        self.runner_id = sanitize_runner_id(self.runner_id)
        # start at 0, so the first next() is 1
        self._trace_counter = itertools.count(1)

    def new_trace(self, base_attributes: Mapping[str, Any] | None = None) -> TraceTelemetry:
        ts = now_utc()
        seq = int(next(self._trace_counter))
        trace_id = make_trace_id(self.runner_id, ts, seq)
        return TraceTelemetry(
            port=self.hub,
            runner_id=self.runner_id,
            trace_id=trace_id,
            base_attributes=(dict(base_attributes or {}) if base_attributes else None),
        )

    def flush(self) -> None:
        try:
            self.hub.flush()
        except Exception:
            return

    def close(self) -> None:
        try:
            self.hub.close()
        except Exception:
            return

--- src\tycherion\application\telemetry\provider.py:END ---

--- src\tycherion\application\telemetry\trace.py:START ---
from __future__ import annotations

import hashlib
import json
import time
import threading
from contextvars import ContextVar
from dataclasses import dataclass
from typing import Any, Mapping

from tycherion.application.telemetry.event_factory import make_event, now_utc
from tycherion.application.telemetry.ids import make_span_id
from tycherion.ports.telemetry import TelemetryEvent, TelemetryLevel, TelemetryPort

# Bump when TelemetryEvent fields change in a backwards-incompatible way.
SCHEMA_VERSION = 3

# Per-task span stack (works for sync, async, nested contexts)
_SPAN_STACK: ContextVar[tuple[str, ...]] = ContextVar("tycherion_span_stack", default=())


def stable_config_hash(
    cfg_dump: Mapping[str, Any],
    *,
    redactions: Mapping[str, set[str]] | None = None,
) -> str:
    """Best-effort hash of a config dump, with sensitive fields redacted.

    `cfg_dump` is expected to be the output of pydantic's `model_dump()`.
    """

    redactions = redactions or {"mt5": {"password", "login", "server"}}

    payload = json.loads(json.dumps(cfg_dump, default=str))  # ensure JSON-ish types
    for section, keys in redactions.items():
        node = payload.get(section)
        if isinstance(node, dict):
            for k in keys:
                if k in node:
                    node[k] = "***"

    raw = json.dumps(payload, sort_keys=True, separators=(",", ":"))
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()


class _TraceState:
    """Mutable, shared state for one trace (shared across child tracers)."""

    __slots__ = ("span_seq", "event_seq", "_lock")

    def __init__(self) -> None:
        self.span_seq = 0
        self.event_seq = 0
        self._lock = threading.Lock()

    def next_span_id(self) -> str:
        with self._lock:
            self.span_seq += 1
            return make_span_id(self.span_seq)

    def next_event_seq(self) -> int:
        with self._lock:
            self.event_seq += 1
            return int(self.event_seq)


@dataclass(frozen=True, slots=True)
class TraceTelemetry:
    """A lightweight tracer aligned with standard observability vocabulary.

    This is intentionally NOT OpenTelemetry SDK. It's a small abstraction that
    keeps the mental model: runner_id + trace_id + spans + structured events.
    """

    port: TelemetryPort | None
    runner_id: str
    trace_id: str
    base_attributes: Mapping[str, Any] | None = None
    _state: Any | None = None

    def __post_init__(self) -> None:
        # dataclass(frozen=True): use object.__setattr__
        if self._state is None:
            object.__setattr__(self, "_state", _TraceState())

    def enabled(self, channel: str, level: str | TelemetryLevel) -> bool:
        try:
            return bool(self.port and self.port.enabled(channel, level))
        except Exception:
            return False

    def child(self, attributes: Mapping[str, Any]) -> "TraceTelemetry":
        merged = dict(self.base_attributes or {})
        merged.update(dict(attributes or {}))
        return TraceTelemetry(
            port=self.port,
            runner_id=self.runner_id,
            trace_id=self.trace_id,
            base_attributes=merged,
            _state=self._state,
        )

    def _emit_built(self, ev: TelemetryEvent) -> None:
        if not self.port:
            return
        try:
            self.port.emit(ev)
        except Exception:
            return

    def emit(
        self,
        *,
        name: str,
        level: str | TelemetryLevel,
        channel: str,
        attributes: Mapping[str, Any] | None = None,
        data: Mapping[str, Any] | None = None,
        span_id: str | None = None,
        parent_span_id: str | None = None,
        mono_ns: int | None = None,
        ts_utc: Any | None = None,
    ) -> None:
        if not self.port:
            return

        lvl = TelemetryLevel.coerce(level)
        # Gating first: do not even increment event_seq if nobody is listening.
        try:
            if not self.port.enabled(channel, lvl):
                return
        except Exception:
            return

        try:
            event_seq = self._state.next_event_seq()  # type: ignore[union-attr]
            base = dict(self.base_attributes or {})
            if attributes:
                base.update(dict(attributes))

            stack = _SPAN_STACK.get()
            resolved_span_id = span_id if span_id is not None else (stack[-1] if stack else None)
            resolved_parent_span_id = (
                parent_span_id
                if parent_span_id is not None
                else (stack[-2] if len(stack) >= 2 else None)
            )

            ev = make_event(
                schema_version=SCHEMA_VERSION,
                runner_id=self.runner_id,
                trace_id=self.trace_id,
                event_seq=event_seq,
                ts_utc=(ts_utc or now_utc()),
                mono_ns=mono_ns,
                span_id=resolved_span_id,
                parent_span_id=resolved_parent_span_id,
                name=name,
                level=lvl,
                channel=channel,
                attributes=base if base else None,
                data=data or {},
            )
            self._emit_built(ev)
        except Exception:
            return

    def span(
        self,
        name: str,
        *,
        channel: str = "ops",
        level: str | TelemetryLevel = TelemetryLevel.INFO,
        attributes: Mapping[str, Any] | None = None,
        data: Mapping[str, Any] | None = None,
    ) -> "Span":
        return Span(
            tracer=self,
            name=str(name),
            channel=str(channel),
            level=TelemetryLevel.coerce(level),
            attributes=dict(attributes or {}) if attributes else None,
            data=dict(data or {}) if data else None,
        )


@dataclass(slots=True)
class Span:
    tracer: TraceTelemetry
    name: str
    channel: str
    level: TelemetryLevel
    attributes: Mapping[str, Any] | None
    data: Mapping[str, Any] | None

    span_id: str | None = None
    parent_span_id: str | None = None
    _token: Any | None = None
    _t0_mono: int | None = None

    def __enter__(self) -> "Span":
        self._t0_mono = time.monotonic_ns()

        before = _SPAN_STACK.get()
        self.parent_span_id = before[-1] if before else None
        self.span_id = self.tracer._state.next_span_id()  # type: ignore[union-attr]

        self._token = _SPAN_STACK.set(before + (self.span_id,))

        self.tracer.emit(
            name=f"{self.name}.started",
            level=self.level,
            channel=self.channel,
            attributes=self.attributes,
            data=self.data,
            span_id=self.span_id,
            parent_span_id=self.parent_span_id,
            mono_ns=self._t0_mono,
        )
        return self

    def __exit__(self, exc_type, exc, tb) -> bool:
        try:
            t1 = time.monotonic_ns()
            duration_ms = int(round((t1 - int(self._t0_mono or t1)) / 1_000_000))

            if exc is None:
                self.tracer.emit(
                    name=f"{self.name}.finished",
                    level=TelemetryLevel.INFO,
                    channel=self.channel,
                    attributes=self.attributes,
                    data={"duration_ms": duration_ms, "status": "ok"},
                    span_id=self.span_id,
                    parent_span_id=self.parent_span_id,
                    mono_ns=t1,
                )
                return False

            self.tracer.emit(
                name=f"{self.name}.failed",
                level=TelemetryLevel.ERROR,
                channel=self.channel,
                attributes=self.attributes,
                data={
                    "duration_ms": duration_ms,
                    "status": "error",
                    "exception_type": getattr(exc_type, "__name__", str(exc_type)),
                    "message": str(exc),
                },
                span_id=self.span_id,
                parent_span_id=self.parent_span_id,
                mono_ns=t1,
            )
            return False
        finally:
            try:
                if self._token is not None:
                    _SPAN_STACK.reset(self._token)
            except Exception:
                pass

--- src\tycherion\application\telemetry\trace.py:END ---

--- src\tycherion\application\telemetry\__init__.py:START ---
from .event_factory import make_event, now_utc
from .hub import TelemetryHub
from .provider import TelemetryProvider
from .trace import SCHEMA_VERSION, TraceTelemetry, Span, stable_config_hash

__all__ = [
    "make_event",
    "now_utc",
    "TelemetryHub",
    "TelemetryProvider",
    "TraceTelemetry",
    "Span",
    "stable_config_hash",
    "SCHEMA_VERSION",
]

--- src\tycherion\application\telemetry\__init__.py:END ---

--- src\tycherion\bootstrap\main.py:START ---
from __future__ import annotations

import os
import socket
from pathlib import Path

import MetaTrader5 as mt5

from tycherion.shared.config import load_config, AppConfig
from tycherion.adapters.mt5.market_data_mt5 import MT5MarketData
from tycherion.adapters.mt5.trading_mt5 import MT5Trader
from tycherion.adapters.mt5.account_mt5 import MT5Account
from tycherion.adapters.mt5.universe_mt5 import MT5Universe

from tycherion.adapters.telemetry.db_journal import DbExecutionJournalSink
from tycherion.adapters.telemetry.mongo_journal import MongoExecutionJournalSink
from tycherion.adapters.telemetry.console import ConsoleTelemetrySink

from tycherion.application.telemetry import TelemetryHub, TelemetryProvider
from tycherion.ports.telemetry import TelemetryLevel

from tycherion.application.plugins import registry as _registry
from tycherion.application.pipeline.service import ModelPipelineService
from tycherion.application.runmodes.live_multimodel import run_live_multimodel


def _ensure_initialized(cfg: AppConfig) -> None:
    if not mt5.initialize(path=cfg.mt5.terminal_path or None):
        raise SystemExit(f"MT5 initialize failed: {mt5.last_error()}")
    if cfg.mt5.login and cfg.mt5.password and cfg.mt5.server:
        if not mt5.login(
            login=int(cfg.mt5.login),
            password=cfg.mt5.password,
            server=cfg.mt5.server,
        ):
            raise SystemExit(f"MT5 login failed: {mt5.last_error()}")


def run_app(config_path: str) -> None:
    cfg = load_config(config_path)

    # Telemetry must be available as early as possible (e.g. plugin discovery).
    provider = _build_telemetry(cfg, config_path)

    # Discover all indicators, models, allocators and balancers
    bootstrap_tracer = provider.new_trace(base_attributes={"component": "bootstrap"})
    with bootstrap_tracer.span("bootstrap.discover", channel="ops", level=TelemetryLevel.INFO):
        _registry.auto_discover(bootstrap_tracer)

    _ensure_initialized(cfg)
    try:
        market_data = MT5MarketData()
        trader = MT5Trader(
            dry_run=cfg.trading.dry_run,
            require_demo=cfg.trading.require_demo,
            deviation_points=cfg.trading.deviation_points,
            volume_mode=cfg.trading.volume_mode,
            fixed_volume=cfg.trading.fixed_volume,
        )
        account = MT5Account()
        universe = MT5Universe()

        pipeline_service = ModelPipelineService(
            market_data=market_data,
            model_registry=_registry.MODELS,
            indicator_picker=_registry.pick_indicator_for,
            timeframe=cfg.timeframe,
            lookback_days=cfg.lookback_days,
            playbook=cfg.application.playbook,
        )

        run_mode = (cfg.application.run_mode.name or "").lower()
        if run_mode == "live_multimodel":
            run_live_multimodel(
                cfg,
                trader,
                account,
                universe,
                pipeline_service,
                telemetry_provider=provider,
                config_path=config_path,
            )
        else:
            raise SystemExit(f"Unknown run_mode: {run_mode}")
    finally:
        try:
            provider.close()
        except Exception:
            pass
        mt5.shutdown()


def _build_telemetry(cfg: AppConfig, config_path: str) -> TelemetryProvider:
    _ = config_path

    runner_id = (os.getenv("TYCHERION_RUNNER_ID") or "").strip()
    if not runner_id:
        # Fallback: still deterministic enough for local dev.
        runner_id = f"runner-{socket.gethostname()}-{os.getpid()}"

    sinks = []
    telemetry_cfg = cfg.telemetry

    if bool(telemetry_cfg.db_enabled) and bool(telemetry_cfg.db_dsn):
        sinks.append(
            DbExecutionJournalSink(
                dsn=str(telemetry_cfg.db_dsn),
                enabled_flag=True,
                channels=set(telemetry_cfg.db_channels or ["audit", "ops"]),
                min_level=TelemetryLevel.coerce(telemetry_cfg.db_min_level),
                batch_size=int(telemetry_cfg.db_batch_size or 50),
            )
        )

    if bool(telemetry_cfg.mongo_enabled) and bool(telemetry_cfg.mongo_uri):
        sinks.append(
            MongoExecutionJournalSink(
                uri=str(telemetry_cfg.mongo_uri),
                db_name=str(telemetry_cfg.mongo_db or "tycherion"),
                collection_name=str(telemetry_cfg.mongo_collection or "execution_journal_events"),
                enabled_flag=True,
                channels=set(telemetry_cfg.mongo_channels or ["audit", "ops"]),
                min_level=TelemetryLevel.coerce(telemetry_cfg.mongo_min_level),
                batch_size=int(telemetry_cfg.mongo_batch_size or 200),
            )
        )

    if bool(telemetry_cfg.console_enabled):
        sinks.append(
            ConsoleTelemetrySink(
                enabled_flag=True,
                channels=set(telemetry_cfg.console_channels or ["ops"]),
                min_level=TelemetryLevel.coerce(telemetry_cfg.console_min_level),
            )
        )

    hub = TelemetryHub(sinks=sinks)
    return TelemetryProvider(runner_id=runner_id, hub=hub)

--- src\tycherion\bootstrap\main.py:END ---

--- src\tycherion\domain\__init__.py:START ---

--- src\tycherion\domain\__init__.py:END ---

--- src\tycherion\domain\market\entities.py:START ---
from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import NewType

Symbol = NewType("Symbol", str)


class AssetClass(str, Enum):
    EQUITY = "equity"
    FUTURE = "future"
    FX = "fx"
    OTHER = "other"


@dataclass
class Instrument:
    """Domain representation of a tradable instrument (stock, future, FX, etc.)."""

    symbol: Symbol
    asset_class: AssetClass
    currency: str
    lot_size: float
    min_volume: float
    volume_step: float


@dataclass
class Bar:
    """Minimal OHLCV bar used by indicators and models when not using DataFrame."""

    symbol: Symbol
    time: datetime
    open: float
    high: float
    low: float
    close: float
    volume: float

--- src\tycherion\domain\market\entities.py:END ---

--- src\tycherion\domain\market\__init__.py:START ---

--- src\tycherion\domain\market\__init__.py:END ---

--- src\tycherion\domain\portfolio\entities.py:START ---
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict


Symbol = str


@dataclass
class Signal:
    """Per-symbol signal produced by the models/ensemble.

    signed: desired direction/intensity in [-1, 1]
    confidence: optional confidence level in [0, 1]
    """

    symbol: Symbol
    signed: float
    confidence: float = 1.0


SignalsBySymbol = Dict[Symbol, Signal]


@dataclass
class Position:
    """Domain-level position in a single instrument.

    quantity: number of shares/contracts/etc.
    price: best available price estimate (e.g. last close or avg price).
    """

    symbol: Symbol
    quantity: float
    price: float


@dataclass
class PortfolioSnapshot:
    """Portfolio snapshot used by allocators/balancers at the domain level.

    Equity is the current account equity in account currency.
    """

    equity: float
    positions: Dict[Symbol, Position]

    def weight_of(self, symbol: Symbol) -> float:
        pos = self.positions.get(symbol)
        if not pos or self.equity <= 0:
            return 0.0
        return float(pos.quantity * pos.price) / float(self.equity)


@dataclass
class TargetAllocation:
    """Target portfolio allocation expressed as weights per symbol in [-1, 1].

    Positive weights are long exposure, negative weights are short exposure.
    """

    weights: Dict[Symbol, float]


@dataclass
class RebalanceInstruction:
    """Domain-level rebalance instruction expressed in weights, not broker
    volumes. Conversion to concrete order sizes happens in the application
    layer (order planner).
    """

    symbol: Symbol
    from_weight: float
    to_weight: float
    delta_weight: float
    side: str  # "BUY" | "SELL"

--- src\tycherion\domain\portfolio\entities.py:END ---

--- src\tycherion\domain\portfolio\__init__.py:START ---

--- src\tycherion\domain\portfolio\__init__.py:END ---

--- src\tycherion\domain\portfolio\allocators\base.py:START ---
from __future__ import annotations

from abc import ABC, abstractmethod

from tycherion.domain.portfolio.entities import SignalsBySymbol, TargetAllocation


class BaseAllocator(ABC):
    """Abstract base class for portfolio allocator plugins."""

    # Set by decorator
    name: str = ""
    tags: set[str] = set()

    @abstractmethod
    def allocate(self, signals: SignalsBySymbol) -> TargetAllocation:
        raise NotImplementedError

--- src\tycherion\domain\portfolio\allocators\base.py:END ---

--- src\tycherion\domain\portfolio\allocators\equal_weight.py:START ---
from __future__ import annotations

from tycherion.domain.portfolio.allocators.base import BaseAllocator
from tycherion.application.plugins.registry import register_allocator
from tycherion.domain.portfolio.entities import SignalsBySymbol, TargetAllocation


@register_allocator(name="equal_weight", tags={"default"})
class EqualWeightAllocator(BaseAllocator):
    """
    Simple allocator: gives the same absolute weight to all symbols that have
    a non-zero signal. Longs get +w, shorts get -w, holds get 0.
    """
    def allocate(self, signals: SignalsBySymbol) -> TargetAllocation:
        nonzero = [s for s in signals.values() if abs(float(s.signed)) > 1e-6]
        if not nonzero:
            # nothing to do
            return TargetAllocation(weights={})

        w = 1.0 / float(len(nonzero))
        weights: dict[str, float] = {}
        for sig in signals.values():
            if sig.signed > 0:
                weights[sig.symbol] = w
            elif sig.signed < 0:
                weights[sig.symbol] = -w
            else:
                weights[sig.symbol] = 0.0
        return TargetAllocation(weights=weights)

--- src\tycherion\domain\portfolio\allocators\equal_weight.py:END ---

--- src\tycherion\domain\portfolio\allocators\proportional.py:START ---
from __future__ import annotations

from tycherion.domain.portfolio.allocators.base import BaseAllocator
from tycherion.application.plugins.registry import register_allocator
from tycherion.domain.portfolio.entities import SignalsBySymbol, TargetAllocation


@register_allocator(name="proportional", tags={"default"})
class ProportionalAllocator(BaseAllocator):
    """
    Allocator that gives each symbol a weight proportional to the absolute
    value of its signal. Signals are normalised so that the sum of absolute
    weights is 1. Longs get +w, shorts get -w.
    """
    def allocate(self, signals: SignalsBySymbol) -> TargetAllocation:
        total = sum(abs(float(s.signed)) for s in signals.values())
        if total <= 1e-9:
            return TargetAllocation(weights={})

        weights: dict[str, float] = {}
        for sig in signals.values():
            if sig.signed == 0:
                weights[sig.symbol] = 0.0
            else:
                frac = abs(float(sig.signed)) / total
                weights[sig.symbol] = frac if sig.signed > 0 else -frac
        return TargetAllocation(weights=weights)

--- src\tycherion\domain\portfolio\allocators\proportional.py:END ---

--- src\tycherion\domain\portfolio\allocators\__init__.py:START ---

--- src\tycherion\domain\portfolio\allocators\__init__.py:END ---

--- src\tycherion\domain\portfolio\balancers\base.py:START ---
from __future__ import annotations

from abc import ABC, abstractmethod

from tycherion.domain.portfolio.entities import (
    PortfolioSnapshot,
    TargetAllocation,
    RebalanceInstruction,
)


class BaseBalancer(ABC):
    """Abstract base class for portfolio balancer / rebalancer plugins."""

    # Set by decorator
    name: str = ""
    tags: set[str] = set()

    @abstractmethod
    def plan(
        self,
        portfolio: PortfolioSnapshot,
        target: TargetAllocation,
        threshold: float = 0.25,
    ) -> list[RebalanceInstruction]:
        raise NotImplementedError

--- src\tycherion\domain\portfolio\balancers\base.py:END ---

--- src\tycherion\domain\portfolio\balancers\threshold.py:START ---
from __future__ import annotations

from tycherion.domain.portfolio.balancers.base import BaseBalancer
from tycherion.application.plugins.registry import register_balancer
from tycherion.domain.portfolio.entities import (
    PortfolioSnapshot,
    TargetAllocation,
    RebalanceInstruction,
)


@register_balancer(name="threshold", tags={"default"})
class ThresholdBalancer(BaseBalancer):
    """
    Domain-level balancer: generates rebalance instructions whenever the
    difference between current and target weight is greater than or equal
    to a configured threshold.
    """
    def plan(
        self,
        portfolio: PortfolioSnapshot,
        target: TargetAllocation,
        threshold: float = 0.25,
    ) -> list[RebalanceInstruction]:
        threshold = max(0.0, min(1.0, float(threshold)))
        instructions: list[RebalanceInstruction] = []

        symbols = set(target.weights.keys()) | set(portfolio.positions.keys())
        for sym in sorted(symbols):
            current_w = float(portfolio.weight_of(sym))
            target_w = float(target.weights.get(sym, 0.0))
            delta = target_w - current_w
            if abs(delta) < threshold:
                continue
            side = "BUY" if delta > 0 else "SELL"
            instructions.append(
                RebalanceInstruction(
                    symbol=sym,
                    from_weight=current_w,
                    to_weight=target_w,
                    delta_weight=delta,
                    side=side,
                )
            )
        return instructions

--- src\tycherion\domain\portfolio\balancers\threshold.py:END ---

--- src\tycherion\domain\portfolio\balancers\__init__.py:START ---

--- src\tycherion\domain\portfolio\balancers\__init__.py:END ---

--- src\tycherion\domain\signals\entities.py:START ---
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, List


@dataclass
class IndicatorOutput:
    """Standard output of an indicator for a single symbol.

    - score: aggregated metric in [-1, 1] (by convention in this project)
    - features: extra numeric features that models may consume.
    """

    score: float
    features: Dict[str, float]


@dataclass
class ModelDecision:
    """Per-model decision for a single symbol.

    side: "BUY" | "SELL" | "HOLD"
    weight: relative intensity (usually in [0, 1])
    confidence: confidence level in [0, 1]
    """

    side: str
    weight: float
    confidence: float

@dataclass
class AggregatedDecision:
    """
    Decisão agregada (ensemble) de todos os models para um símbolo.

    side       -> direção final ("BUY"/"SELL"/"HOLD")
    weight     -> intensidade em [0, 1]
    confidence -> confiança em [0, 1]
    signed     -> direção * intensidade em [-1, 1]
    """
    side: str
    weight: float
    confidence: float
    signed: float


@dataclass
class ModelStageResult:
    """Result for a symbol at a specific model stage in the pipeline."""

    model_name: str
    score: float


@dataclass
class SymbolState:
    """Mutable per-symbol state that flows through the analysis pipeline.

    This is intentionally generic so we can reuse it for universe filters,
    macro models and per-symbol alpha models over time.
    """
    symbol: str
    is_held: bool = False      # True if the symbol is currently in the portfolio
    alive: bool = True         # If False and not held, the symbol can be dropped from the pipeline

    base_score: float = 0.0    # Optional starting score (e.g. from simple filters)
    sanity_score: float = 0.0  # Data-quality / tradability / liquidity score
    macro_score: float = 0.0   # Macro / regime score for this symbol
    alpha_score: float = 0.0   # Final alpha-like score, typically coming from signal models

    pipeline_results: List[ModelStageResult] = field(default_factory=list)

    notes: Dict[str, float] = field(default_factory=dict)


--- src\tycherion\domain\signals\entities.py:END ---

--- src\tycherion\domain\signals\__init__.py:START ---

--- src\tycherion\domain\signals\__init__.py:END ---

--- src\tycherion\domain\signals\indicators\base.py:START ---
from __future__ import annotations

from abc import ABC, abstractmethod
import pandas as pd

from tycherion.domain.signals.entities import IndicatorOutput


class BaseIndicator(ABC):
    """Abstract base class for indicator plugins."""

    # Set by decorator
    key: str = ""
    method: str = ""
    tags: set[str] = set()

    @abstractmethod
    def compute(self, df: pd.DataFrame) -> IndicatorOutput:
        raise NotImplementedError

--- src\tycherion\domain\signals\indicators\base.py:END ---

--- src\tycherion\domain\signals\indicators\stretch_zscore.py:START ---
from __future__ import annotations

from tycherion.domain.signals.indicators.base import BaseIndicator
import pandas as pd

from tycherion.application.plugins.registry import register_indicator
from tycherion.domain.signals.entities import IndicatorOutput


@register_indicator(key="stretch", method="zscore_20", tags={"default"})
class StretchZScore20(BaseIndicator):
    period = 20

    def compute(self, df: pd.DataFrame) -> IndicatorOutput:
        if df.empty or len(df) < self.period:
            return IndicatorOutput(score=0.0, features={})
        close = df["close"].astype(float)
        ma = close.rolling(self.period).mean()
        sd = close.rolling(self.period).std(ddof=0).replace(0, 1e-9)
        z = (close - ma) / sd
        zval = float(z.iloc[-1])
        score = max(-1.0, min(1.0, -zval / 3.0))
        return IndicatorOutput(score=score, features={"z": zval})

--- src\tycherion\domain\signals\indicators\stretch_zscore.py:END ---

--- src\tycherion\domain\signals\indicators\trend_donchian.py:START ---
from __future__ import annotations

from tycherion.domain.signals.indicators.base import BaseIndicator
import pandas as pd

from tycherion.application.plugins.registry import register_indicator
from tycherion.domain.signals.entities import IndicatorOutput


@register_indicator(key="trend", method="donchian_50_50", tags={"default"})
class TrendDonchian5050(BaseIndicator):
    high_n = 50
    low_n = 50

    def compute(self, df: pd.DataFrame) -> IndicatorOutput:
        if df.empty or len(df) < max(self.high_n, self.low_n):
            return IndicatorOutput(score=0.0, features={})
        hh = df["high"].rolling(self.high_n).max()
        ll = df["low"].rolling(self.low_n).min()
        mid = (hh + ll) / 2.0
        rng = (hh - ll).replace(0, 1e-9)
        pos = (df["close"] - mid) / (rng / 2.0)
        score = float(pos.iloc[-1])
        score = max(-1.0, min(1.0, score))
        return IndicatorOutput(
            score=score,
            features={"upper": float(hh.iloc[-1]), "lower": float(ll.iloc[-1])},
        )

--- src\tycherion\domain\signals\indicators\trend_donchian.py:END ---

--- src\tycherion\domain\signals\indicators\volatility_atr.py:START ---
from __future__ import annotations

from tycherion.domain.signals.indicators.base import BaseIndicator
import pandas as pd

from tycherion.application.plugins.registry import register_indicator
from tycherion.domain.signals.entities import IndicatorOutput


@register_indicator(key="volatility", method="atr_14", tags={"default"})
class VolATR14(BaseIndicator):
    period = 14

    def compute(self, df: pd.DataFrame) -> IndicatorOutput:
        if df.empty or len(df) < self.period + 1:
            return IndicatorOutput(score=0.0, features={})
        high = df["high"].astype(float)
        low = df["low"].astype(float)
        close = df["close"].astype(float)
        prev_close = close.shift(1)
        tr = (high - low).abs()
        tr = pd.concat(
            [tr, (high - prev_close).abs(), (low - prev_close).abs()], axis=1
        ).max(axis=1)
        atr = tr.rolling(self.period).mean()
        val = float(atr.iloc[-1])
        score = 1.0 / (1.0 + val) if val > 0 else 0.0
        return IndicatorOutput(score=score, features={"atr": val})

--- src\tycherion\domain\signals\indicators\volatility_atr.py:END ---

--- src\tycherion\domain\signals\indicators\__init__.py:START ---

--- src\tycherion\domain\signals\indicators\__init__.py:END ---

--- src\tycherion\domain\signals\models\base.py:START ---
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Dict

from tycherion.domain.signals.entities import IndicatorOutput, ModelDecision


class SignalModel(ABC):
    """Abstract base class for per-symbol signal models."""

    name: str = ""
    tags: set[str] = set()

    @abstractmethod
    def requires(self) -> set[str]:
        raise NotImplementedError

    @abstractmethod
    def decide(self, indicators: Dict[str, IndicatorOutput]) -> ModelDecision:
        raise NotImplementedError

--- src\tycherion\domain\signals\models\base.py:END ---

--- src\tycherion\domain\signals\models\mean_reversion.py:START ---
from __future__ import annotations

from tycherion.domain.signals.models.base import SignalModel
from typing import Dict

from tycherion.application.plugins.registry import register_model
from tycherion.domain.signals.entities import IndicatorOutput, ModelDecision


@register_model(name="mean_reversion", tags={"default"})
class MeanReversion(SignalModel):
    def requires(self) -> set[str]:
        return {"stretch", "volatility"}

    def decide(self, indicators: Dict[str, IndicatorOutput]) -> ModelDecision:
        stretch = indicators.get("stretch") if indicators is not None else None
        z = float(stretch.features.get("z", 0.0)) if stretch else 0.0

        if z <= -2.0:
            w = min(1.0, abs(z) / 3.0)
            return ModelDecision(side="BUY", weight=w, confidence=0.6)
        if z >= 2.0:
            w = min(1.0, abs(z) / 3.0)
            return ModelDecision(side="SELL", weight=w, confidence=0.6)
        return ModelDecision(side="HOLD", weight=0.0, confidence=0.4)


--- src\tycherion\domain\signals\models\mean_reversion.py:END ---

--- src\tycherion\domain\signals\models\trend_following.py:START ---
from __future__ import annotations

from tycherion.domain.signals.models.base import SignalModel
from typing import Dict

from tycherion.application.plugins.registry import register_model
from tycherion.domain.signals.entities import IndicatorOutput, ModelDecision


@register_model(name="trend_following", tags={"default"})
class TrendFollowing(SignalModel):
    def requires(self) -> set[str]:
        return {"trend", "volatility"}

    def decide(self, indicators: Dict[str, IndicatorOutput]) -> ModelDecision:
        trend = indicators.get("trend") if indicators is not None else None
        tr = float(trend.score) if trend else 0.0

        if tr > 0.2:
            return ModelDecision(
                side="BUY",
                weight=min(1.0, 0.5 + tr * 0.5),
                confidence=0.7,
            )
        if tr < -0.2:
            return ModelDecision(
                side="SELL",
                weight=min(1.0, 0.5 + (-tr) * 0.5),
                confidence=0.7,
            )
        return ModelDecision(side="HOLD", weight=0.0, confidence=0.3)


--- src\tycherion\domain\signals\models\trend_following.py:END ---

--- src\tycherion\domain\signals\models\__init__.py:START ---

--- src\tycherion\domain\signals\models\__init__.py:END ---

--- src\tycherion\ports\account.py:START ---
from __future__ import annotations

from typing import Protocol, List

from tycherion.domain.portfolio.entities import Position


class AccountPort(Protocol):
    def is_demo(self) -> bool: ...
    def balance(self) -> float: ...
    def equity(self) -> float: ...
    def positions(self) -> List[Position]: ...

--- src\tycherion\ports\account.py:END ---

--- src\tycherion\ports\market_data.py:START ---
from __future__ import annotations
from typing import Protocol
from datetime import datetime
import pandas as pd

class MarketDataPort(Protocol):
    def get_bars(self, symbol: str, timeframe: str, start: datetime, end: datetime) -> pd.DataFrame: ...

--- src\tycherion\ports\market_data.py:END ---

--- src\tycherion\ports\telemetry.py:START ---
from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any, Mapping, Protocol


class TelemetryLevel(str, Enum):
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARN = "WARN"
    ERROR = "ERROR"

    @classmethod
    def coerce(cls, value: str | "TelemetryLevel") -> "TelemetryLevel":
        if isinstance(value, TelemetryLevel):
            return value
        v = (value or "INFO").upper().strip()
        try:
            return TelemetryLevel(v)
        except Exception:
            return TelemetryLevel.INFO

    def rank(self) -> int:
        order = {
            TelemetryLevel.DEBUG: 10,
            TelemetryLevel.INFO: 20,
            TelemetryLevel.WARN: 30,
            TelemetryLevel.ERROR: 40,
        }
        return int(order[self])


@dataclass(frozen=True, slots=True)
class TelemetryEvent:
    """Canonical, small telemetry envelope.

    Design goals:
    - stable schema (versioned)
    - append-only journaling friendly
    - deterministic ordering within a trace (event_seq)

    NOTE: `attributes` and `data` must be JSON-serialisable.
    """

    schema_version: int
    runner_id: str
    trace_id: str

    # Order within the trace (1..N). Useful for ordering + idempotent dedupe in sinks.
    event_seq: int

    # When the event happened (audit / timeline)
    ts_utc: datetime

    # Optional monotonic timestamp for precision timing (not affected by NTP).
    mono_ns: int | None

    # Hierarchy
    span_id: str | None
    parent_span_id: str | None

    # Semantic payload
    name: str
    level: TelemetryLevel
    channel: str
    attributes: Mapping[str, Any] | None
    data: Mapping[str, Any]


class TelemetrySink(Protocol):
    """Adapter-side sink.

    A sink can filter independently. The hub will call `enabled` for gating,
    then `emit` to persist/print.
    """

    def enabled(self, channel: str, level: TelemetryLevel, name: str | None = None) -> bool: ...

    def emit(self, event: TelemetryEvent) -> None: ...


class TelemetryPort(Protocol):
    """Application-facing telemetry API (fan-out hub + scoped wrappers)."""

    def emit(self, event: TelemetryEvent) -> None: ...

    def enabled(self, channel: str, level: str | TelemetryLevel) -> bool: ...

    def child(self, attributes: Mapping[str, Any]) -> "TelemetryPort": ...

    def flush(self) -> None: ...

    def close(self) -> None: ...

--- src\tycherion\ports\telemetry.py:END ---

--- src\tycherion\ports\trading.py:START ---
from __future__ import annotations
from dataclasses import dataclass
from typing import Protocol, Optional

@dataclass
class TradeResult:
    ok: bool
    retcode: int
    order: Optional[int]
    message: str

class TradingPort(Protocol):
    def market_buy(self, symbol: str, volume: Optional[float] = None) -> TradeResult: ...
    def market_sell(self, symbol: str, volume: Optional[float] = None) -> TradeResult: ...

--- src\tycherion\ports\trading.py:END ---

--- src\tycherion\ports\universe.py:START ---
from __future__ import annotations
from typing import Protocol, List

class UniversePort(Protocol):
    def visible_symbols(self) -> List[str]: ...
    def by_pattern(self, pattern: str) -> List[str]: ...

--- src\tycherion\ports\universe.py:END ---

--- src\tycherion\shared\config.py:START ---
from __future__ import annotations
from pydantic import BaseModel, field_validator
from typing import Optional, Any
import os, yaml
from dotenv import load_dotenv

class Trading(BaseModel):
    dry_run: bool = True
    require_demo: bool = True
    deviation_points: int = 10
    volume_mode: str = "min"     # 'min' | 'fixed'
    fixed_volume: float = 0.01

class Risk(BaseModel):
    risk_per_trade_pct: float = 0.5
    max_daily_loss_pct: float = 2.0

class MT5(BaseModel):
    terminal_path: Optional[str] = None
    server: Optional[str] = None
    login: Optional[int] = None
    password: Optional[str] = None

class RunMode(BaseModel):
    name: str = "live_multimodel"

class ScheduleCfg(BaseModel):
    run_forever: bool = False
    interval_seconds: int = 60

class CoverageCfg(BaseModel):
    source: str = "market_watch"
    symbols: list[str] = []
    pattern: str | None = None


class PipelineStageCfg(BaseModel):
    """Configuration of a single stage in the model pipeline."""

    name: str
    drop_threshold: float | None = None


class ModelsCfg(BaseModel):
    """Application-level model selection.

    `pipeline` defines an ordered list of models to run per symbol. The order
    is the order of execution. Each stage can optionally define a
    `drop_threshold` used to discard non-held symbols early.
    """

    pipeline: list[PipelineStageCfg] = []

    @field_validator("pipeline", mode="before")
    @classmethod
    def _coerce_pipeline(cls, v: Any):
        # Accept both:
        # - pipeline: ["trend_following", "mean_reversion"]
        # - pipeline: [{name: "...", drop_threshold: ...}, ...]
        if v is None:
            return []
        if isinstance(v, list):
            out: list[Any] = []
            for item in v:
                if isinstance(item, str):
                    out.append({"name": item})
                else:
                    out.append(item)
            return out
        return v


class PortfolioCfg(BaseModel):
    allocator: str = "proportional"     # plugin name
    balancer: str = "threshold"         # plugin name
    threshold_weight: float = 0.25      # only rebalance if |w| >= threshold

class ApplicationCfg(BaseModel):
    run_mode: RunMode = RunMode()
    playbook: str = "default"
    schedule: ScheduleCfg = ScheduleCfg()
    coverage: CoverageCfg = CoverageCfg()
    models: ModelsCfg = ModelsCfg()
    portfolio: PortfolioCfg = PortfolioCfg()


class TelemetrySinkCfg(BaseModel):
    enabled: bool = True
    channels: list[str] = ["audit", "ops"]
    min_level: str = "INFO"  # DEBUG/INFO/WARN/ERROR


class TelemetryCfg(BaseModel):
    """Telemetry configuration (bootstrap/application concern, not domain).

    Notes about the two databases we plan to have in Tycherion:
    - PostgreSQL: for *analytics* and structured datasets (models, indicators, actions, etc.).
      The schema for this is not defined yet.
    - MongoDB: for *operational health / audit journal* (telemetry events, errors, spans, etc.).

    Right now this config focuses on telemetry sinks (journal-like append-only storage).
    """

    # PostgreSQL execution journal (optional)
    db_enabled: bool = True
    # PostgreSQL DSN, e.g. "postgresql://user:pass@host:5432/dbname"
    db_dsn: Optional[str] = None
    db_channels: list[str] = ["audit", "ops"]
    db_min_level: str = "INFO"
    db_batch_size: int = 50

    # MongoDB execution journal (optional)
    mongo_enabled: bool = False
    # Mongo URI, e.g. "mongodb://user:pass@host:27017/?authSource=admin"
    mongo_uri: Optional[str] = None
    mongo_db: str = "tycherion"
    mongo_collection: str = "execution_journal_events"
    mongo_channels: list[str] = ["audit", "ops"]
    mongo_min_level: str = "INFO"
    mongo_batch_size: int = 200

    # Console sink
    console_enabled: bool = False
    console_channels: list[str] = ["ops"]
    console_min_level: str = "INFO"


class AppConfig(BaseModel):
    timeframe: str
    lookback_days: int
    trading: Trading = Trading()
    risk: Risk = Risk()
    mt5: MT5 = MT5()
    application: ApplicationCfg = ApplicationCfg()
    telemetry: TelemetryCfg = TelemetryCfg()

def load_config(path: str) -> AppConfig:
    load_dotenv(override=False)
    import pathlib
    p = pathlib.Path(path)
    if not p.exists():
        raise FileNotFoundError(f"Config not found: {path}")
    with open(path, "r", encoding="utf-8") as f:
        raw = yaml.safe_load(f) or {}
    raw.setdefault("mt5", {})
    mt5_cfg = raw["mt5"] or {}

    def coalesce(yaml_val, env_val):
        return env_val if (yaml_val in (None, "", 0) and env_val not in (None, "")) else yaml_val

    env_terminal = os.getenv("MT5_TERMINAL_PATH")
    env_server   = os.getenv("MT5_SERVER")
    env_login    = os.getenv("MT5_LOGIN")
    env_pass     = os.getenv("MT5_PASSWORD")

    mt5_cfg["terminal_path"] = coalesce(mt5_cfg.get("terminal_path"), env_terminal)
    mt5_cfg["server"]        = coalesce(mt5_cfg.get("server"),        env_server)
    mt5_cfg["login"]         = coalesce(mt5_cfg.get("login"),         int(env_login) if env_login and env_login.isdigit() else None)
    mt5_cfg["password"]      = coalesce(mt5_cfg.get("password"),      env_pass)

    raw["mt5"] = mt5_cfg
    return AppConfig.model_validate(raw)

--- src\tycherion\shared\config.py:END ---

--- src\tycherion\shared\decorators.py:START ---
from __future__ import annotations
from functools import wraps
import logging
import MetaTrader5 as mt5

_log = logging.getLogger(__name__)

def demo_only(fn):
    @wraps(fn)
    def wrapper(self, *args, **kwargs):
        require = getattr(self, "require_demo", True)
        if require:
            ai = mt5.account_info()
            if not ai or ai.trade_mode != mt5.ACCOUNT_TRADE_MODE_DEMO:
                raise RuntimeError("Blocked: only allowed in DEMO account.")
        return fn(self, *args, **kwargs)
    return wrapper

def logged(fn):
    @wraps(fn)
    def wrapper(*args, **kwargs):
        name = fn.__qualname__
        try:
            res = fn(*args, **kwargs)
            _log.debug("%s: ok -> %s", name, res)
            return res
        except Exception as e:
            _log.exception("%s: error", name)
            raise
    return wrapper

--- src\tycherion\shared\decorators.py:END ---

--- tests\test_telemetry.py:START ---
from __future__ import annotations

import unittest
from datetime import datetime, timezone
import pathlib
import sys

ROOT = pathlib.Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT / "src"))

import pandas as pd

from tycherion.application.pipeline.config import PipelineConfig, PipelineStageConfig
from tycherion.application.pipeline.service import ModelPipelineService
from tycherion.application.telemetry import TelemetryHub, TelemetryProvider
from tycherion.adapters.telemetry.memory import InMemoryTelemetrySink
from tycherion.domain.portfolio.entities import PortfolioSnapshot
from tycherion.domain.signals.entities import ModelDecision
from tycherion.domain.signals.models.base import SignalModel
from tycherion.ports.telemetry import TelemetryLevel


class TestTelemetryHub(unittest.TestCase):
    def test_enabled_any_sink_accepts(self) -> None:
        sink = InMemoryTelemetrySink(
            enabled_flag=True,
            channels={"debug"},
            min_level=TelemetryLevel.DEBUG,
        )
        hub = TelemetryHub(sinks=[sink])

        self.assertTrue(hub.enabled("debug", "DEBUG"))
        self.assertFalse(hub.enabled("audit", "INFO"))


class TestTraceTelemetryIds(unittest.TestCase):
    def test_event_seq_monotonic_and_span_hierarchy(self) -> None:
        sink = InMemoryTelemetrySink(
            enabled_flag=True,
            channels={"ops", "audit"},
            min_level=TelemetryLevel.INFO,
        )
        hub = TelemetryHub(sinks=[sink])
        provider = TelemetryProvider(runner_id="test-runner", hub=hub)

        t = provider.new_trace(base_attributes={"component": "test"})

        with t.span("outer", channel="ops", level="INFO"):
            with t.span("inner", channel="ops", level="INFO"):
                t.emit(name="hello", channel="ops", level="INFO", data={"x": 1})

        self.assertGreaterEqual(len(sink.events), 3)

        # event_seq should be 1..N (no gaps) for emitted events in this trace
        seqs = [e.event_seq for e in sink.events]
        self.assertEqual(seqs, list(range(1, len(seqs) + 1)))

        # span ids should be 16-hex chars and inner span parent should be outer span
        outer_started = next(e for e in sink.events if e.name == "outer.started")
        inner_started = next(e for e in sink.events if e.name == "inner.started")

        self.assertIsNotNone(outer_started.span_id)
        self.assertIsNotNone(inner_started.span_id)
        self.assertEqual(len(str(outer_started.span_id)), 16)
        self.assertEqual(len(str(inner_started.span_id)), 16)

        self.assertEqual(inner_started.parent_span_id, outer_started.span_id)


class _DummyMarketData:
    def get_bars(self, symbol: str, timeframe: str, start: datetime, end: datetime) -> pd.DataFrame:
        _ = (symbol, timeframe, start, end)
        return pd.DataFrame(
            {
                "time": [datetime(2020, 1, 1, tzinfo=timezone.utc)],
                "open": [1.0],
                "high": [1.0],
                "low": [1.0],
                "close": [1.0],
                "tick_volume": [1],
            }
        )


class _DummyModel(SignalModel):
    def requires(self) -> set[str]:
        return set()

    def decide(self, indicators):  # type: ignore[override]
        _ = indicators
        return ModelDecision(side="BUY", weight=0.5, confidence=0.1)


class TestDebugGating(unittest.TestCase):
    def test_debug_events_not_emitted_when_debug_disabled(self) -> None:
        sink = InMemoryTelemetrySink(
            enabled_flag=True,
            channels={"audit", "ops"},
            min_level=TelemetryLevel.INFO,
        )
        hub = TelemetryHub(sinks=[sink])
        provider = TelemetryProvider(runner_id="test-runner", hub=hub)

        svc = ModelPipelineService(
            market_data=_DummyMarketData(),
            model_registry={"dummy": _DummyModel()},
            indicator_picker=lambda key, pb: None,  # type: ignore[return-value]
            timeframe="D1",
            lookback_days=10,
            playbook=None,
        )

        pipeline_config = PipelineConfig(stages=[PipelineStageConfig(name="dummy", drop_threshold=None)])
        portfolio = PortfolioSnapshot(equity=1000.0, positions={})

        tracer = provider.new_trace(base_attributes={"component": "test"})
        svc.run(
            universe_symbols=["AAA"],
            portfolio_snapshot=portfolio,
            pipeline_config=pipeline_config,
            tracer=tracer,
        )

        self.assertTrue(len(sink.events) > 0)
        self.assertEqual(0, sum(1 for e in sink.events if e.channel == "debug"))


if __name__ == "__main__":
    unittest.main()

--- tests\test_telemetry.py:END ---
