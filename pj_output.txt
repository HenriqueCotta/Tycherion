Tycherion
├── .codex
│   └── >>> config.toml <<<
├── >>> .env <<<
├── >>> .env.example <<<
├── .git (ignored)
├── .gitignore (ignored)
├── .venv (ignored)
├── .vscode
│   └── >>> settings.json <<<
├── README.md (ignored)
├── configs
│   └── >>> demo.yaml <<<
├── pj_output.txt (ignored)
├── >>> pyproject.toml <<<
├── >>> requirements.txt <<<
├── requirements_v1.txt (ignored)
├── scripts
│   └── >>> run_demo.py <<<
├── src
│   └── tycherion
│       ├── adapters
│       │   ├── mt5
│       │   │   ├── __pycache__ (ignored)
│       │   │   ├── >>> account_mt5.py <<<
│       │   │   ├── >>> market_data_mt5.py <<<
│       │   │   ├── >>> trading_mt5.py <<<
│       │   │   └── >>> universe_mt5.py <<<
│       │   ├── observability
│       │   │   ├── >>> __init__.py <<<
│       │   │   ├── __pycache__ (ignored)
│       │   │   ├── memory
│       │   │   │   ├── >>> __init__.py <<<
│       │   │   │   ├── __pycache__ (ignored)
│       │   │   │   └── >>> memory_observability.py <<<
│       │   │   ├── noop
│       │   │   │   ├── >>> __init__.py <<<
│       │   │   │   ├── __pycache__ (ignored)
│       │   │   │   └── >>> noop_observability.py <<<
│       │   │   └── otel
│       │   │       ├── >>> __init__.py <<<
│       │   │       ├── __pycache__ (ignored)
│       │   │       ├── >>> console.py <<<
│       │   │       ├── >>> event_seq.py <<<
│       │   │       ├── >>> mongo_audit.py <<<
│       │   │       ├── >>> otel_logs.py <<<
│       │   │       ├── >>> otel_metrics.py <<<
│       │   │       ├── >>> otel_observability.py <<<
│       │   │       └── >>> otel_traces.py <<<
│       │   └── telemetry
│       │       ├── >>> __init__.py <<<
│       │       ├── __pycache__ (ignored)
│       │       ├── >>> console.py <<<
│       │       ├── >>> db_journal.py <<<
│       │       ├── >>> memory.py <<<
│       │       └── >>> mongo_journal.py <<<
│       ├── application
│       │   ├── pipeline
│       │   │   ├── >>> __init__.py <<<
│       │   │   ├── __pycache__ (ignored)
│       │   │   ├── >>> config.py <<<
│       │   │   ├── >>> result.py <<<
│       │   │   └── >>> service.py <<<
│       │   ├── plugins
│       │   │   ├── __pycache__ (ignored)
│       │   │   └── >>> registry.py <<<
│       │   ├── runmodes
│       │   │   ├── __pycache__ (ignored)
│       │   │   └── >>> live_multimodel.py <<<
│       │   ├── services
│       │   │   ├── __pycache__ (ignored)
│       │   │   ├── >>> coverage_selector.py <<<
│       │   │   ├── >>> ensemble.py <<<
│       │   │   ├── >>> order_planner.py <<<
│       │   │   └── >>> sizer.py <<<
│       │   └── telemetry
│       │       ├── >>> __init__.py <<<
│       │       ├── __pycache__ (ignored)
│       │       ├── >>> event_factory.py <<<
│       │       ├── >>> hub.py <<<
│       │       ├── >>> ids.py <<<
│       │       ├── >>> provider.py <<<
│       │       └── >>> trace.py <<<
│       ├── bootstrap
│       │   ├── __pycache__ (ignored)
│       │   └── >>> main.py <<<
│       ├── domain
│       │   ├── >>> __init__.py <<<
│       │   ├── __pycache__ (ignored)
│       │   ├── market
│       │   │   ├── >>> __init__.py <<<
│       │   │   ├── __pycache__ (ignored)
│       │   │   └── >>> entities.py <<<
│       │   ├── portfolio
│       │   │   ├── >>> __init__.py <<<
│       │   │   ├── __pycache__ (ignored)
│       │   │   ├── allocators
│       │   │   │   ├── >>> __init__.py <<<
│       │   │   │   ├── __pycache__ (ignored)
│       │   │   │   ├── >>> base.py <<<
│       │   │   │   ├── >>> equal_weight.py <<<
│       │   │   │   └── >>> proportional.py <<<
│       │   │   ├── balancers
│       │   │   │   ├── >>> __init__.py <<<
│       │   │   │   ├── __pycache__ (ignored)
│       │   │   │   ├── >>> base.py <<<
│       │   │   │   └── >>> threshold.py <<<
│       │   │   └── >>> entities.py <<<
│       │   └── signals
│       │       ├── >>> __init__.py <<<
│       │       ├── __pycache__ (ignored)
│       │       ├── >>> entities.py <<<
│       │       ├── indicators
│       │       │   ├── >>> __init__.py <<<
│       │       │   ├── __pycache__ (ignored)
│       │       │   ├── >>> base.py <<<
│       │       │   ├── >>> stretch_zscore.py <<<
│       │       │   ├── >>> trend_donchian.py <<<
│       │       │   └── >>> volatility_atr.py <<<
│       │       └── models
│       │           ├── >>> __init__.py <<<
│       │           ├── __pycache__ (ignored)
│       │           ├── >>> base.py <<<
│       │           ├── >>> mean_reversion.py <<<
│       │           └── >>> trend_following.py <<<
│       ├── ports
│       │   ├── __pycache__ (ignored)
│       │   ├── >>> account.py <<<
│       │   ├── >>> market_data.py <<<
│       │   ├── observability
│       │   │   ├── >>> __init__.py <<<
│       │   │   ├── __pycache__ (ignored)
│       │   │   ├── >>> logs.py <<<
│       │   │   ├── >>> metrics.py <<<
│       │   │   ├── >>> observability.py <<<
│       │   │   ├── >>> traces.py <<<
│       │   │   └── >>> types.py <<<
│       │   ├── >>> telemetry.py <<<
│       │   ├── >>> trading.py <<<
│       │   └── >>> universe.py <<<
│       └── shared
│           ├── __pycache__ (ignored)
│           ├── >>> config.py <<<
│           └── >>> decorators.py <<<
├── tests
│   └── >>> test_telemetry.py <<<
├── tycherion_guidelines.md (ignored)
└── >>> tycherion_observability_flow.mmd <<<



--- .env:START ---
# Fixed identity for this machine/instance (used by telemetry runner_id)
TYCHERION_RUNNER_ID="dev-runner-01"

MT5_TERMINAL_PATH="C:\\Program Files\\MetaTrader 5 Terminal\\terminal64.exe"

# DEMO
MT5_SERVER="Rico-DEMO"
MT5_LOGIN="3008317111"
MT5_PASSWORD="BusterBD001."

# # PROD
# MT5_SERVER="Rico-PROD"
# MT5_LOGIN="3008317111"
# MT5_PASSWORD="BusterBD001."
--- .env:END ---

--- .env.example:START ---
# Fixed identity for this machine/instance (used by telemetry runner_id)
TYCHERION_RUNNER_ID="dev-runner-01"

# Opcional: se quiser logar via código em vez do Terminal já autenticado
MT5_TERMINAL_PATH=
MT5_SERVER=
MT5_LOGIN=
MT5_PASSWORD=

--- .env.example:END ---

--- pyproject.toml:START ---
[build-system]
requires = ["setuptools>=68", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "tycherion"
version = "0.1.0"
requires-python = ">=3.10"
dependencies = [
  "MetaTrader5>=5.0",
  "pandas>=2.2",
  "pyyaml>=6.0",
  "python-dotenv>=1.0",
  "pydantic>=2.8",
  "typing-extensions>=4.12",
  "opentelemetry-api>=1.26",
  "opentelemetry-sdk>=1.26",
  "opentelemetry-exporter-otlp-proto-grpc>=1.26",
  "opentelemetry-exporter-otlp-proto-http>=1.26",
  "pymongo>=4.7"
]

[tool.setuptools]
package-dir = {"" = "src"}

[tool.setuptools.packages.find]
where = ["src"]

[tool.mypy]
python_version = "3.10"
strict = true

[tool.ruff]
line-length = 100

--- pyproject.toml:END ---

--- requirements.txt:START ---
MetaTrader5>=5.0
pandas>=2.2
pyyaml>=6.0
python-dotenv>=1.0
pydantic>=2.8
typing-extensions>=4.12
opentelemetry-api>=1.26
opentelemetry-sdk>=1.26
opentelemetry-exporter-otlp-proto-grpc>=1.26
opentelemetry-exporter-otlp-proto-http>=1.26
pymongo>=4.7

--- requirements.txt:END ---

--- tycherion_observability_flow.mmd:START ---
---
id: eb18670e-f616-48e5-8b72-28a183db386f
---
flowchart TD
  A["scripts/run_demo.py"] --> B["bootstrap/main.py<br/>run_app()"]
  B --> C["bootstrap/main.py<br/>_build_observability()"]
  C -->|success| D["Adapter: OtelObservability"]
  C -->|fallback| Dn["Adapter: NoopObservability"]
  D --> E["Port Facade: ObservabilityPort"]
  Dn --> E

  subgraph CORE["Core: Application & Domain"]
    direction TB
    R["application/runmodes/live_multimodel.py"] -->|inject ObservabilityPort| P["application/pipeline/service.py<br/>ModelPipelineService.run()"]
    R --> S1["macro spans<br/>run, coverage.fetch, pipeline,<br/>allocator, balancer, execution"]
    P --> S2["span events<br/>pipeline.stage_started / completed"]
    P --> L1["logs<br/>logger.emit(body, severity, attrs)"]
  end

  subgraph PORTS["Ports: src/tycherion/ports/observability"]
    direction TB
    OP["ObservabilityPort"]
    TP["TracerProviderPort"] --> T["TracerPort"] --> SC["start_as_current_span(name, attrs)<br/>yields SpanPort"]
    LP["LoggerProviderPort"] --> LG["LoggerPort.emit(...)"]
    MP["MeterProviderPort"] --> M["MeterPort"] --> CTR["CounterPort.add(value, attrs)"]
  end

  CORE --> OP
  OP --> TP
  OP --> LP
  OP --> MP

  subgraph ADAPTERS["Adapters: src/tycherion/adapters/observability/otel"]
    direction TB

    D1["OtelObservability"]
    ES["EventSeqManager<br/>(contextvars, per active trace)"]
    CON["ConsoleRenderer<br/>(human-readable output)"]
    MONGO["MongoOpsJournal<br/>(optional ops_journal sink)"]

    RES["OTel Resource attributes<br/>service.name=tycherion<br/>service.instance.id=runner_id<br/>tycherion.runner_id, tycherion.schema_version"]
    SDKTP["OTel SDK TracerProvider"]
    SDKMP["OTel SDK MeterProvider"]

    WTP["OtelTracerProvider<br/>implements TracerProviderPort"]
    WT["OtelTracer<br/>implements TracerPort"]
    WS["OtelSpan<br/>implements SpanPort"]

    WLP["OtelLoggerProvider<br/>implements LoggerProviderPort"]
    WLG["OtelLogger<br/>implements LoggerPort"]

    WMP["OtelMeterProvider<br/>implements MeterProviderPort"]

    BSP["BatchSpanProcessor"]
    OTLP["OTLP Span Exporter (optional)<br/>TYCHERION_OTLP_ENABLED + endpoint"]
    COL["Collector target (future)<br/>Grafana Alloy / OTel Collector"]

    PMR["PeriodicExportingMetricReader"]
    OTLPm["OTLP Metric Exporter (optional)"]
  end

  %% Adapter wiring
  D1 --> RES --> SDKTP
  D1 --> SDKMP
  D1 --> ES
  D1 --> CON
  D1 -->|if enabled| MONGO

  %% Providers returned by facade
  D --> OP
  D1 --> WTP --> WT --> WS
  D1 --> WLP --> WLG
  D1 --> WMP

  %% Core calls through ports
  TP --> WTP
  T --> WT
  SC --> WS
  LG --> WLG
  M --> WMP

  %% Trace + event_seq behavior
  WT -->|start span| SDKTP
  WT -->|root trace begins| ES
  WS -->|add_event attaches<br/>tycherion.schema_version + tycherion.event_seq| ES
  WLG -->|emit correlates current span<br/>and attaches tycherion.event_seq| ES

  %% Fan-out
  WS --> CON
  WLG --> CON
  WS -->|span events| MONGO
  WLG -->|logs| MONGO

  %% OTLP export (spans/metrics)
  SDKTP --> BSP --> OTLP --> COL
  SDKMP --> PMR --> OTLPm --> COL

  %% Notes
  N1["Core never imports opentelemetry.*<br/>Only interacts via Ports API"]:::note
  N2["Logs are correlated using OTel trace context.<br/>Current sinks: Console + optional Mongo.<br/>OTel Logs SDK can be added later without changing Ports."]:::note
  N3["event_seq is Tycherion-specific.<br/>It increments only while a trace is active<br/>and is attached to logs + span events."]:::note

  CORE --- N1
  ADAPTERS --- N2
  ES --- N3

  classDef note fill:#fff7e6,stroke:#f0b429,color:#4a3b00;
--- tycherion_observability_flow.mmd:END ---

--- .codex\config.toml:START ---
project_doc_fallback_filenames = ["tycherion_guidelines.md"]
project_doc_max_bytes = 65536

--- .codex\config.toml:END ---

--- .vscode\settings.json:START ---
{
    "python.analysis.extraPaths": [
        "./src"
    ]
}
--- .vscode\settings.json:END ---

--- configs\demo.yaml:START ---
timeframe: H1
lookback_days: 15
trading:
  dry_run: true
  require_demo: true
  deviation_points: 10
  volume_mode: min
  fixed_volume: 100.0
risk:
  risk_per_trade_pct: 0.5
  max_daily_loss_pct: 2.0
mt5:
  terminal_path: null
  server: null
  login: null
  password: null
application:
  run_mode:
    name: live_multimodel
  playbook: default
  schedule:
    run_forever: true
    interval_seconds: 60
  coverage:
    source: static
    symbols:
    - PETR4
    - VALE3
    - WIN$
    - WDO$
    pattern: null
  models:
    pipeline:
    - trend_following
    - mean_reversion
  portfolio:
    allocator: proportional
    balancer: threshold
    threshold_weight: 0.25
telemetry:
  db_enabled: false
  db_min_level: INFO
  db_batch_size: 50
  mongo_enabled: false
  mongo_min_level: INFO
  mongo_batch_size: 200
  console_enabled: true
  console_min_level: INFO
  otlp_enabled: false
  otlp_endpoint: http://localhost:4317
  mongo_db: tycherion
  mongo_collection: ops_journal

--- configs\demo.yaml:END ---

--- scripts\run_demo.py:START ---
import sys, pathlib
ROOT = pathlib.Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT / "src"))
from tycherion.bootstrap.main import run_app
if __name__ == "__main__":
    run_app(config_path=str(ROOT / "configs" / "demo.yaml"))

--- scripts\run_demo.py:END ---

--- src\tycherion\adapters\mt5\account_mt5.py:START ---
from __future__ import annotations

import MetaTrader5 as mt5

from tycherion.ports.account import AccountPort
from tycherion.domain.portfolio.entities import Position


class MT5Account(AccountPort):
    def is_demo(self) -> bool:
        ai = mt5.account_info()
        return bool(ai and ai.trade_mode == mt5.ACCOUNT_TRADE_MODE_DEMO)

    def balance(self) -> float:
        ai = mt5.account_info()
        return float(getattr(ai, "balance", 0.0) or 0.0)

    def equity(self) -> float:
        ai = mt5.account_info()
        return float(getattr(ai, "equity", 0.0) or 0.0)

    def positions(self) -> list[Position]:
        poss = mt5.positions_get()
        out: list[Position] = []
        if poss:
            for p in poss:
                out.append(
                    Position(
                        symbol=p.symbol,
                        quantity=float(getattr(p, "volume", 0.0) or 0.0),
                        price=float(getattr(p, "price_open", 0.0) or 0.0),
                    )
                )
        return out

--- src\tycherion\adapters\mt5\account_mt5.py:END ---

--- src\tycherion\adapters\mt5\market_data_mt5.py:START ---
from __future__ import annotations
from datetime import datetime, timezone
from typing import Dict
import pandas as pd
import MetaTrader5 as mt5
from tycherion.ports.market_data import MarketDataPort

_TF_MAP: Dict[str, int] = {
    "M1": mt5.TIMEFRAME_M1,
    "M5": mt5.TIMEFRAME_M5,
    "M15": mt5.TIMEFRAME_M15,
    "M30": mt5.TIMEFRAME_M30,
    "H1": mt5.TIMEFRAME_H1,
    "H4": mt5.TIMEFRAME_H4,
    "D1": mt5.TIMEFRAME_D1,
}

class MT5MarketData(MarketDataPort):
    def get_bars(self, symbol: str, timeframe: str, start: datetime, end: datetime) -> pd.DataFrame:
        tf = _TF_MAP.get(timeframe.upper())
        if tf is None:
            raise ValueError(f"Unsupported timeframe: {timeframe}")
        rates = mt5.copy_rates_range(
            symbol, tf,
            start.astimezone(timezone.utc),
            end.astimezone(timezone.utc)
        )
        if rates is None or len(rates) == 0:
            return pd.DataFrame(columns=["time","open","high","low","close","tick_volume","spread","real_volume"])
        df = pd.DataFrame(rates)
        df["time"] = pd.to_datetime(df["time"], unit="s", utc=True)
        return df
--- src\tycherion\adapters\mt5\market_data_mt5.py:END ---

--- src\tycherion\adapters\mt5\trading_mt5.py:START ---
from __future__ import annotations
from dataclasses import dataclass
from typing import Optional
import MetaTrader5 as mt5
from tycherion.ports.trading import TradingPort, TradeResult
from tycherion.shared.decorators import demo_only, logged
from tycherion.application.services.sizer import symbol_min_volume, volume_from_weight

@dataclass
class MT5Trader(TradingPort):
    dry_run: bool = True
    require_demo: bool = True
    deviation_points: int = 10
    volume_mode: str = "min"
    fixed_volume: float = 0.01

    def _resolve_volume(self, symbol: str, volume: Optional[float]) -> float:
        if volume is not None:
            return float(volume)
        return volume_from_weight(symbol, 1.0, self.volume_mode, self.fixed_volume)

    @logged
    @demo_only
    def market_buy(self, symbol: str, volume: Optional[float] = None) -> TradeResult:
        if self.dry_run:
            return TradeResult(True, 0, None, "DRY_RUN: buy skipped")
        if not mt5.symbol_select(symbol, True):
            return TradeResult(False, -1, None, f"symbol_select failed: {symbol}")
        tick = mt5.symbol_info_tick(symbol)
        if not tick:
            return TradeResult(False, -2, None, "missing tick")
        vol = self._resolve_volume(symbol, volume)
        if vol < symbol_min_volume(symbol):
            vol = symbol_min_volume(symbol)
        request = {
            "action": mt5.TRADE_ACTION_DEAL,
            "symbol": symbol,
            "type": mt5.ORDER_TYPE_BUY,
            "volume": vol,
            "price": tick.ask,
            "deviation": self.deviation_points,
            "type_time": mt5.ORDER_TIME_GTC,
            "type_filling": mt5.ORDER_FILLING_RETURN,
            "magic": 401,
            "comment": "tycherion-buy",
        }
        check = mt5.order_check(request)
        if not check or check.retcode != mt5.TRADE_RETCODE_DONE:
            return TradeResult(False, getattr(check, "retcode", -3), None, f"order_check failed: {check}")
        res = mt5.order_send(request)
        ok = bool(res and res.retcode in (mt5.TRADE_RETCODE_DONE, mt5.TRADE_RETCODE_PLACED))
        return TradeResult(ok, getattr(res, "retcode", -4), getattr(res, "order", None), str(res))

    @logged
    @demo_only
    def market_sell(self, symbol: str, volume: Optional[float] = None) -> TradeResult:
        if self.dry_run:
            return TradeResult(True, 0, None, "DRY_RUN: sell skipped")
        if not mt5.symbol_select(symbol, True):
            return TradeResult(False, -1, None, f"symbol_select failed: {symbol}")
        tick = mt5.symbol_info_tick(symbol)
        if not tick:
            return TradeResult(False, -2, None, "missing tick")
        vol = self._resolve_volume(symbol, volume)
        if vol < symbol_min_volume(symbol):
            vol = symbol_min_volume(symbol)
        request = {
            "action": mt5.TRADE_ACTION_DEAL,
            "symbol": symbol,
            "type": mt5.ORDER_TYPE_SELL,
            "volume": vol,
            "price": tick.bid,
            "deviation": self.deviation_points,
            "type_time": mt5.ORDER_TIME_GTC,
            "type_filling": mt5.ORDER_FILLING_RETURN,
            "magic": 401,
            "comment": "tycherion-sell",
        }
        check = mt5.order_check(request)
        if not check or check.retcode != mt5.TRADE_RETCODE_DONE:
            return TradeResult(False, getattr(check, "retcode", -3), None, f"order_check failed: {check}")
        res = mt5.order_send(request)
        ok = bool(res and res.retcode in (mt5.TRADE_RETCODE_DONE, mt5.TRADE_RETCODE_PLACED))
        return TradeResult(ok, getattr(res, "retcode", -4), getattr(res, "order", None), str(res))

--- src\tycherion\adapters\mt5\trading_mt5.py:END ---

--- src\tycherion\adapters\mt5\universe_mt5.py:START ---
from __future__ import annotations
import MetaTrader5 as mt5
from typing import List
from tycherion.ports.universe import UniversePort

class MT5Universe(UniversePort):
    def visible_symbols(self) -> List[str]:
        syms = mt5.symbols_get()
        return [s.name for s in syms if getattr(s, "visible", False)]

    def by_pattern(self, pattern: str) -> List[str]:
        syms = mt5.symbols_get(pattern)
        return [s.name for s in syms]

--- src\tycherion\adapters\mt5\universe_mt5.py:END ---

--- src\tycherion\adapters\observability\__init__.py:START ---

--- src\tycherion\adapters\observability\__init__.py:END ---

--- src\tycherion\adapters\observability\memory\memory_observability.py:START ---
from __future__ import annotations

from contextlib import contextmanager
from dataclasses import dataclass, field
from datetime import datetime, timezone
import os
import secrets
import time
from typing import Any, Dict, List, Optional

import contextvars

from tycherion.ports.observability.logs import LoggerPort, LoggerProviderPort
from tycherion.ports.observability.metrics import CounterPort, MeterPort, MeterProviderPort
from tycherion.ports.observability.observability import ObservabilityPort
from tycherion.ports.observability.traces import SpanPort, TracerPort, TracerProviderPort
from tycherion.ports.observability.types import Attributes, Severity, TYCHERION_SCHEMA_VERSION


_current_span: contextvars.ContextVar["MemorySpan | None"] = contextvars.ContextVar("tycherion_mem_current_span", default=None)
_current_trace_state: contextvars.ContextVar["_TraceState | None"] = contextvars.ContextVar("tycherion_mem_trace_state", default=None)


@dataclass
class _TraceState:
    trace_id: str
    event_seq: int = 0


def _new_trace_id() -> str:
    return secrets.token_hex(16)  # 32 hex chars


def _new_span_id() -> str:
    return secrets.token_hex(8)  # 16 hex chars


@dataclass
class MemorySpan(SpanPort):
    name: str
    trace_id: str
    span_id: str
    parent_span_id: Optional[str]
    start_ns: int = field(default_factory=time.time_ns)
    end_ns: Optional[int] = None
    status: str = "UNSET"
    attributes: Dict[str, Any] = field(default_factory=dict)
    events: List[Dict[str, Any]] = field(default_factory=list)
    exceptions: List[str] = field(default_factory=list)

    def set_attribute(self, key: str, value: object) -> None:
        self.attributes[key] = value

    def set_attributes(self, attributes: Attributes) -> None:
        for k, v in attributes.items():
            self.attributes[k] = v

    def add_event(self, name: str, attributes: Attributes | None = None) -> None:
        st = _current_trace_state.get()
        seq = None
        if st and st.trace_id == self.trace_id:
            st.event_seq += 1
            seq = st.event_seq

        ev_attrs: Dict[str, Any] = dict(attributes or {})
        ev_attrs["tycherion.schema_version"] = TYCHERION_SCHEMA_VERSION
        if seq is not None:
            ev_attrs["tycherion.event_seq"] = seq

        self.events.append(
            {
                "ts": datetime.now(timezone.utc).isoformat(),
                "name": name,
                "attributes": ev_attrs,
            }
        )

    def record_exception(self, exc: BaseException) -> None:
        self.exceptions.append(repr(exc))

    def set_status_ok(self) -> None:
        self.status = "OK"

    def set_status_error(self, message: str | None = None) -> None:
        self.status = "ERROR"
        if message:
            self.attributes.setdefault("error.message", message)

    def is_recording(self) -> bool:
        return True


class MemoryTracer(TracerPort):
    def __init__(self, sink: "MemorySink") -> None:
        self._sink = sink

    @contextmanager
    def start_as_current_span(self, name: str, attributes: Attributes | None = None):
        parent = _current_span.get()
        if parent is None:
            trace_id = _new_trace_id()
            _current_trace_state.set(_TraceState(trace_id=trace_id, event_seq=0))
        else:
            trace_id = parent.trace_id

        span = MemorySpan(
            name=name,
            trace_id=trace_id,
            span_id=_new_span_id(),
            parent_span_id=parent.span_id if parent else None,
        )
        span.set_attributes(attributes or {})
        token = _current_span.set(span)
        self._sink.spans_started.append(span)
        try:
            yield span
        finally:
            span.end_ns = time.time_ns()
            _current_span.reset(token)
            self._sink.spans_ended.append(span)
            if parent is None:
                _current_trace_state.set(None)


class MemoryTracerProvider(TracerProviderPort):
    def __init__(self, sink: "MemorySink") -> None:
        self._sink = sink

    def get_tracer(self, name: str, version: str | None = None) -> TracerPort:
        return MemoryTracer(self._sink)


@dataclass
class MemoryLogRecord:
    ts: str
    body: str
    severity: Severity
    attributes: Dict[str, Any]
    trace_id: Optional[str]
    span_id: Optional[str]


class MemoryLogger(LoggerPort):
    def __init__(self, sink: "MemorySink", min_severity: Severity = Severity.INFO) -> None:
        self._sink = sink
        self._min_severity = min_severity
        self._rank = {
            Severity.TRACE: 0,
            Severity.DEBUG: 10,
            Severity.INFO: 20,
            Severity.WARN: 30,
            Severity.ERROR: 40,
            Severity.FATAL: 50,
        }

    def is_enabled(self, severity: Severity) -> bool:
        return self._rank[severity] >= self._rank[self._min_severity]

    def emit(self, body: str, severity: Severity, attributes: Attributes | None = None) -> None:
        if not self.is_enabled(severity):
            return

        span = _current_span.get()
        trace_id = span.trace_id if span else None
        span_id = span.span_id if span else None

        st = _current_trace_state.get()
        seq = None
        if st and trace_id and st.trace_id == trace_id:
            st.event_seq += 1
            seq = st.event_seq

        attrs: Dict[str, Any] = dict(attributes or {})
        attrs["tycherion.schema_version"] = TYCHERION_SCHEMA_VERSION
        if seq is not None:
            attrs["tycherion.event_seq"] = seq

        self._sink.logs.append(
            MemoryLogRecord(
                ts=datetime.now(timezone.utc).isoformat(),
                body=body,
                severity=severity,
                attributes=attrs,
                trace_id=trace_id,
                span_id=span_id,
            )
        )


class MemoryLoggerProvider(LoggerProviderPort):
    def __init__(self, sink: "MemorySink", min_severity: Severity = Severity.INFO) -> None:
        self._sink = sink
        self._min_severity = min_severity

    def get_logger(self, name: str, version: str | None = None) -> LoggerPort:
        return MemoryLogger(self._sink, min_severity=self._min_severity)


class MemoryCounter(CounterPort):
    def add(self, amount: int, attributes: Attributes | None = None) -> None:
        return None


class MemoryMeter(MeterPort):
    def create_counter(self, name: str, unit: str | None = None, description: str | None = None) -> CounterPort:
        return MemoryCounter()


class MemoryMeterProvider(MeterProviderPort):
    def get_meter(self, name: str, version: str | None = None) -> MeterPort:
        return MemoryMeter()


@dataclass
class MemorySink:
    spans_started: List[MemorySpan] = field(default_factory=list)
    spans_ended: List[MemorySpan] = field(default_factory=list)
    logs: List[MemoryLogRecord] = field(default_factory=list)


class MemoryObservability(ObservabilityPort):
    def __init__(self, min_log_severity: Severity = Severity.INFO) -> None:
        self.sink = MemorySink()
        self._traces = MemoryTracerProvider(self.sink)
        self._logs = MemoryLoggerProvider(self.sink, min_severity=min_log_severity)
        self._metrics = MemoryMeterProvider()

    @property
    def traces(self) -> TracerProviderPort:
        return self._traces

    @property
    def logs(self) -> LoggerProviderPort:
        return self._logs

    @property
    def metrics(self) -> MeterProviderPort:
        return self._metrics

    def shutdown(self) -> None:
        return None

    def force_flush(self) -> None:
        return None

--- src\tycherion\adapters\observability\memory\memory_observability.py:END ---

--- src\tycherion\adapters\observability\memory\__init__.py:START ---

--- src\tycherion\adapters\observability\memory\__init__.py:END ---

--- src\tycherion\adapters\observability\noop\noop_observability.py:START ---
from __future__ import annotations

from contextlib import contextmanager

from tycherion.ports.observability.logs import LoggerPort, LoggerProviderPort
from tycherion.ports.observability.metrics import CounterPort, MeterPort, MeterProviderPort
from tycherion.ports.observability.observability import ObservabilityPort
from tycherion.ports.observability.traces import SpanPort, TracerPort, TracerProviderPort
from tycherion.ports.observability.types import Attributes, Severity


class _NoopSpan(SpanPort):
    def set_attribute(self, key: str, value: object) -> None:
        return None

    def set_attributes(self, attributes: Attributes) -> None:
        return None

    def add_event(self, name: str, attributes: Attributes | None = None) -> None:
        return None

    def record_exception(self, exc: BaseException) -> None:
        return None

    def set_status_ok(self) -> None:
        return None

    def set_status_error(self, message: str | None = None) -> None:
        return None

    def is_recording(self) -> bool:
        return False


class _NoopTracer(TracerPort):
    @contextmanager
    def start_as_current_span(self, name: str, attributes: Attributes | None = None):
        yield _NoopSpan()


class _NoopTracerProvider(TracerProviderPort):
    def get_tracer(self, name: str, version: str | None = None) -> TracerPort:
        return _NoopTracer()


class _NoopLogger(LoggerPort):
    def emit(self, body: str, severity: Severity, attributes: Attributes | None = None) -> None:
        return None

    def is_enabled(self, severity: Severity) -> bool:
        return False


class _NoopLoggerProvider(LoggerProviderPort):
    def get_logger(self, name: str, version: str | None = None) -> LoggerPort:
        return _NoopLogger()


class _NoopCounter(CounterPort):
    def add(self, amount: int, attributes: Attributes | None = None) -> None:
        return None


class _NoopMeter(MeterPort):
    def create_counter(self, name: str, unit: str | None = None, description: str | None = None) -> CounterPort:
        return _NoopCounter()


class _NoopMeterProvider(MeterProviderPort):
    def get_meter(self, name: str, version: str | None = None) -> MeterPort:
        return _NoopMeter()


class NoopObservability(ObservabilityPort):
    def __init__(self) -> None:
        self._traces = _NoopTracerProvider()
        self._logs = _NoopLoggerProvider()
        self._metrics = _NoopMeterProvider()

    @property
    def traces(self) -> TracerProviderPort:
        return self._traces

    @property
    def logs(self) -> LoggerProviderPort:
        return self._logs

    @property
    def metrics(self) -> MeterProviderPort:
        return self._metrics

    def shutdown(self) -> None:
        return None

    def force_flush(self) -> None:
        return None

--- src\tycherion\adapters\observability\noop\noop_observability.py:END ---

--- src\tycherion\adapters\observability\noop\__init__.py:START ---

--- src\tycherion\adapters\observability\noop\__init__.py:END ---

--- src\tycherion\adapters\observability\otel\console.py:START ---
from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime
import sys
from typing import Any, Mapping

from tycherion.ports.observability.types import Severity


@dataclass(slots=True)
class ConsoleConfig:
    enabled: bool = True
    min_severity: Severity = Severity.INFO
    show_span_lifecycle: bool = True


class ConsoleRenderer:
    def __init__(self, cfg: ConsoleConfig) -> None:
        self._cfg = cfg
        self._rank = {
            Severity.TRACE: 0,
            Severity.DEBUG: 10,
            Severity.INFO: 20,
            Severity.WARN: 30,
            Severity.ERROR: 40,
            Severity.FATAL: 50,
        }

    def enabled_for(self, sev: Severity) -> bool:
        if not self._cfg.enabled:
            return False
        return self._rank[sev] >= self._rank[self._cfg.min_severity]

    def _short(self, hex_id: str | None) -> str | None:
        if not hex_id:
            return None
        return hex_id[:8]

    def _ts(self) -> str:
        return datetime.now().strftime("%H:%M:%S")

    def _fmt_kv(self, attrs: Mapping[str, Any] | None) -> str:
        if not attrs:
            return ""
        items = []
        for k, v in attrs.items():
            if v is None:
                continue
            items.append(f"{k}={v}")
        return " ".join(items)

    def log(self, *, body: str, severity: Severity, attributes: Mapping[str, Any] | None, trace_id: str | None, span_id: str | None, event_seq: int | None) -> None:
        if not self.enabled_for(severity):
            return
        meta = []
        if event_seq is not None:
            meta.append(f"seq={event_seq}")
        if trace_id:
            meta.append(f"trace={trace_id if severity in (Severity.ERROR, Severity.FATAL) else self._short(trace_id)}")
        if span_id:
            meta.append(f"span={self._short(span_id)}")
        meta_s = (" | " + " ".join(meta)) if meta else ""
        attrs_s = self._fmt_kv(attributes)
        attrs_s = (attrs_s + " ") if attrs_s else ""
        line = f"{self._ts()} [{severity.value}] {attrs_s}{body}{meta_s}"
        print(line, file=sys.stdout)

    def span_started(self, *, name: str, attributes: Mapping[str, Any] | None, trace_id: str, span_id: str) -> None:
        if not (self._cfg.enabled and self._cfg.show_span_lifecycle):
            return
        meta = f"trace={self._short(trace_id)} span={self._short(span_id)}"
        attrs_s = self._fmt_kv(attributes)
        attrs_s = (attrs_s + " ") if attrs_s else ""
        print(f"{self._ts()} [SPAN] {attrs_s}{name} started | {meta}", file=sys.stdout)

    def span_ended(self, *, name: str, status: str, duration_ms: float | None, trace_id: str, span_id: str, error: bool) -> None:
        if not (self._cfg.enabled and self._cfg.show_span_lifecycle):
            return
        dur = f"{duration_ms:.1f}ms" if duration_ms is not None else "?"
        # If error, print full trace_id to make backend lookup easy.
        trace_meta = trace_id if error else self._short(trace_id)
        meta = f"trace={trace_meta} span={self._short(span_id)}"
        print(f"{self._ts()} [SPAN] {name} ended status={status} dur={dur} | {meta}", file=sys.stdout)

    def span_event(self, *, name: str, attributes: Mapping[str, Any] | None, trace_id: str | None, span_id: str | None, event_seq: int | None) -> None:
        # Span events are usually info-ish, but we still respect min_severity (INFO).
        if not self.enabled_for(Severity.INFO):
            return
        meta = []
        if event_seq is not None:
            meta.append(f"seq={event_seq}")
        if trace_id:
            # Span events don't carry severity. Keep output short by default.
            meta.append(f"trace={self._short(trace_id)}")
        if span_id:
            meta.append(f"span={self._short(span_id)}")
        meta_s = (" | " + " ".join(meta)) if meta else ""
        attrs_s = self._fmt_kv(attributes)
        attrs_s = (attrs_s + " ") if attrs_s else ""
        print(f"{self._ts()} [EVT] {attrs_s}{name}{meta_s}", file=sys.stdout)

--- src\tycherion\adapters\observability\otel\console.py:END ---

--- src\tycherion\adapters\observability\otel\event_seq.py:START ---
from __future__ import annotations

from dataclasses import dataclass
import contextvars


@dataclass(slots=True)
class _EventSeqState:
    trace_id_hex: str
    seq: int = 0


_state: contextvars.ContextVar[_EventSeqState | None] = contextvars.ContextVar("tycherion_event_seq_state", default=None)


class EventSeqManager:
    """Tycherion event_seq kept in contextvars and incremented per active trace."""

    def start_trace(self, trace_id_hex: str) -> contextvars.Token[_EventSeqState | None]:
        return _state.set(_EventSeqState(trace_id_hex=trace_id_hex, seq=0))

    def end_trace(self, token: contextvars.Token[_EventSeqState | None]) -> None:
        _state.reset(token)

    def next_for_trace(self, trace_id_hex: str) -> int | None:
        st = _state.get()
        if st is None or st.trace_id_hex != trace_id_hex:
            return None
        st.seq += 1
        return st.seq

    def current_seq(self, trace_id_hex: str) -> int | None:
        st = _state.get()
        if st is None or st.trace_id_hex != trace_id_hex:
            return None
        return st.seq

--- src\tycherion\adapters\observability\otel\event_seq.py:END ---

--- src\tycherion\adapters\observability\otel\mongo_audit.py:START ---
from __future__ import annotations

from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Any, Mapping

from tycherion.ports.observability.types import Severity


def _mongo_client(uri: str):
    try:
        from pymongo import MongoClient  # type: ignore
    except Exception as e:
        raise RuntimeError(
            "Mongo audit sink requires `pymongo` to be installed in the runtime environment"
        ) from e
    return MongoClient(uri)


@dataclass(slots=True)
class MongoOpsJournal:
    """Append-only ops journal persisted in MongoDB.

    Stores logs and span events in a single collection for easy filtering.

    Dedupe strategy:
    - unique index on (trace_id, event_seq) when available
    - inserts are best-effort, duplicates are ignored
    """

    uri: str
    db_name: str = "tycherion"
    collection_name: str = "ops_journal"
    enabled_flag: bool = True
    min_severity: Severity = Severity.INFO
    batch_size: int = 200

    runner_id: str | None = None
    schema_version: str | None = None

    _client: Any | None = field(default=None, init=False, repr=False)
    _collection: Any | None = field(default=None, init=False, repr=False)
    _buffer: list[dict[str, Any]] = field(default_factory=list, init=False, repr=False)

    def _ensure_collection(self):
        if self._collection is not None:
            return self._collection

        client = _mongo_client(self.uri)
        db = client[self.db_name]
        col = db[self.collection_name]

        try:
            col.create_index([("trace_id", 1), ("event_seq", 1)], unique=True, name="uq_trace_eventseq")
            col.create_index([("ts_utc", 1)], name="idx_ts")
            col.create_index([("runner_id", 1)], name="idx_runner")
            col.create_index([("severity", 1)], name="idx_severity")
            col.create_index([("signal", 1)], name="idx_signal")
        except Exception:
            pass

        self._client = client
        self._collection = col
        return col

    def _rank(self, s: Severity) -> int:
        return {
            Severity.TRACE: 0,
            Severity.DEBUG: 10,
            Severity.INFO: 20,
            Severity.WARN: 30,
            Severity.ERROR: 40,
            Severity.FATAL: 50,
        }[s]

    def enabled(self, severity: Severity) -> bool:
        if not self.enabled_flag:
            return False
        return self._rank(severity) >= self._rank(self.min_severity)

    def emit_log(
        self,
        *,
        body: str,
        severity: Severity,
        attributes: Mapping[str, Any] | None,
        trace_id: str | None,
        span_id: str | None,
        event_seq: int | None,
    ) -> None:
        if not self.enabled(severity):
            return

        doc: dict[str, Any] = {
            "signal": "log",
            "ts_utc": datetime.now(timezone.utc),
            "runner_id": self.runner_id,
            "schema_version": self.schema_version,
            "severity": severity.value,
            "body": body,
            "attributes": dict(attributes or {}),
            "trace_id": trace_id,
            "span_id": span_id,
            "event_seq": event_seq,
        }
        self._buffer.append(doc)
        if len(self._buffer) >= max(1, int(self.batch_size)):
            self.flush()

    def emit_span_event(
        self,
        *,
        name: str,
        attributes: Mapping[str, Any] | None,
        trace_id: str | None,
        span_id: str | None,
        event_seq: int | None,
    ) -> None:
        # Span events are considered INFO.
        if not self.enabled(Severity.INFO):
            return

        doc: dict[str, Any] = {
            "signal": "span_event",
            "ts_utc": datetime.now(timezone.utc),
            "runner_id": self.runner_id,
            "schema_version": self.schema_version,
            "severity": Severity.INFO.value,
            "name": name,
            "attributes": dict(attributes or {}),
            "trace_id": trace_id,
            "span_id": span_id,
            "event_seq": event_seq,
        }
        self._buffer.append(doc)
        if len(self._buffer) >= max(1, int(self.batch_size)):
            self.flush()

    def flush(self) -> None:
        if not self._buffer:
            return
        try:
            col = self._ensure_collection()
        except Exception:
            self._buffer.clear()
            return

        docs = list(self._buffer)
        self._buffer.clear()

        try:
            col.insert_many(docs, ordered=False)
        except Exception:
            return

    def close(self) -> None:
        try:
            self.flush()
        finally:
            try:
                if self._client is not None:
                    self._client.close()
            except Exception:
                pass
            self._client = None
            self._collection = None

--- src\tycherion\adapters\observability\otel\mongo_audit.py:END ---

--- src\tycherion\adapters\observability\otel\otel_logs.py:START ---
from __future__ import annotations

from typing import Any, Mapping

from opentelemetry import trace as otel_trace

from tycherion.adapters.observability.otel.console import ConsoleRenderer
from tycherion.adapters.observability.otel.event_seq import EventSeqManager
from tycherion.adapters.observability.otel.mongo_audit import MongoOpsJournal
from tycherion.ports.observability.logs import LoggerPort, LoggerProviderPort
from tycherion.ports.observability.types import Attributes, Severity


def _current_trace_span_ids() -> tuple[str | None, str | None]:
    try:
        span = otel_trace.get_current_span()
        ctx = span.get_span_context()
        if not getattr(ctx, "is_valid", False):
            return None, None
        trace_id = format(int(ctx.trace_id), "032x")
        span_id = format(int(ctx.span_id), "016x")
        return trace_id, span_id
    except Exception:
        return None, None


class OtelLogger(LoggerPort):
    def __init__(
        self,
        *,
        schema_version: str,
        min_severity: Severity,
        console: ConsoleRenderer,
        event_seq: EventSeqManager,
        mongo: MongoOpsJournal | None,
    ) -> None:
        self._schema_version = schema_version
        self._min_severity = min_severity
        self._console = console
        self._event_seq = event_seq
        self._mongo = mongo
        self._rank = {
            Severity.TRACE: 0,
            Severity.DEBUG: 10,
            Severity.INFO: 20,
            Severity.WARN: 30,
            Severity.ERROR: 40,
            Severity.FATAL: 50,
        }

    def is_enabled(self, severity: Severity) -> bool:
        return self._rank[severity] >= self._rank[self._min_severity]

    def emit(self, body: str, severity: Severity, attributes: Attributes | None = None) -> None:
        if not self.is_enabled(severity):
            return

        trace_id, span_id = _current_trace_span_ids()

        seq = None
        if trace_id:
            seq = self._event_seq.next_for_trace(trace_id)

        attrs: dict[str, Any] = dict(attributes or {})
        attrs["tycherion.schema_version"] = self._schema_version
        if seq is not None:
            attrs["tycherion.event_seq"] = seq

        self._console.log(
            body=body,
            severity=severity,
            attributes=attrs,
            trace_id=trace_id,
            span_id=span_id,
            event_seq=seq,
        )

        if self._mongo is not None:
            self._mongo.emit_log(
                body=body,
                severity=severity,
                attributes=attrs,
                trace_id=trace_id,
                span_id=span_id,
                event_seq=seq,
            )


class OtelLoggerProvider(LoggerProviderPort):
    def __init__(
        self,
        *,
        schema_version: str,
        min_severity: Severity,
        console: ConsoleRenderer,
        event_seq: EventSeqManager,
        mongo: MongoOpsJournal | None,
    ) -> None:
        self._schema_version = schema_version
        self._min_severity = min_severity
        self._console = console
        self._event_seq = event_seq
        self._mongo = mongo

    def get_logger(self, name: str, version: str | None = None) -> LoggerPort:
        # name/version are carried in OTel "instrumentation scope". For now, we keep them for
        # API compatibility and future wiring into OTel Logs SDK.
        _ = (name, version)
        return OtelLogger(
            schema_version=self._schema_version,
            min_severity=self._min_severity,
            console=self._console,
            event_seq=self._event_seq,
            mongo=self._mongo,
        )

--- src\tycherion\adapters\observability\otel\otel_logs.py:END ---

--- src\tycherion\adapters\observability\otel\otel_metrics.py:START ---
from __future__ import annotations

from typing import Any

from tycherion.ports.observability.metrics import CounterPort, MeterPort, MeterProviderPort
from tycherion.ports.observability.types import Attributes


class _OtelCounter(CounterPort):
    def __init__(self, counter: Any) -> None:
        self._counter = counter

    def add(self, amount: int, attributes: Attributes | None = None) -> None:
        try:
            self._counter.add(amount, attributes=dict(attributes or {}))
        except Exception:
            return None


class _OtelMeter(MeterPort):
    def __init__(self, meter: Any) -> None:
        self._meter = meter

    def create_counter(self, name: str, unit: str | None = None, description: str | None = None) -> CounterPort:
        try:
            c = self._meter.create_counter(name, unit=unit, description=description)
            return _OtelCounter(c)
        except Exception:
            # Fallback: no-op counter
            return _OtelCounter(counter=_NoopCounter())


class _NoopCounter:
    def add(self, amount: int, attributes: dict | None = None) -> None:
        return None


class OtelMeterProvider(MeterProviderPort):
    def __init__(self, provider: Any) -> None:
        self._provider = provider

    def get_meter(self, name: str, version: str | None = None) -> MeterPort:
        # opentelemetry-python uses instrumentation scope params; keyword names
        # differ across versions. Use positional for maximum compatibility.
        meter = self._provider.get_meter(name, version)
        return _OtelMeter(meter)

--- src\tycherion\adapters\observability\otel\otel_metrics.py:END ---

--- src\tycherion\adapters\observability\otel\otel_observability.py:START ---
from __future__ import annotations

from dataclasses import dataclass
from typing import Any

from opentelemetry import metrics as otel_metrics_api
from opentelemetry import trace as otel_trace

from tycherion.adapters.observability.otel.console import ConsoleConfig, ConsoleRenderer
from tycherion.adapters.observability.otel.event_seq import EventSeqManager
from tycherion.adapters.observability.otel.mongo_audit import MongoOpsJournal
from tycherion.adapters.observability.otel.otel_logs import OtelLoggerProvider
from tycherion.adapters.observability.otel.otel_metrics import OtelMeterProvider
from tycherion.adapters.observability.otel.otel_traces import OtelTracerProvider
from tycherion.ports.observability.observability import ObservabilityPort
from tycherion.ports.observability.traces import TracerProviderPort
from tycherion.ports.observability.logs import LoggerProviderPort
from tycherion.ports.observability.metrics import MeterProviderPort
from tycherion.ports.observability.types import Severity


@dataclass(slots=True)
class OtelObservabilityConfig:
    runner_id: str
    schema_version: str

    # Console output
    console_enabled: bool = True
    console_min_severity: Severity = Severity.INFO
    console_show_span_lifecycle: bool = True

    # OTLP (future: Alloy/Collector -> Tempo/Loki/Prometheus)
    otlp_enabled: bool = False
    otlp_endpoint: str = "http://localhost:4317"

    # Mongo ops journal (optional)
    mongo_audit_enabled: bool = False
    mongo_uri: str | None = None
    mongo_db: str = "tycherion"
    mongo_collection: str = "ops_journal"
    mongo_min_severity: Severity = Severity.INFO
    mongo_batch_size: int = 200


class OtelObservability(ObservabilityPort):
    def __init__(self, cfg: OtelObservabilityConfig) -> None:
        self._cfg = cfg
        self._event_seq = EventSeqManager()

        self._console = ConsoleRenderer(
            ConsoleConfig(
                enabled=bool(cfg.console_enabled),
                min_severity=cfg.console_min_severity,
                show_span_lifecycle=bool(cfg.console_show_span_lifecycle),
            )
        )

        self._mongo: MongoOpsJournal | None = None
        if cfg.mongo_audit_enabled:
            if not cfg.mongo_uri:
                raise ValueError("mongo_audit_enabled=True requires mongo_uri")
            self._mongo = MongoOpsJournal(
                uri=cfg.mongo_uri,
                db_name=cfg.mongo_db,
                collection_name=cfg.mongo_collection,
                enabled_flag=True,
                min_severity=cfg.mongo_min_severity,
                batch_size=cfg.mongo_batch_size,
                runner_id=cfg.runner_id,
                schema_version=cfg.schema_version,
            )

        # SDK imports are intentionally delayed so importing this module does not hard-require
        # opentelemetry-sdk until you actually instantiate this adapter.
        try:
            from opentelemetry.sdk.resources import Resource  # type: ignore
            from opentelemetry.sdk.trace import TracerProvider  # type: ignore
            from opentelemetry.sdk.trace.export import BatchSpanProcessor  # type: ignore
        except Exception as e:
            raise RuntimeError(
                "OtelObservability requires `opentelemetry-sdk` to be installed. "
                "Install project dependencies (see requirements/pyproject)."
            ) from e

        resource = Resource.create(
            {
                "service.name": "tycherion",
                "service.instance.id": cfg.runner_id,
                "tycherion.runner_id": cfg.runner_id,
                "tycherion.schema_version": cfg.schema_version,
            }
        )

        tracer_provider = TracerProvider(resource=resource)

        if cfg.otlp_enabled:
            span_exporter = _build_otlp_span_exporter(cfg.otlp_endpoint)
            if span_exporter is not None:
                tracer_provider.add_span_processor(BatchSpanProcessor(span_exporter))

        # Register global providers (safe-ish; used by trace context propagation)
        try:
            otel_trace.set_tracer_provider(tracer_provider)
        except Exception:
            # If provider was already set by runtime, keep ours referenced locally anyway.
            pass

        self._sdk_tracer_provider = tracer_provider
        self._traces = OtelTracerProvider(
            tracer_provider,
            schema_version=cfg.schema_version,
            event_seq=self._event_seq,
            console=self._console,
            mongo=self._mongo,
        )

        # Metrics provider (minimal)
        meter_provider = _build_meter_provider(resource=resource, otlp_enabled=cfg.otlp_enabled, otlp_endpoint=cfg.otlp_endpoint)
        try:
            if meter_provider is not None:
                otel_metrics_api.set_meter_provider(meter_provider)
        except Exception:
            pass

        self._sdk_meter_provider = meter_provider
        self._metrics = OtelMeterProvider(meter_provider) if meter_provider is not None else OtelMeterProvider(_NoopMeterProvider())

        self._logs = OtelLoggerProvider(
            schema_version=cfg.schema_version,
            min_severity=cfg.console_min_severity,  # console gate
            console=self._console,
            event_seq=self._event_seq,
            mongo=self._mongo,
        )

    @property
    def traces(self) -> TracerProviderPort:
        return self._traces

    @property
    def logs(self) -> LoggerProviderPort:
        return self._logs

    @property
    def metrics(self) -> MeterProviderPort:
        return self._metrics

    def force_flush(self) -> None:
        try:
            self._sdk_tracer_provider.force_flush()
        except Exception:
            pass
        try:
            if self._sdk_meter_provider is not None:
                self._sdk_meter_provider.force_flush()
        except Exception:
            pass
        try:
            if self._mongo is not None:
                self._mongo.flush()
        except Exception:
            pass

    def shutdown(self) -> None:
        try:
            self.force_flush()
        finally:
            try:
                self._sdk_tracer_provider.shutdown()
            except Exception:
                pass
            try:
                if self._sdk_meter_provider is not None:
                    self._sdk_meter_provider.shutdown()
            except Exception:
                pass
            try:
                if self._mongo is not None:
                    self._mongo.close()
            except Exception:
                pass


def _build_otlp_span_exporter(endpoint: str):
    # Prefer gRPC exporter (4317) by default; fall back to HTTP exporter (4318) when available.
    # We intentionally keep this "best-effort" to avoid hard failures when only one exporter is installed.
    try:
        if ":4318" in endpoint or endpoint.rstrip("/").endswith("4318"):
            from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter  # type: ignore

            return OTLPSpanExporter(endpoint=endpoint)
        from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter  # type: ignore

        return OTLPSpanExporter(endpoint=endpoint, insecure=True)
    except Exception:
        return None


def _build_meter_provider(*, resource: Any, otlp_enabled: bool, otlp_endpoint: str):
    try:
        from opentelemetry.sdk.metrics import MeterProvider  # type: ignore
    except Exception:
        return None

    if not otlp_enabled:
        return MeterProvider(resource=resource)

    try:
        if ":4318" in otlp_endpoint or otlp_endpoint.rstrip("/").endswith("4318"):
            from opentelemetry.exporter.otlp.proto.http.metric_exporter import OTLPMetricExporter  # type: ignore
            from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader  # type: ignore

            reader = PeriodicExportingMetricReader(OTLPMetricExporter(endpoint=otlp_endpoint))
            return MeterProvider(resource=resource, metric_readers=[reader])

        from opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter  # type: ignore
        from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader  # type: ignore

        reader = PeriodicExportingMetricReader(OTLPMetricExporter(endpoint=otlp_endpoint, insecure=True))
        return MeterProvider(resource=resource, metric_readers=[reader])
    except Exception:
        return MeterProvider(resource=resource)


class _NoopMeterProvider:
    def get_meter(self, name: str, version: str | None = None):
        _ = (name, version)
        return _NoopMeter()


class _NoopMeter:
    def create_counter(self, name: str, unit: str | None = None, description: str | None = None):
        _ = (name, unit, description)
        return _NoopCounter()


class _NoopCounter:
    def add(self, amount: int, attributes: dict | None = None) -> None:
        _ = (amount, attributes)
        return None

--- src\tycherion\adapters\observability\otel\otel_observability.py:END ---

--- src\tycherion\adapters\observability\otel\otel_traces.py:START ---
from __future__ import annotations

from contextlib import contextmanager
import time
from typing import Any, Mapping

from opentelemetry import trace as otel_trace
from opentelemetry.trace.status import Status, StatusCode

from tycherion.adapters.observability.otel.console import ConsoleRenderer
from tycherion.adapters.observability.otel.event_seq import EventSeqManager
from tycherion.adapters.observability.otel.mongo_audit import MongoOpsJournal
from tycherion.ports.observability.traces import SpanPort, TracerPort, TracerProviderPort
from tycherion.ports.observability.types import Attributes, TYCHERION_SCHEMA_VERSION


def _hex_trace_id(span_or_ctx: Any) -> str | None:
    try:
        ctx = span_or_ctx.get_span_context()
        if not getattr(ctx, "is_valid", False):
            return None
        return format(int(ctx.trace_id), "032x")
    except Exception:
        return None


def _hex_span_id(span_or_ctx: Any) -> str | None:
    try:
        ctx = span_or_ctx.get_span_context()
        if not getattr(ctx, "is_valid", False):
            return None
        return format(int(ctx.span_id), "016x")
    except Exception:
        return None


class OtelSpan(SpanPort):
    def __init__(
        self,
        span: Any,
        *,
        schema_version: str,
        event_seq: EventSeqManager,
        console: ConsoleRenderer,
        mongo: MongoOpsJournal | None,
    ) -> None:
        self._span = span
        self._schema_version = schema_version
        self._event_seq = event_seq
        self._console = console
        self._mongo = mongo

        self._trace_id_hex = _hex_trace_id(span) or None
        self._span_id_hex = _hex_span_id(span) or None
        self._start_ns = time.time_ns()
        self._status = "UNSET"

    @property
    def trace_id_hex(self) -> str | None:
        return self._trace_id_hex

    @property
    def span_id_hex(self) -> str | None:
        return self._span_id_hex

    @property
    def status(self) -> str:
        return self._status

    @property
    def start_ns(self) -> int:
        return self._start_ns

    def set_attribute(self, key: str, value: object) -> None:
        try:
            self._span.set_attribute(key, value)
        except Exception:
            return None

    def set_attributes(self, attributes: Attributes) -> None:
        for k, v in (attributes or {}).items():
            self.set_attribute(k, v)

    def _decorate_event_attrs(self, attributes: Attributes | None) -> dict[str, Any]:
        attrs: dict[str, Any] = dict(attributes or {})
        attrs["tycherion.schema_version"] = self._schema_version
        if self._trace_id_hex:
            seq = self._event_seq.next_for_trace(self._trace_id_hex)
        else:
            seq = None
        if seq is not None:
            attrs["tycherion.event_seq"] = seq
        return attrs

    def add_event(self, name: str, attributes: Attributes | None = None) -> None:
        attrs = self._decorate_event_attrs(attributes)

        try:
            self._span.add_event(name, attributes=attrs)
        except Exception:
            pass

        self._console.span_event(
            name=name,
            attributes=attrs,
            trace_id=self._trace_id_hex,
            span_id=self._span_id_hex,
            event_seq=attrs.get("tycherion.event_seq"),
        )

        if self._mongo is not None:
            self._mongo.emit_span_event(
                name=name,
                attributes=attrs,
                trace_id=self._trace_id_hex,
                span_id=self._span_id_hex,
                event_seq=attrs.get("tycherion.event_seq"),
            )

    def record_exception(self, exc: BaseException) -> None:
        try:
            self._span.record_exception(exc)
        except Exception:
            return None

    def set_status_ok(self) -> None:
        self._status = "OK"
        try:
            self._span.set_status(Status(StatusCode.OK))
        except Exception:
            return None

    def set_status_error(self, message: str | None = None) -> None:
        self._status = "ERROR"
        try:
            self._span.set_status(Status(StatusCode.ERROR, description=message))
        except Exception:
            return None

    def is_recording(self) -> bool:
        try:
            return bool(self._span.is_recording())
        except Exception:
            return False


class OtelTracer(TracerPort):
    def __init__(
        self,
        tracer: Any,
        *,
        schema_version: str,
        event_seq: EventSeqManager,
        console: ConsoleRenderer,
        mongo: MongoOpsJournal | None,
    ) -> None:
        self._tracer = tracer
        self._schema_version = schema_version
        self._event_seq = event_seq
        self._console = console
        self._mongo = mongo

    def _decorate_span_attrs(self, attributes: Attributes | None) -> dict[str, Any]:
        attrs: dict[str, Any] = dict(attributes or {})
        attrs.setdefault("tycherion.schema_version", self._schema_version)
        return attrs

    @contextmanager
    def start_as_current_span(self, name: str, attributes: Attributes | None = None):
        parent_ctx = otel_trace.get_current_span().get_span_context()
        new_trace = not getattr(parent_ctx, "is_valid", False)

        attrs = self._decorate_span_attrs(attributes)

        token = None
        start_ns = time.time_ns()
        with self._tracer.start_as_current_span(name, attributes=attrs) as span:
            trace_id_hex = _hex_trace_id(span) or ""
            span_id_hex = _hex_span_id(span) or ""
            if new_trace:
                token = self._event_seq.start_trace(trace_id_hex)

            wrapped = OtelSpan(
                span,
                schema_version=self._schema_version,
                event_seq=self._event_seq,
                console=self._console,
                mongo=self._mongo,
            )
            self._console.span_started(
                name=name,
                attributes=attrs,
                trace_id=trace_id_hex,
                span_id=span_id_hex,
            )
            try:
                yield wrapped
            finally:
                end_ns = time.time_ns()
                duration_ms = (end_ns - start_ns) / 1_000_000
                error = wrapped.status == "ERROR"
                self._console.span_ended(
                    name=name,
                    status=wrapped.status,
                    duration_ms=duration_ms,
                    trace_id=trace_id_hex,
                    span_id=span_id_hex,
                    error=error,
                )
                if token is not None:
                    self._event_seq.end_trace(token)


class OtelTracerProvider(TracerProviderPort):
    def __init__(
        self,
        provider: Any,
        *,
        schema_version: str,
        event_seq: EventSeqManager,
        console: ConsoleRenderer,
        mongo: MongoOpsJournal | None,
    ) -> None:
        self._provider = provider
        self._schema_version = schema_version
        self._event_seq = event_seq
        self._console = console
        self._mongo = mongo

    def get_tracer(self, name: str, version: str | None = None) -> TracerPort:
        # opentelemetry-python's TracerProvider.get_tracer() accepts the scope
        # version as the *second positional argument* (keyword names differ
        # across OTel releases: "instrumentation_scope_version", etc.).
        # Using positional keeps us compatible with a wider range of versions.
        tracer = self._provider.get_tracer(name, version)
        return OtelTracer(
            tracer,
            schema_version=self._schema_version,
            event_seq=self._event_seq,
            console=self._console,
            mongo=self._mongo,
        )

--- src\tycherion\adapters\observability\otel\otel_traces.py:END ---

--- src\tycherion\adapters\observability\otel\__init__.py:START ---

--- src\tycherion\adapters\observability\otel\__init__.py:END ---

--- src\tycherion\adapters\telemetry\console.py:START ---
from __future__ import annotations

import sys
from dataclasses import dataclass, field
from typing import Any

from tycherion.ports.telemetry import TelemetryEvent, TelemetryLevel, TelemetrySink


def _short(v: Any, limit: int = 80) -> str:
    s = str(v)
    return s if len(s) <= limit else (s[: limit - 1] + "…")


def _summarize(event: TelemetryEvent) -> str:
    attributes = dict(event.attributes or {})
    data = dict(event.data or {})

    parts: list[str] = []
    for k in ("component", "stage", "symbol", "model"):
        if k in attributes and attributes[k] not in (None, ""):
            parts.append(f"{k}={_short(attributes[k], 40)}")

    for k in (
        "dropped_count",
        "passed_count",
        "symbols_count",
        "duration_ms",
        "threshold",
        "score",
        "side",
        "weight",
        "confidence",
        "reason",
    ):
        if k in data:
            parts.append(f"{k}={_short(data[k], 40)}")

    if not parts and data:
        parts.append(f"data_keys={list(data.keys())[:8]}")

    return " ".join(parts)


@dataclass(slots=True)
class ConsoleTelemetrySink(TelemetrySink):
    """Human-friendly console output.

    Default should be disabled via config. When enabled, prints one line per event.
    """

    enabled_flag: bool = False
    channels: set[str] = field(default_factory=lambda: {"ops"})
    min_level: TelemetryLevel = TelemetryLevel.INFO
    stream: Any = sys.stdout

    def enabled(self, channel: str, level: TelemetryLevel, name: str | None = None) -> bool:
        _ = name
        if not self.enabled_flag:
            return False
        if channel not in self.channels:
            return False
        return TelemetryLevel.coerce(level).rank() >= TelemetryLevel.coerce(self.min_level).rank()

    def emit(self, event: TelemetryEvent) -> None:
        # Keep this stable and easy to grep
        trace_short = _short(event.trace_id, 18)
        msg = (
            f"[{event.level.value}]"
            f"[{event.channel}]"
            f"[runner={_short(event.runner_id, 18)}]"
            f"[trace={trace_short}]"
            f"[event_seq={event.event_seq}]"
        )

        if event.span_id:
            msg += f"[span={_short(event.span_id, 8)}]"
        if event.parent_span_id:
            msg += f"[parent={_short(event.parent_span_id, 8)}]"

        msg += f" {event.name}"
        summary = _summarize(event)
        if summary:
            msg = f"{msg} {summary}"
        try:
            self.stream.write(msg + "\n")
            self.stream.flush()
        except Exception:
            return

--- src\tycherion\adapters\telemetry\console.py:END ---

--- src\tycherion\adapters\telemetry\db_journal.py:START ---
from __future__ import annotations

import json
from dataclasses import dataclass, field

from tycherion.ports.telemetry import TelemetryEvent, TelemetryLevel, TelemetrySink


# Idempotent-ish DDL:
# - Create table if missing
# - Add new columns for newer schema if table existed from older versions
# - Create a unique index for idempotent dedupe (trace_id, event_seq)
_DDL = """
CREATE TABLE IF NOT EXISTS execution_journal_events (
  id BIGSERIAL PRIMARY KEY,
  runner_id TEXT NULL,
  trace_id TEXT NOT NULL,
  event_seq BIGINT NULL,
  ts_utc TEXT NOT NULL,
  mono_ns BIGINT NULL,
  span_id TEXT NULL,
  parent_span_id TEXT NULL,
  name TEXT NOT NULL,
  level TEXT NOT NULL,
  channel TEXT NOT NULL,
  attributes_json TEXT NOT NULL,
  data_json TEXT NOT NULL,
  schema_version INTEGER NOT NULL
);

ALTER TABLE execution_journal_events ADD COLUMN IF NOT EXISTS runner_id TEXT NULL;
ALTER TABLE execution_journal_events ADD COLUMN IF NOT EXISTS event_seq BIGINT NULL;
ALTER TABLE execution_journal_events ADD COLUMN IF NOT EXISTS mono_ns BIGINT NULL;

CREATE INDEX IF NOT EXISTS idx_eje_trace_ts ON execution_journal_events(trace_id, ts_utc);
CREATE INDEX IF NOT EXISTS idx_eje_span ON execution_journal_events(span_id);
CREATE INDEX IF NOT EXISTS idx_eje_name ON execution_journal_events(name);
CREATE INDEX IF NOT EXISTS idx_eje_channel ON execution_journal_events(channel);
CREATE INDEX IF NOT EXISTS idx_eje_runner ON execution_journal_events(runner_id);

-- Dedupe / idempotency
CREATE UNIQUE INDEX IF NOT EXISTS uq_eje_trace_eventseq ON execution_journal_events(trace_id, event_seq);
"""


def _connect(dsn: str):
    """Lazy import of a Postgres driver.

    We intentionally do NOT add dependencies to Tycherion. The sink will work
    if the runtime environment provides a PostgreSQL DB-API driver.
    """

    try:
        import psycopg  # type: ignore

        return psycopg.connect(dsn)
    except Exception:
        pass

    try:
        import psycopg2  # type: ignore

        return psycopg2.connect(dsn)
    except Exception as e:
        raise RuntimeError(
            "PostgreSQL telemetry sink requires a driver (psycopg or psycopg2) to be installed"
        ) from e


@dataclass(slots=True)
class DbExecutionJournalSink(TelemetrySink):
    """Append-only execution journal persisted in PostgreSQL.

    Dedupe strategy:
    - table has a surrogate PK (BIGSERIAL)
    - for idempotency, we also have a unique index on (trace_id, event_seq)
    - inserts use ON CONFLICT DO NOTHING when supported
    """

    dsn: str
    enabled_flag: bool = True
    channels: set[str] = field(default_factory=lambda: {"audit", "ops"})
    min_level: TelemetryLevel = TelemetryLevel.INFO
    batch_size: int = 100

    _conn: object | None = field(default=None, init=False, repr=False)
    _buffer: list[
        tuple[
            str | None,
            str,
            int | None,
            str,
            int | None,
            str | None,
            str | None,
            str,
            str,
            str,
            str,
            str,
            int,
        ]
    ] = field(default_factory=list, init=False, repr=False)
    _supports_on_conflict: bool = field(default=True, init=False, repr=False)

    def _ensure_conn(self):
        if self._conn is not None:
            return self._conn
        conn = _connect(self.dsn)
        try:
            cur = conn.cursor()
            cur.execute(_DDL)
            conn.commit()
        except Exception:
            try:
                conn.close()
            except Exception:
                pass
            raise
        self._conn = conn
        return conn

    def enabled(self, channel: str, level: TelemetryLevel, name: str | None = None) -> bool:
        _ = name
        if not self.enabled_flag:
            return False
        if channel not in self.channels:
            return False
        return TelemetryLevel.coerce(level).rank() >= TelemetryLevel.coerce(self.min_level).rank()

    def emit(self, event: TelemetryEvent) -> None:
        if not self.enabled(event.channel, event.level, event.name):
            return
        try:
            attributes_json = json.dumps(
                dict(event.attributes or {}), separators=(",", ":"), ensure_ascii=False
            )
            data_json = json.dumps(dict(event.data or {}), separators=(",", ":"), ensure_ascii=False)
            row = (
                str(event.runner_id) if event.runner_id else None,
                str(event.trace_id),
                int(event.event_seq) if event.event_seq is not None else None,
                event.ts_utc.isoformat(),
                int(event.mono_ns) if event.mono_ns is not None else None,
                str(event.span_id) if event.span_id else None,
                str(event.parent_span_id) if event.parent_span_id else None,
                str(event.name),
                str(event.level.value),
                str(event.channel),
                attributes_json,
                data_json,
                int(event.schema_version),
            )
            self._buffer.append(row)
            if len(self._buffer) >= max(1, int(self.batch_size)):
                self.flush()
        except Exception:
            return

    def flush(self) -> None:
        if not self._buffer:
            return
        try:
            conn = self._ensure_conn()
        except Exception:
            self._buffer.clear()
            return

        rows = list(self._buffer)
        self._buffer.clear()

        def _exec(sql: str) -> None:
            cur = conn.cursor()
            cur.executemany(sql, rows)
            conn.commit()

        try:
            if self._supports_on_conflict:
                _exec(
                    """
                    INSERT INTO execution_journal_events
                      (runner_id, trace_id, event_seq, ts_utc, mono_ns, span_id, parent_span_id, name, level, channel, attributes_json, data_json, schema_version)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (trace_id, event_seq) DO NOTHING
                    """
                )
            else:
                _exec(
                    """
                    INSERT INTO execution_journal_events
                      (runner_id, trace_id, event_seq, ts_utc, mono_ns, span_id, parent_span_id, name, level, channel, attributes_json, data_json, schema_version)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """
                )
        except Exception:
            # If ON CONFLICT fails (missing unique index/constraint), fallback once.
            try:
                conn.rollback()
            except Exception:
                pass

            if self._supports_on_conflict:
                self._supports_on_conflict = False
                try:
                    _exec(
                        """
                        INSERT INTO execution_journal_events
                          (runner_id, trace_id, event_seq, ts_utc, mono_ns, span_id, parent_span_id, name, level, channel, attributes_json, data_json, schema_version)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        """
                    )
                    return
                except Exception:
                    try:
                        conn.rollback()
                    except Exception:
                        pass
            return

    def close(self) -> None:
        try:
            self.flush()
        finally:
            if self._conn is not None:
                try:
                    self._conn.close()
                except Exception:
                    pass
                self._conn = None

--- src\tycherion\adapters\telemetry\db_journal.py:END ---

--- src\tycherion\adapters\telemetry\memory.py:START ---
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Iterable

from tycherion.ports.telemetry import TelemetryEvent, TelemetryLevel, TelemetrySink


@dataclass(slots=True)
class InMemoryTelemetrySink(TelemetrySink):
    """Test-friendly telemetry sink.

    Stores events in memory so tests can assert on them without stdout/DB.
    """

    enabled_flag: bool = True
    channels: set[str] = field(default_factory=lambda: {"audit", "ops"})
    min_level: TelemetryLevel = TelemetryLevel.INFO
    events: list[TelemetryEvent] = field(default_factory=list)

    def enabled(self, channel: str, level: TelemetryLevel, name: str | None = None) -> bool:
        _ = name
        if not self.enabled_flag:
            return False
        if channel not in self.channels:
            return False
        return TelemetryLevel.coerce(level).rank() >= TelemetryLevel.coerce(self.min_level).rank()

    def emit(self, event: TelemetryEvent) -> None:
        self.events.append(event)

--- src\tycherion\adapters\telemetry\memory.py:END ---

--- src\tycherion\adapters\telemetry\mongo_journal.py:START ---
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any

from tycherion.ports.telemetry import TelemetryEvent, TelemetryLevel, TelemetrySink


def _mongo_client(uri: str):
    try:
        from pymongo import MongoClient  # type: ignore
    except Exception as e:
        raise RuntimeError(
            "MongoDB telemetry sink requires `pymongo` to be installed in the runtime environment"
        ) from e
    return MongoClient(uri)


@dataclass(slots=True)
class MongoExecutionJournalSink(TelemetrySink):
    """Append-only execution journal persisted in MongoDB.

    This sink is meant for operational health/audit data:
    - errors, spans, lifecycle, audits
    - queryable by trace_id, runner_id, event_seq, time

    Dedupe strategy:
    - unique index on (trace_id, event_seq)
    - inserts are best-effort, duplicates are ignored
    """

    uri: str
    db_name: str = "tycherion"
    collection_name: str = "execution_journal_events"
    enabled_flag: bool = True
    channels: set[str] = field(default_factory=lambda: {"audit", "ops"})
    min_level: TelemetryLevel = TelemetryLevel.INFO
    batch_size: int = 200

    _client: Any | None = field(default=None, init=False, repr=False)
    _collection: Any | None = field(default=None, init=False, repr=False)
    _buffer: list[dict[str, Any]] = field(default_factory=list, init=False, repr=False)

    def _ensure_collection(self):
        if self._collection is not None:
            return self._collection

        client = _mongo_client(self.uri)
        db = client[self.db_name]
        col = db[self.collection_name]

        # Best-effort index creation (ignore permissions/duplicate issues)
        try:
            col.create_index([("trace_id", 1), ("event_seq", 1)], unique=True, name="uq_trace_eventseq")
            col.create_index([("ts_utc", 1)], name="idx_ts")
            col.create_index([("runner_id", 1)], name="idx_runner")
            col.create_index([("channel", 1)], name="idx_channel")
            col.create_index([("level", 1)], name="idx_level")
        except Exception:
            pass

        self._client = client
        self._collection = col
        return col

    def enabled(self, channel: str, level: TelemetryLevel, name: str | None = None) -> bool:
        _ = name
        if not self.enabled_flag:
            return False
        if channel not in self.channels:
            return False
        return TelemetryLevel.coerce(level).rank() >= TelemetryLevel.coerce(self.min_level).rank()

    def emit(self, event: TelemetryEvent) -> None:
        if not self.enabled(event.channel, event.level, event.name):
            return

        try:
            doc: dict[str, Any] = {
                "schema_version": int(event.schema_version),
                "runner_id": str(event.runner_id),
                "trace_id": str(event.trace_id),
                "event_seq": int(event.event_seq),
                "ts_utc": event.ts_utc,  # pymongo stores datetime natively
                "mono_ns": int(event.mono_ns) if event.mono_ns is not None else None,
                "span_id": str(event.span_id) if event.span_id else None,
                "parent_span_id": str(event.parent_span_id) if event.parent_span_id else None,
                "name": str(event.name),
                "level": str(event.level.value),
                "channel": str(event.channel),
                "attributes": dict(event.attributes or {}),
                "data": dict(event.data or {}),
            }
            self._buffer.append(doc)
            if len(self._buffer) >= max(1, int(self.batch_size)):
                self.flush()
        except Exception:
            return

    def flush(self) -> None:
        if not self._buffer:
            return

        try:
            col = self._ensure_collection()
        except Exception:
            self._buffer.clear()
            return

        docs = list(self._buffer)
        self._buffer.clear()

        try:
            # ordered=False keeps going after dup errors
            col.insert_many(docs, ordered=False)
        except Exception:
            # Ignore duplicate key and other errors (best-effort journal)
            return

    def close(self) -> None:
        try:
            self.flush()
        finally:
            try:
                if self._client is not None:
                    self._client.close()
            except Exception:
                pass
            self._client = None
            self._collection = None

--- src\tycherion\adapters\telemetry\mongo_journal.py:END ---

--- src\tycherion\adapters\telemetry\__init__.py:START ---
from .console import ConsoleTelemetrySink
from .db_journal import DbExecutionJournalSink
from .mongo_journal import MongoExecutionJournalSink
from .memory import InMemoryTelemetrySink

__all__ = [
    "ConsoleTelemetrySink",
    "DbExecutionJournalSink",
    "MongoExecutionJournalSink",
    "InMemoryTelemetrySink",
]

--- src\tycherion\adapters\telemetry\__init__.py:END ---

--- src\tycherion\application\pipeline\config.py:START ---
from __future__ import annotations

from dataclasses import dataclass
from typing import Iterable, List

from tycherion.shared.config import AppConfig, PipelineStageCfg


@dataclass(frozen=True, slots=True)
class PipelineStageConfig:
    """Configuration of a single pipeline stage (application-level, YAML-agnostic)."""

    name: str
    drop_threshold: float | None = None


@dataclass(frozen=True, slots=True)
class PipelineConfig:
    """Internal normalized pipeline configuration.

    This object is the only thing the pipeline execution should consume.
    It is intentionally decoupled from YAML and Pydantic.
    """

    stages: List[PipelineStageConfig]


def build_pipeline_config(cfg: AppConfig) -> PipelineConfig:
    """Build a PipelineConfig from the current AppConfig.

    The AppConfig is created by YAML/adapters, but the rest of the application
    should not read YAML-derived structures directly.
    """
    stages_in: Iterable[PipelineStageCfg] = cfg.application.models.pipeline or []
    stages: list[PipelineStageConfig] = []
    for st in stages_in:
        stages.append(
            PipelineStageConfig(
                name=str(st.name),
                drop_threshold=(float(st.drop_threshold) if st.drop_threshold is not None else None),
            )
        )
    if not stages:
        raise RuntimeError(
            "No model pipeline configured. Please set application.models.pipeline in your YAML."
        )
    return PipelineConfig(stages=stages)

--- src\tycherion\application\pipeline\config.py:END ---

--- src\tycherion\application\pipeline\result.py:START ---
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict

from tycherion.domain.portfolio.entities import SignalsBySymbol
from tycherion.domain.signals.entities import SymbolState

from .config import PipelineConfig


@dataclass(frozen=True, slots=True)
class PipelineRunResult:
    pipeline_config: PipelineConfig
    states_by_symbol: Dict[str, SymbolState]
    signals_by_symbol: SignalsBySymbol
    stage_stats: Dict[str, int]

--- src\tycherion\application\pipeline\result.py:END ---

--- src\tycherion\application\pipeline\service.py:START ---
from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import Callable, Dict, Mapping, Optional, Tuple

import pandas as pd

from tycherion.domain.portfolio.entities import PortfolioSnapshot, Signal, SignalsBySymbol
from tycherion.domain.signals.entities import (
    IndicatorOutput,
    ModelDecision,
    ModelStageResult,
    SymbolState,
)
from tycherion.domain.signals.models.base import SignalModel
from tycherion.domain.signals.indicators.base import BaseIndicator
from tycherion.ports.market_data import MarketDataPort

from tycherion.ports.observability.observability import ObservabilityPort
from tycherion.ports.observability.traces import SpanPort
from tycherion.ports.observability.logs import LoggerPort
from tycherion.ports.observability.types import Severity, TYCHERION_SCHEMA_VERSION

from .config import PipelineConfig, PipelineStageConfig
from .result import PipelineRunResult


@dataclass(slots=True)
class ModelPipelineService:
    """Façade that runs the ordered per-symbol model pipeline."""

    market_data: MarketDataPort
    model_registry: Mapping[str, SignalModel]
    indicator_picker: Callable[[str, Optional[str]], BaseIndicator]
    timeframe: str
    lookback_days: int
    playbook: str | None = None

    def run(
        self,
        universe_symbols: list[str],
        portfolio_snapshot: PortfolioSnapshot,
        pipeline_config: PipelineConfig,
        *,
        observability: ObservabilityPort,
    ) -> PipelineRunResult:
        tracer = observability.traces.get_tracer("tycherion.pipeline", version=TYCHERION_SCHEMA_VERSION)
        logger = observability.logs.get_logger("tycherion.pipeline", version=TYCHERION_SCHEMA_VERSION)

        held_symbols = set(portfolio_snapshot.positions.keys())

        with tracer.start_as_current_span(
            "pipeline",
            attributes={
                "symbols_count": int(len(universe_symbols)),
                "stages": [st.name for st in pipeline_config.stages],
                "timeframe": self.timeframe,
                "lookback_days": int(self.lookback_days),
            },
        ) as span:
            # 1) Init per-symbol state
            states: Dict[str, SymbolState] = {
                sym: SymbolState(symbol=sym, is_held=(sym in held_symbols))
                for sym in universe_symbols
            }

            # 2) Resolve models
            resolved = self._resolve_models(pipeline_config)

            # 3) Determine indicator needs once for the whole pipeline
            needed_keys: set[str] = set()
            for _, model in resolved:
                try:
                    needed_keys.update(model.requires() or set())
                except Exception:
                    pass

            # 4) Time window for analysis
            end = datetime.now(timezone.utc)
            start = end - timedelta(days=int(self.lookback_days))

            stage_stats: Dict[str, int] = {st.name: 0 for st in pipeline_config.stages}
            stage_passed: Dict[str, int] = {st.name: 0 for st in pipeline_config.stages}

            for st in pipeline_config.stages:
                span.add_event(
                    "pipeline.stage_started",
                    {"stage": st.name, "threshold": st.drop_threshold},
                )

            for symbol, state in states.items():
                if not state.alive and not state.is_held:
                    continue

                df = self._safe_get_bars(symbol, start, end, state, span, logger)
                if df is None or df.empty:
                    if not state.is_held:
                        logger.emit(
                            "pipeline.symbol_dropped",
                            Severity.WARN,
                            {
                                "tycherion.channel": "audit",
                                "symbol": symbol,
                                "reason": "no_market_data",
                            },
                        )
                        state.alive = False
                    continue

                if logger.is_enabled(Severity.DEBUG):
                    try:
                        logger.emit(
                            "market_data.sample",
                            Severity.DEBUG,
                            {
                                "tycherion.channel": "debug",
                                "symbol": symbol,
                                "rows": int(len(df)),
                                "columns": list(df.columns)[:20],
                                "head": df.head(2).to_dict(orient="list"),
                                "tail": df.tail(2).to_dict(orient="list"),
                            },
                        )
                    except Exception:
                        pass

                bundle = self._compute_indicators(df, needed_keys, state, span, logger)

                # Pipeline execution per stage
                for stage_cfg, model in resolved:
                    if not state.alive and not state.is_held:
                        break

                    stage_passed[stage_cfg.name] = int(stage_passed.get(stage_cfg.name, 0)) + 1
                    score = self._run_stage(symbol, stage_cfg, model, bundle, state, span, logger)

                    # Drop policy
                    if stage_cfg.drop_threshold is not None and score < float(stage_cfg.drop_threshold):
                        if state.is_held:
                            state.notes[f"below_threshold_{stage_cfg.name}"] = 1.0
                            continue
                        state.alive = False
                        state.notes[f"dropped_by_{stage_cfg.name}"] = 1.0
                        stage_stats[stage_cfg.name] = int(stage_stats.get(stage_cfg.name, 0)) + 1
                        logger.emit(
                            "pipeline.symbol_dropped",
                            Severity.INFO,
                            {
                                "tycherion.channel": "audit",
                                "symbol": symbol,
                                "stage": stage_cfg.name,
                                "score": float(score),
                                "threshold": float(stage_cfg.drop_threshold),
                                "reason": "below_threshold",
                            },
                        )
                        break

                # Final signal fields (simple v1 rule: last stage score)
                last_score = float(state.pipeline_results[-1].score) if state.pipeline_results else 0.0
                state.alpha_score = last_score
                state.notes["final_confidence"] = abs(last_score)

            # 5) Convert states into SignalsBySymbol
            signals: SignalsBySymbol = {}
            for symbol, state in states.items():
                if not state.alive and not state.is_held:
                    continue
                signed = float(state.alpha_score)
                confidence = float(state.notes.get("final_confidence", abs(signed)))
                signals[symbol] = Signal(symbol=symbol, signed=signed, confidence=confidence)
                logger.emit(
                    "pipeline.signal_emitted",
                    Severity.INFO,
                    {
                        "tycherion.channel": "audit",
                        "symbol": symbol,
                        "signed": signed,
                        "confidence": confidence,
                    },
                )

            for st in pipeline_config.stages:
                dropped = int(stage_stats.get(st.name, 0))
                passed = int(stage_passed.get(st.name, 0))
                span.add_event(
                    "pipeline.stage_completed",
                    {
                        "stage": st.name,
                        "passed_count": passed,
                        "dropped_count": dropped,
                        "threshold": st.drop_threshold,
                    },
                )

            span.add_event(
                "pipeline.summary",
                {
                    "signals_count": int(len(signals)),
                    "alive_count": int(sum(1 for s in states.values() if s.alive or s.is_held)),
                },
            )

            return PipelineRunResult(
                pipeline_config=pipeline_config,
                states_by_symbol=states,
                signals_by_symbol=signals,
                stage_stats=stage_stats,
            )

    def _resolve_models(self, pipeline_config: PipelineConfig) -> list[Tuple[PipelineStageConfig, SignalModel]]:
        pipeline: list[Tuple[PipelineStageConfig, SignalModel]] = []
        for stage in pipeline_config.stages:
            name = stage.name
            model = self.model_registry.get(name)
            if model is None:
                available = ", ".join(sorted(self.model_registry.keys()))
                raise RuntimeError(f"Model not found: {name!r}. Available models: {available}")
            pipeline.append((stage, model))
        return pipeline

    def _safe_get_bars(
        self,
        symbol: str,
        start: datetime,
        end: datetime,
        state: SymbolState,
        span: SpanPort,
        logger: LoggerPort,
    ) -> pd.DataFrame | None:
        try:
            return self.market_data.get_bars(symbol, self.timeframe, start, end)
        except Exception as e:
            state.notes["data_error"] = 1.0
            span.record_exception(e)
            logger.emit(
                "error.exception",
                Severity.ERROR,
                {
                    "tycherion.channel": "ops",
                    "symbol": symbol,
                    "exception_type": type(e).__name__,
                    "message": str(e),
                    "stage": "get_bars",
                },
            )
            return None

    def _compute_indicators(
        self,
        df: pd.DataFrame,
        needed_keys: set[str],
        state: SymbolState,
        span: SpanPort,
        logger: LoggerPort,
    ) -> Dict[str, IndicatorOutput]:
        bundle: Dict[str, IndicatorOutput] = {}
        for key in needed_keys:
            try:
                ind = self.indicator_picker(key, self.playbook)
                bundle[key] = ind.compute(df.copy())
            except Exception as e:
                state.notes[f"indicator_error_{key}"] = 1.0
                span.record_exception(e)
                logger.emit(
                    "error.exception",
                    Severity.ERROR,
                    {
                        "tycherion.channel": "ops",
                        "exception_type": type(e).__name__,
                        "message": str(e),
                        "stage": "indicator",
                        "indicator": key,
                    },
                )
                bundle[key] = IndicatorOutput(score=0.0, features={})
        return bundle

    def _run_stage(
        self,
        symbol: str,
        stage_cfg: PipelineStageConfig,
        model: SignalModel,
        indicators: Dict[str, IndicatorOutput],
        state: SymbolState,
        span: SpanPort,
        logger: LoggerPort,
    ) -> float:
        stage_name = stage_cfg.name
        try:
            if logger.is_enabled(Severity.DEBUG):
                try:
                    logger.emit(
                        "model.input_snapshot",
                        Severity.DEBUG,
                        {
                            "tycherion.channel": "debug",
                            "symbol": symbol,
                            "stage": stage_name,
                            "model": stage_name,
                            "indicator_keys": list(indicators.keys())[:30],
                            "features_keys": {
                                k: list(v.features.keys())[:20]
                                for k, v in indicators.items()
                                if getattr(v, "features", None)
                            },
                        },
                    )
                except Exception:
                    pass

            decision = model.decide(indicators)
        except Exception as e:
            state.notes[f"model_error_{stage_name}"] = 1.0
            span.record_exception(e)
            logger.emit(
                "error.exception",
                Severity.ERROR,
                {
                    "tycherion.channel": "ops",
                    "symbol": symbol,
                    "stage": stage_name,
                    "model": stage_name,
                    "exception_type": type(e).__name__,
                    "message": str(e),
                    "stage_kind": "model",
                },
            )
            decision = ModelDecision(side="HOLD", weight=0.0, confidence=0.0)

        score = self._decision_to_score(decision)
        state.pipeline_results.append(ModelStageResult(model_name=stage_name, score=score))

        logger.emit(
            "model.decided",
            Severity.INFO,
            {
                "tycherion.channel": "audit",
                "symbol": symbol,
                "stage": stage_name,
                "model": stage_name,
                "score": float(score),
                "side": decision.side,
                "weight": float(decision.weight or 0.0),
                "confidence": float(decision.confidence or 0.0),
            },
        )
        return score

    @staticmethod
    def _decision_to_score(d: ModelDecision) -> float:
        """Map a ModelDecision into a numeric score in [-1, 1]."""
        side = (d.side or "HOLD").upper()
        w = float(d.weight or 0.0)
        w = max(0.0, min(1.0, w))
        if side == "BUY":
            s = w
        elif side == "SELL":
            s = -w
        else:
            s = 0.0
        return max(-1.0, min(1.0, s))

--- src\tycherion\application\pipeline\service.py:END ---

--- src\tycherion\application\pipeline\__init__.py:START ---

--- src\tycherion\application\pipeline\__init__.py:END ---

--- src\tycherion\application\plugins\registry.py:START ---
from __future__ import annotations

from typing import Dict, List, Iterable

from tycherion.ports.observability.observability import ObservabilityPort
from tycherion.ports.observability.types import Severity, TYCHERION_SCHEMA_VERSION

from tycherion.domain.signals.indicators.base import BaseIndicator
from tycherion.domain.signals.models.base import SignalModel
from tycherion.domain.portfolio.allocators.base import BaseAllocator
from tycherion.domain.portfolio.balancers.base import BaseBalancer

INDICATORS: Dict[str, List[BaseIndicator]] = {}
MODELS: Dict[str, SignalModel] = {}
ALLOCATORS: Dict[str, BaseAllocator] = {}
BALANCERS: Dict[str, BaseBalancer] = {}
DEFAULT_METHOD: Dict[str, str] = {}


def register_indicator(*, key: str, method: str, tags: set[str]):
    """Register an indicator implementation for a given logical key (e.g. "trend")
    and method (e.g. "donchian_50_50").
    """

    def deco(cls):
        inst = cls()
        inst.key = key
        inst.method = method
        inst.tags = tags
        INDICATORS.setdefault(key, []).append(inst)
        return cls

    return deco


def register_model(*, name: str, tags: set[str]):
    """Register a per-symbol signal model."""

    def deco(cls):
        inst = cls()
        inst.name = name
        inst.tags = tags
        MODELS[name] = inst
        return cls

    return deco


def register_allocator(*, name: str, tags: set[str]):
    """Register a portfolio allocator strategy."""

    def deco(cls):
        inst = cls()
        inst.name = name
        inst.tags = tags
        ALLOCATORS[name] = inst
        return cls

    return deco


def register_balancer(*, name: str, tags: set[str]):
    """Register a portfolio balancer / rebalancer strategy."""

    def deco(cls):
        inst = cls()
        inst.name = name
        inst.tags = tags
        BALANCERS[name] = inst
        return cls

    return deco


def set_default_indicator_method(key: str, method: str) -> None:
    DEFAULT_METHOD[key] = method


def pick_indicator_for(key: str, playbook: str | None = None) -> BaseIndicator:
    """Pick an indicator instance for a given key and (optionally) playbook."""

    candidates: Iterable[BaseIndicator] = INDICATORS.get(key, [])
    candidates = list(candidates)
    if not candidates:
        raise KeyError(f"No indicators registered for key={key!r}")

    # filter by tags / playbook
    if playbook:
        tagged = [ind for ind in candidates if playbook in getattr(ind, "tags", set())]
        if tagged:
            candidates = tagged

    # then prefer "default"
    defaults = [ind for ind in candidates if "default" in getattr(ind, "tags", set())]
    if defaults:
        candidates = defaults

    # lastly, prefer DEFAULT_METHOD if configured
    method = DEFAULT_METHOD.get(key)
    if method:
        for ind in candidates:
            if getattr(ind, "method", None) == method:
                return ind

    return candidates[0]


def auto_discover(*, observability: ObservabilityPort | None) -> None:
    """Import all plugin modules so that their decorators run and fill registries."""

    import importlib
    import pkgutil

    tracer = observability.traces.get_tracer("tycherion.plugins", version=TYCHERION_SCHEMA_VERSION) if observability else None
    logger = observability.logs.get_logger("tycherion.plugins", version=TYCHERION_SCHEMA_VERSION) if observability else None

    def _log(body: str, severity: Severity, **data) -> None:
        if logger is None:
            return
        attrs = {"tycherion.channel": "ops", **data}
        logger.emit(body, severity, attrs)

    bases = (
        "tycherion.domain.signals.indicators",
        "tycherion.domain.signals.models",
        "tycherion.domain.portfolio.allocators",
        "tycherion.domain.portfolio.balancers",
    )

    if tracer is None:
        # No observability: best effort discovery without logs.
        for base in bases:
            pkg = importlib.import_module(base)
            for mod in pkgutil.walk_packages(getattr(pkg, "__path__", None), pkg.__name__ + "."):
                importlib.import_module(mod.name)
        return

    with tracer.start_as_current_span("plugins.discover", attributes={"component": "plugins"}):
        for base in bases:
            try:
                pkg = importlib.import_module(base)
            except Exception as e:
                _log("plugins.base_import_failed", Severity.WARN, base=base, error=str(e))
                continue

            pkg_path = getattr(pkg, "__path__", None)
            if not pkg_path:
                continue

            for mod in pkgutil.walk_packages(pkg_path, pkg.__name__ + "."):
                try:
                    importlib.import_module(mod.name)
                except Exception as e:
                    _log("plugins.module_import_failed", Severity.WARN, module=mod.name, error=str(e))

        _log(
            "plugins.discovered",
            Severity.INFO,
            indicators_count=int(sum(len(v) for v in INDICATORS.values())),
            models_count=int(len(MODELS)),
            allocators_count=int(len(ALLOCATORS)),
            balancers_count=int(len(BALANCERS)),
        )

--- src\tycherion\application\plugins\registry.py:END ---

--- src\tycherion\application\runmodes\live_multimodel.py:START ---
from __future__ import annotations

import hashlib
import json
import time
from typing import Dict

from tycherion.shared.config import AppConfig
from tycherion.ports.trading import TradingPort
from tycherion.ports.account import AccountPort
from tycherion.ports.universe import UniversePort

from tycherion.ports.observability.observability import ObservabilityPort
from tycherion.ports.observability.types import Severity, TYCHERION_SCHEMA_VERSION

from tycherion.application.plugins.registry import (
    ALLOCATORS,
    BALANCERS,
)
from tycherion.application.services.coverage_selector import build_coverage
from tycherion.application.services.order_planner import build_orders
from tycherion.domain.portfolio.entities import (
    PortfolioSnapshot,
    Position,
)

from tycherion.application.pipeline.config import build_pipeline_config
from tycherion.application.pipeline.service import ModelPipelineService


def _build_portfolio_snapshot(account: AccountPort) -> PortfolioSnapshot:
    equity = float(account.equity())
    positions: Dict[str, Position] = {}
    for p in account.positions():
        positions[p.symbol] = p
    return PortfolioSnapshot(equity=equity, positions=positions)


def _stable_config_hash(d: dict) -> str:
    try:
        blob = json.dumps(d, sort_keys=True, default=str).encode("utf-8")
        return hashlib.sha256(blob).hexdigest()[:16]
    except Exception:
        return ""


def run_live_multimodel(
    cfg: AppConfig,
    trader: TradingPort,
    account: AccountPort,
    universe: UniversePort,
    pipeline_service: ModelPipelineService,
    *,
    observability: ObservabilityPort,
    config_path: str | None = None,
) -> None:
    """Live runmode that delegates per-symbol pipeline execution to ModelPipelineService."""

    allocator = ALLOCATORS.get(cfg.application.portfolio.allocator)
    if not allocator:
        raise RuntimeError(f"Allocator not found: {cfg.application.portfolio.allocator!r}")

    balancer = BALANCERS.get(cfg.application.portfolio.balancer)
    if not balancer:
        raise RuntimeError(f"Balancer not found: {cfg.application.portfolio.balancer!r}")

    pipeline_config = build_pipeline_config(cfg)

    tracer = observability.traces.get_tracer("tycherion.runmodes.live_multimodel", version=TYCHERION_SCHEMA_VERSION)
    logger = observability.logs.get_logger("tycherion.runmodes.live_multimodel", version=TYCHERION_SCHEMA_VERSION)

    def step_once() -> None:
        cfg_hash = _stable_config_hash(cfg.model_dump())

        with tracer.start_as_current_span(
            "run",
            attributes={
                "run_mode": "live_multimodel",
                "timeframe": cfg.timeframe,
                "lookback_days": int(cfg.lookback_days),
                "pipeline_stages": [st.name for st in pipeline_config.stages],
                "config_hash": cfg_hash,
                "config_path": config_path,
            },
        ) as span_run:
            try:
                # 1) Structural universe from coverage + ensure held symbols are included
                with tracer.start_as_current_span("coverage.fetch") as span_cov:
                    coverage = build_coverage(cfg, pipeline_service.market_data, universe)
                    portfolio = _build_portfolio_snapshot(account)
                    held_symbols = set(portfolio.positions.keys())
                    universe_symbols = sorted(set(coverage) | held_symbols)

                    span_cov.add_event(
                        "coverage.summary",
                        {
                            "symbols_count": int(len(universe_symbols)),
                            "symbols_sample": universe_symbols[: min(10, len(universe_symbols))],
                        },
                    )

                # 2) Run pipeline (single entrypoint)
                result = pipeline_service.run(
                    universe_symbols=universe_symbols,
                    portfolio_snapshot=portfolio,
                    pipeline_config=pipeline_config,
                    observability=observability,
                )

                span_run.add_event(
                    "pipeline.run_summary",
                    {"stage_stats": dict(result.stage_stats or {})},
                )

                # 3) Allocation -> target weights
                with tracer.start_as_current_span("allocator") as span_alloc:
                    target_alloc = allocator.allocate(result.signals_by_symbol)
                    span_alloc.add_event("allocator.completed", {"symbols_count": int(len(result.signals_by_symbol))})

                # 4) Balancing -> rebalance plan
                with tracer.start_as_current_span("balancer") as span_bal:
                    plan = balancer.plan(
                        portfolio=portfolio,
                        target=target_alloc,
                        threshold=cfg.application.portfolio.threshold_weight,
                    )
                    span_bal.add_event("rebalance.plan_built", {"instructions_count": int(len(plan))})

                # 5) Orders -> execution
                with tracer.start_as_current_span("execution") as span_exec:
                    orders = build_orders(portfolio, plan, cfg.trading)
                    span_exec.add_event("orders.built", {"orders_count": int(len(orders))})

                    for od in orders:
                        if od.side.upper() == "BUY":
                            res = trader.market_buy(od.symbol, volume=od.volume)
                        else:
                            res = trader.market_sell(od.symbol, volume=od.volume)

                        logger.emit(
                            "trade.executed",
                            Severity.INFO,
                            {
                                "tycherion.channel": "ops",
                                "symbol": od.symbol,
                                "side": od.side,
                                "volume": float(od.volume),
                                "result": str(res),
                            },
                        )

                span_run.set_status_ok()
            except BaseException as e:
                span_run.record_exception(e)
                span_run.set_status_error(str(e))
                logger.emit(
                    "run.exception",
                    Severity.ERROR,
                    {
                        "tycherion.channel": "ops",
                        "run_mode": "live_multimodel",
                        "exception_type": type(e).__name__,
                        "message": str(e),
                    },
                )
                raise

    if cfg.application.schedule.run_forever:
        while True:
            try:
                step_once()
                time.sleep(max(1, cfg.application.schedule.interval_seconds))
            except KeyboardInterrupt:
                logger.emit(
                    "run.stopped",
                    Severity.INFO,
                    {
                        "tycherion.channel": "ops",
                        "run_mode": "live_multimodel",
                        "reason": "KeyboardInterrupt",
                    },
                )
                break
            except Exception as e:
                # Error already recorded inside the run span, but keep a top-level log too.
                logger.emit(
                    "run.loop_exception",
                    Severity.ERROR,
                    {
                        "tycherion.channel": "ops",
                        "run_mode": "live_multimodel",
                        "exception_type": type(e).__name__,
                        "message": str(e),
                    },
                )
                time.sleep(3)
    else:
        step_once()

--- src\tycherion\application\runmodes\live_multimodel.py:END ---

--- src\tycherion\application\services\coverage_selector.py:START ---
from __future__ import annotations

from tycherion.shared.config import AppConfig
from tycherion.ports.market_data import MarketDataPort
from tycherion.ports.universe import UniversePort


def _build_base_coverage(cfg: AppConfig, universe: UniversePort) -> list[str]:
    """Build the *structural* universe of symbols.

    Coverage is intentionally dumb. It only answers: *which* symbols should be
    considered, based on the configured source. Any kind of "smart filtering"
    (liquidity, regimes, sanity checks, alpha, etc.) must live in the model
    pipeline, not here.
    """
    src = (cfg.application.coverage.source or "").lower()
    if src == "static":
        # Remove duplicates while preserving order
        return list(dict.fromkeys(cfg.application.coverage.symbols or []))
    if src == "market_watch":
        return universe.visible_symbols()
    if src == "pattern":
        patt = cfg.application.coverage.pattern or "*"
        return universe.by_pattern(patt)
    return universe.visible_symbols()


def build_coverage(cfg: AppConfig, data: MarketDataPort, universe: UniversePort) -> list[str]:
    """Build the list of symbols to analyse in this run.

    NOTE: `data` is kept in the signature for backward compatibility, but is
    intentionally unused. The universe thinning that previously depended on
    recent `tick_volume` (coverage.top_n) is deprecated and removed.
    """
    _ = data  # explicit unused
    return _build_base_coverage(cfg, universe)

--- src\tycherion\application\services\coverage_selector.py:END ---

--- src\tycherion\application\services\ensemble.py:START ---
# application/services/ensemble.py (versão nova)

from __future__ import annotations

from typing import List
from tycherion.domain.signals.entities import ModelDecision, AggregatedDecision


def combine(decisions: List[ModelDecision]) -> AggregatedDecision:
    """
    Combina uma lista de ModelDecision em uma decisão agregada única.
    """
    if not decisions:
        return AggregatedDecision(
            side="HOLD",
            weight=0.0,
            confidence=0.0,
            signed=0.0,
        )

    num, den = 0.0, 0.0
    for d in decisions:
        side = (d.side or "HOLD").upper()
        w = float(d.weight)
        c = float(d.confidence if d.confidence is not None else 0.5)
        c = max(0.0, min(1.0, c))

        if side == "BUY":
            signed = w
        elif side == "SELL":
            signed = -w
        else:
            signed = 0.0

        num += signed * c
        den += c

    if den <= 0:
        return AggregatedDecision(
            side="HOLD",
            weight=0.0,
            confidence=0.0,
            signed=0.0,
        )

    s = num / den
    side = "BUY" if s > 0.1 else ("SELL" if s < -0.1 else "HOLD")
    weight = min(1.0, abs(s))
    confidence = min(1.0, den / max(1, len(decisions)))

    return AggregatedDecision(
        side=side,
        weight=weight,
        confidence=confidence,
        signed=s,
    )


--- src\tycherion\application\services\ensemble.py:END ---

--- src\tycherion\application\services\order_planner.py:START ---
from __future__ import annotations

from dataclasses import dataclass
from typing import List

from tycherion.domain.portfolio.entities import PortfolioSnapshot, RebalanceInstruction
from tycherion.shared.config import Trading


@dataclass
class SuggestedOrder:
    symbol: str
    side: str   # "BUY" | "SELL"
    volume: float


def build_orders(
    portfolio: PortfolioSnapshot,
    plan: List[RebalanceInstruction],
    trading_cfg: Trading,
) -> List[SuggestedOrder]:
    """
    Convert domain-level rebalance instructions (expressed in weights) into
    concrete order suggestions with broker volumes. This is the point where
    we cross from the pure portfolio domain into broker-specific constraints.
    """
    # Lazy import to avoid circular deps
    from tycherion.application.services.sizer import (
        volume_from_weight,
        symbol_min_volume,
    )

    orders: List[SuggestedOrder] = []
    for instr in plan:
        # For now we scale volumes solely by absolute delta_weight. In the
        # future this can incorporate volatility, risk, etc.
        w = abs(float(instr.delta_weight))
        if w <= 0.0:
            continue

        vol = volume_from_weight(
            instr.symbol,
            w,
            trading_cfg.volume_mode,
            trading_cfg.fixed_volume,
        )
        min_vol = symbol_min_volume(instr.symbol)
        vol = max(vol, min_vol)
        if vol <= 0.0:
            continue

        orders.append(
            SuggestedOrder(
                symbol=instr.symbol,
                side=instr.side,
                volume=vol,
            )
        )
    return orders


--- src\tycherion\application\services\order_planner.py:END ---

--- src\tycherion\application\services\sizer.py:START ---
from __future__ import annotations
import MetaTrader5 as mt5

def symbol_min_volume(symbol: str) -> float:
    info = mt5.symbol_info(symbol)
    if not info:
        return 0.0
    v = max(info.volume_min, info.volume_step)
    steps = round(v / info.volume_step)
    return steps * info.volume_step

def volume_from_weight(symbol: str, weight: float, mode: str, fixed_volume: float) -> float:
    weight = max(0.0, min(1.0, float(weight)))
    if weight < 1e-6:
        return 0.0
    if mode == 'fixed':
        return float(fixed_volume) * weight
    return symbol_min_volume(symbol)


--- src\tycherion\application\services\sizer.py:END ---

--- src\tycherion\application\telemetry\event_factory.py:START ---
from __future__ import annotations

from datetime import datetime, timezone
from typing import Any, Mapping

from tycherion.ports.telemetry import TelemetryEvent, TelemetryLevel


def now_utc() -> datetime:
    return datetime.now(timezone.utc)


def make_event(
    *,
    schema_version: int,
    runner_id: str,
    trace_id: str,
    event_seq: int,
    ts_utc: datetime | None = None,
    mono_ns: int | None = None,
    span_id: str | None = None,
    parent_span_id: str | None = None,
    name: str,
    level: str | TelemetryLevel,
    channel: str,
    attributes: Mapping[str, Any] | None = None,
    data: Mapping[str, Any] | None = None,
) -> TelemetryEvent:
    return TelemetryEvent(
        schema_version=int(schema_version),
        runner_id=str(runner_id),
        trace_id=str(trace_id),
        event_seq=int(event_seq),
        ts_utc=(ts_utc or now_utc()),
        mono_ns=(int(mono_ns) if mono_ns is not None else None),
        span_id=(str(span_id) if span_id is not None else None),
        parent_span_id=(str(parent_span_id) if parent_span_id is not None else None),
        name=str(name),
        level=TelemetryLevel.coerce(level),
        channel=str(channel),
        attributes=(dict(attributes or {}) if attributes is not None else None),
        data=dict(data or {}),
    )

--- src\tycherion\application\telemetry\event_factory.py:END ---

--- src\tycherion\application\telemetry\hub.py:START ---
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Mapping

from tycherion.ports.telemetry import TelemetryEvent, TelemetryLevel, TelemetryPort, TelemetrySink


@dataclass(slots=True)
class TelemetryHub(TelemetryPort):
    """Fan-out hub.

    The hub is application-layer infrastructure: it receives canonical
    TelemetryEvent envelopes and forwards them to sinks. Sinks can filter
    independently.
    """

    sinks: list[TelemetrySink]
    base_attributes: Mapping[str, Any] | None = None

    def emit(self, event: TelemetryEvent) -> None:
        attributes = dict(self.base_attributes or {})
        if event.attributes:
            attributes.update(dict(event.attributes))
        merged = TelemetryEvent(
            schema_version=event.schema_version,
            runner_id=event.runner_id,
            trace_id=event.trace_id,
            event_seq=event.event_seq,
            ts_utc=event.ts_utc,
            mono_ns=event.mono_ns,
            span_id=event.span_id,
            parent_span_id=event.parent_span_id,
            name=event.name,
            level=event.level,
            channel=event.channel,
            attributes=attributes if attributes else None,
            data=dict(event.data or {}),
        )

        for s in list(self.sinks):
            try:
                if not s.enabled(merged.channel, merged.level, merged.name):
                    continue
                s.emit(merged)
            except Exception:
                continue

    def enabled(self, channel: str, level: str | TelemetryLevel) -> bool:
        lv = TelemetryLevel.coerce(level)
        for s in list(self.sinks):
            try:
                if s.enabled(str(channel), lv, None):
                    return True
            except Exception:
                continue
        return False

    def child(self, attributes: Mapping[str, Any]) -> TelemetryPort:
        merged = dict(self.base_attributes or {})
        merged.update(dict(attributes or {}))
        return TelemetryHub(sinks=self.sinks, base_attributes=merged)

    def flush(self) -> None:
        for s in list(self.sinks):
            flush = getattr(s, "flush", None)
            if callable(flush):
                try:
                    flush()
                except Exception:
                    continue

    def close(self) -> None:
        for s in list(self.sinks):
            close = getattr(s, "close", None)
            if callable(close):
                try:
                    close()
                except Exception:
                    continue

--- src\tycherion\application\telemetry\hub.py:END ---

--- src\tycherion\application\telemetry\ids.py:START ---
from __future__ import annotations

import re
from datetime import datetime, timezone


_RUNNER_SAFE_RE = re.compile(r"[^a-zA-Z0-9._-]+")


def sanitize_runner_id(runner_id: str) -> str:
    """Make runner_id safe to embed into trace_id strings.

    We keep this intentionally conservative: only alnum, dot, underscore, dash.
    """

    runner_id = (runner_id or "").strip()
    if not runner_id:
        return "runner-unknown"
    runner_id = _RUNNER_SAFE_RE.sub("_", runner_id)
    return runner_id[:80]  # keep IDs compact


def format_ts_compact(ts_utc: datetime) -> str:
    """UTC timestamp as YYYYMMDDHHMMSSffffff (microseconds)."""

    if ts_utc.tzinfo is None:
        ts_utc = ts_utc.replace(tzinfo=timezone.utc)
    ts_utc = ts_utc.astimezone(timezone.utc)
    return ts_utc.strftime("%Y%m%d%H%M%S%f")


def make_trace_id(runner_id: str, ts_utc: datetime, trace_seq: int) -> str:
    runner_id = sanitize_runner_id(runner_id)
    return f"{runner_id}-{format_ts_compact(ts_utc)}-{trace_seq:04x}"


def make_span_id(span_seq: int) -> str:
    return f"{span_seq:016x}"

--- src\tycherion\application\telemetry\ids.py:END ---

--- src\tycherion\application\telemetry\provider.py:START ---
from __future__ import annotations

import itertools
from dataclasses import dataclass
from typing import Any, Mapping

from tycherion.application.telemetry.event_factory import now_utc
from tycherion.application.telemetry.ids import make_trace_id, sanitize_runner_id
from tycherion.application.telemetry.trace import TraceTelemetry
from tycherion.ports.telemetry import TelemetryPort


@dataclass(slots=True)
class TelemetryProvider:
    """Factory for per-run TraceTelemetry objects.

    Centralising trace creation prevents each runmode from inventing its own trace_id
    and keeps ID format consistent across the system.
    """

    runner_id: str
    hub: TelemetryPort
    _trace_counter: Any = None

    def __post_init__(self) -> None:
        self.runner_id = sanitize_runner_id(self.runner_id)
        # start at 0, so the first next() is 1
        self._trace_counter = itertools.count(1)

    def new_trace(self, base_attributes: Mapping[str, Any] | None = None) -> TraceTelemetry:
        ts = now_utc()
        seq = int(next(self._trace_counter))
        trace_id = make_trace_id(self.runner_id, ts, seq)
        return TraceTelemetry(
            port=self.hub,
            runner_id=self.runner_id,
            trace_id=trace_id,
            base_attributes=(dict(base_attributes or {}) if base_attributes else None),
        )

    def flush(self) -> None:
        try:
            self.hub.flush()
        except Exception:
            return

    def close(self) -> None:
        try:
            self.hub.close()
        except Exception:
            return

--- src\tycherion\application\telemetry\provider.py:END ---

--- src\tycherion\application\telemetry\trace.py:START ---
from __future__ import annotations

import hashlib
import json
import time
import threading
from contextvars import ContextVar
from dataclasses import dataclass
from typing import Any, Mapping

from tycherion.application.telemetry.event_factory import make_event, now_utc
from tycherion.application.telemetry.ids import make_span_id
from tycherion.ports.telemetry import TelemetryEvent, TelemetryLevel, TelemetryPort

# Bump when TelemetryEvent fields change in a backwards-incompatible way.
SCHEMA_VERSION = 3

# Per-task span stack (works for sync, async, nested contexts)
_SPAN_STACK: ContextVar[tuple[str, ...]] = ContextVar("tycherion_span_stack", default=())


def stable_config_hash(
    cfg_dump: Mapping[str, Any],
    *,
    redactions: Mapping[str, set[str]] | None = None,
) -> str:
    """Best-effort hash of a config dump, with sensitive fields redacted.

    `cfg_dump` is expected to be the output of pydantic's `model_dump()`.
    """

    redactions = redactions or {"mt5": {"password", "login", "server"}}

    payload = json.loads(json.dumps(cfg_dump, default=str))  # ensure JSON-ish types
    for section, keys in redactions.items():
        node = payload.get(section)
        if isinstance(node, dict):
            for k in keys:
                if k in node:
                    node[k] = "***"

    raw = json.dumps(payload, sort_keys=True, separators=(",", ":"))
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()


class _TraceState:
    """Mutable, shared state for one trace (shared across child tracers)."""

    __slots__ = ("span_seq", "event_seq", "_lock")

    def __init__(self) -> None:
        self.span_seq = 0
        self.event_seq = 0
        self._lock = threading.Lock()

    def next_span_id(self) -> str:
        with self._lock:
            self.span_seq += 1
            return make_span_id(self.span_seq)

    def next_event_seq(self) -> int:
        with self._lock:
            self.event_seq += 1
            return int(self.event_seq)


@dataclass(frozen=True, slots=True)
class TraceTelemetry:
    """A lightweight tracer aligned with standard observability vocabulary.

    This is intentionally NOT OpenTelemetry SDK. It's a small abstraction that
    keeps the mental model: runner_id + trace_id + spans + structured events.
    """

    port: TelemetryPort | None
    runner_id: str
    trace_id: str
    base_attributes: Mapping[str, Any] | None = None
    _state: Any | None = None

    def __post_init__(self) -> None:
        # dataclass(frozen=True): use object.__setattr__
        if self._state is None:
            object.__setattr__(self, "_state", _TraceState())

    def enabled(self, channel: str, level: str | TelemetryLevel) -> bool:
        try:
            return bool(self.port and self.port.enabled(channel, level))
        except Exception:
            return False

    def child(self, attributes: Mapping[str, Any]) -> "TraceTelemetry":
        merged = dict(self.base_attributes or {})
        merged.update(dict(attributes or {}))
        return TraceTelemetry(
            port=self.port,
            runner_id=self.runner_id,
            trace_id=self.trace_id,
            base_attributes=merged,
            _state=self._state,
        )

    def _emit_built(self, ev: TelemetryEvent) -> None:
        if not self.port:
            return
        try:
            self.port.emit(ev)
        except Exception:
            return

    def emit(
        self,
        *,
        name: str,
        level: str | TelemetryLevel,
        channel: str,
        attributes: Mapping[str, Any] | None = None,
        data: Mapping[str, Any] | None = None,
        span_id: str | None = None,
        parent_span_id: str | None = None,
        mono_ns: int | None = None,
        ts_utc: Any | None = None,
    ) -> None:
        if not self.port:
            return

        lvl = TelemetryLevel.coerce(level)
        # Gating first: do not even increment event_seq if nobody is listening.
        try:
            if not self.port.enabled(channel, lvl):
                return
        except Exception:
            return

        try:
            event_seq = self._state.next_event_seq()  # type: ignore[union-attr]
            base = dict(self.base_attributes or {})
            if attributes:
                base.update(dict(attributes))

            stack = _SPAN_STACK.get()
            resolved_span_id = span_id if span_id is not None else (stack[-1] if stack else None)
            resolved_parent_span_id = (
                parent_span_id
                if parent_span_id is not None
                else (stack[-2] if len(stack) >= 2 else None)
            )

            ev = make_event(
                schema_version=SCHEMA_VERSION,
                runner_id=self.runner_id,
                trace_id=self.trace_id,
                event_seq=event_seq,
                ts_utc=(ts_utc or now_utc()),
                mono_ns=mono_ns,
                span_id=resolved_span_id,
                parent_span_id=resolved_parent_span_id,
                name=name,
                level=lvl,
                channel=channel,
                attributes=base if base else None,
                data=data or {},
            )
            self._emit_built(ev)
        except Exception:
            return

    def span(
        self,
        name: str,
        *,
        channel: str = "ops",
        level: str | TelemetryLevel = TelemetryLevel.INFO,
        attributes: Mapping[str, Any] | None = None,
        data: Mapping[str, Any] | None = None,
    ) -> "Span":
        return Span(
            tracer=self,
            name=str(name),
            channel=str(channel),
            level=TelemetryLevel.coerce(level),
            attributes=dict(attributes or {}) if attributes else None,
            data=dict(data or {}) if data else None,
        )


@dataclass(slots=True)
class Span:
    tracer: TraceTelemetry
    name: str
    channel: str
    level: TelemetryLevel
    attributes: Mapping[str, Any] | None
    data: Mapping[str, Any] | None

    span_id: str | None = None
    parent_span_id: str | None = None
    _token: Any | None = None
    _t0_mono: int | None = None

    def __enter__(self) -> "Span":
        self._t0_mono = time.monotonic_ns()

        before = _SPAN_STACK.get()
        self.parent_span_id = before[-1] if before else None
        self.span_id = self.tracer._state.next_span_id()  # type: ignore[union-attr]

        self._token = _SPAN_STACK.set(before + (self.span_id,))

        self.tracer.emit(
            name=f"{self.name}.started",
            level=self.level,
            channel=self.channel,
            attributes=self.attributes,
            data=self.data,
            span_id=self.span_id,
            parent_span_id=self.parent_span_id,
            mono_ns=self._t0_mono,
        )
        return self

    def __exit__(self, exc_type, exc, tb) -> bool:
        try:
            t1 = time.monotonic_ns()
            duration_ms = int(round((t1 - int(self._t0_mono or t1)) / 1_000_000))

            if exc is None:
                self.tracer.emit(
                    name=f"{self.name}.finished",
                    level=TelemetryLevel.INFO,
                    channel=self.channel,
                    attributes=self.attributes,
                    data={"duration_ms": duration_ms, "status": "ok"},
                    span_id=self.span_id,
                    parent_span_id=self.parent_span_id,
                    mono_ns=t1,
                )
                return False

            self.tracer.emit(
                name=f"{self.name}.failed",
                level=TelemetryLevel.ERROR,
                channel=self.channel,
                attributes=self.attributes,
                data={
                    "duration_ms": duration_ms,
                    "status": "error",
                    "exception_type": getattr(exc_type, "__name__", str(exc_type)),
                    "message": str(exc),
                },
                span_id=self.span_id,
                parent_span_id=self.parent_span_id,
                mono_ns=t1,
            )
            return False
        finally:
            try:
                if self._token is not None:
                    _SPAN_STACK.reset(self._token)
            except Exception:
                pass

--- src\tycherion\application\telemetry\trace.py:END ---

--- src\tycherion\application\telemetry\__init__.py:START ---
from .event_factory import make_event, now_utc
from .hub import TelemetryHub
from .provider import TelemetryProvider
from .trace import SCHEMA_VERSION, TraceTelemetry, Span, stable_config_hash

__all__ = [
    "make_event",
    "now_utc",
    "TelemetryHub",
    "TelemetryProvider",
    "TraceTelemetry",
    "Span",
    "stable_config_hash",
    "SCHEMA_VERSION",
]

--- src\tycherion\application\telemetry\__init__.py:END ---

--- src\tycherion\bootstrap\main.py:START ---
from __future__ import annotations

import os
import socket

import MetaTrader5 as mt5

from tycherion.shared.config import load_config, AppConfig
from tycherion.adapters.mt5.market_data_mt5 import MT5MarketData
from tycherion.adapters.mt5.trading_mt5 import MT5Trader
from tycherion.adapters.mt5.account_mt5 import MT5Account
from tycherion.adapters.mt5.universe_mt5 import MT5Universe

from tycherion.adapters.observability.noop.noop_observability import NoopObservability

from tycherion.ports.observability.observability import ObservabilityPort
from tycherion.ports.observability.types import Severity, TYCHERION_SCHEMA_VERSION

from tycherion.application.plugins import registry as _registry
from tycherion.application.pipeline.service import ModelPipelineService
from tycherion.application.runmodes.live_multimodel import run_live_multimodel


def _ensure_initialized(cfg: AppConfig) -> None:
    if not mt5.initialize(path=cfg.mt5.terminal_path or None):
        raise SystemExit(f"MT5 initialize failed: {mt5.last_error()}")
    if cfg.mt5.login and cfg.mt5.password and cfg.mt5.server:
        if not mt5.login(
            login=int(cfg.mt5.login),
            password=cfg.mt5.password,
            server=cfg.mt5.server,
        ):
            raise SystemExit(f"MT5 login failed: {mt5.last_error()}")


def run_app(config_path: str) -> None:
    cfg = load_config(config_path)

    # Observability must be available as early as possible (e.g. plugin discovery).
    obs = _build_observability(cfg, config_path)

    tracer = obs.traces.get_tracer("tycherion.bootstrap", version=TYCHERION_SCHEMA_VERSION)
    logger = obs.logs.get_logger("tycherion.bootstrap", version=TYCHERION_SCHEMA_VERSION)

    with tracer.start_as_current_span("bootstrap.discover", attributes={"component": "bootstrap"}):
        _registry.auto_discover(observability=obs)
        logger.emit("Plugin discovery completed", Severity.INFO, {"tycherion.channel": "ops"})

    _ensure_initialized(cfg)
    try:
        market_data = MT5MarketData()
        trader = MT5Trader(
            dry_run=cfg.trading.dry_run,
            require_demo=cfg.trading.require_demo,
            deviation_points=cfg.trading.deviation_points,
            volume_mode=cfg.trading.volume_mode,
            fixed_volume=cfg.trading.fixed_volume,
        )
        account = MT5Account()
        universe = MT5Universe()

        pipeline_service = ModelPipelineService(
            market_data=market_data,
            model_registry=_registry.MODELS,
            indicator_picker=_registry.pick_indicator_for,
            timeframe=cfg.timeframe,
            lookback_days=cfg.lookback_days,
            playbook=cfg.application.playbook,
        )

        run_mode = (cfg.application.run_mode.name or "").lower()
        if run_mode == "live_multimodel":
            run_live_multimodel(
                cfg,
                trader,
                account,
                universe,
                pipeline_service,
                observability=obs,
                config_path=config_path,
            )
        else:
            raise SystemExit(f"Unknown run_mode: {run_mode}")
    finally:
        try:
            obs.shutdown()
        except Exception:
            pass
        mt5.shutdown()


def _parse_severity(level: str | None) -> Severity:
    lvl = (level or "INFO").strip().upper()
    try:
        return Severity[lvl]
    except Exception:
        # Accept legacy values too
        if lvl in ("WARNING",):
            return Severity.WARN
        return Severity.INFO


def _build_observability(cfg: AppConfig, config_path: str) -> ObservabilityPort:
    _ = config_path

    runner_id = (os.getenv("TYCHERION_RUNNER_ID") or "").strip()
    if not runner_id:
        # Fallback: deterministic enough for local dev.
        runner_id = f"runner-{socket.gethostname()}-{os.getpid()}"

    tel = cfg.telemetry

    try:
        from tycherion.adapters.observability.otel.otel_observability import (
            OtelObservability,
            OtelObservabilityConfig,
        )

        return OtelObservability(
            OtelObservabilityConfig(
                runner_id=runner_id,
                schema_version=TYCHERION_SCHEMA_VERSION,
                console_enabled=bool(tel.console_enabled),
                console_min_severity=_parse_severity(tel.console_min_level),
                console_show_span_lifecycle=True,
                otlp_enabled=bool(getattr(tel, "otlp_enabled", False)),
                otlp_endpoint=str(getattr(tel, "otlp_endpoint", "http://localhost:4317") or "http://localhost:4317"),
                mongo_audit_enabled=bool(tel.mongo_enabled),
                mongo_uri=str(tel.mongo_uri) if tel.mongo_uri else None,
                mongo_db=str(tel.mongo_db or "tycherion"),
                mongo_collection=str(tel.mongo_collection or "ops_journal"),
                mongo_min_severity=_parse_severity(tel.mongo_min_level),
                mongo_batch_size=int(tel.mongo_batch_size or 200),
            )
        )
    except Exception as e:
        # Hard-fail would be annoying during local dev if deps are missing, so we degrade to noop.
        print(f"[tycherion] Observability disabled (failed to init OTel adapter): {e}")
        return NoopObservability()

--- src\tycherion\bootstrap\main.py:END ---

--- src\tycherion\domain\__init__.py:START ---

--- src\tycherion\domain\__init__.py:END ---

--- src\tycherion\domain\market\entities.py:START ---
from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import NewType

Symbol = NewType("Symbol", str)


class AssetClass(str, Enum):
    EQUITY = "equity"
    FUTURE = "future"
    FX = "fx"
    OTHER = "other"


@dataclass
class Instrument:
    """Domain representation of a tradable instrument (stock, future, FX, etc.)."""

    symbol: Symbol
    asset_class: AssetClass
    currency: str
    lot_size: float
    min_volume: float
    volume_step: float


@dataclass
class Bar:
    """Minimal OHLCV bar used by indicators and models when not using DataFrame."""

    symbol: Symbol
    time: datetime
    open: float
    high: float
    low: float
    close: float
    volume: float

--- src\tycherion\domain\market\entities.py:END ---

--- src\tycherion\domain\market\__init__.py:START ---

--- src\tycherion\domain\market\__init__.py:END ---

--- src\tycherion\domain\portfolio\entities.py:START ---
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict


Symbol = str


@dataclass
class Signal:
    """Per-symbol signal produced by the models/ensemble.

    signed: desired direction/intensity in [-1, 1]
    confidence: optional confidence level in [0, 1]
    """

    symbol: Symbol
    signed: float
    confidence: float = 1.0


SignalsBySymbol = Dict[Symbol, Signal]


@dataclass
class Position:
    """Domain-level position in a single instrument.

    quantity: number of shares/contracts/etc.
    price: best available price estimate (e.g. last close or avg price).
    """

    symbol: Symbol
    quantity: float
    price: float


@dataclass
class PortfolioSnapshot:
    """Portfolio snapshot used by allocators/balancers at the domain level.

    Equity is the current account equity in account currency.
    """

    equity: float
    positions: Dict[Symbol, Position]

    def weight_of(self, symbol: Symbol) -> float:
        pos = self.positions.get(symbol)
        if not pos or self.equity <= 0:
            return 0.0
        return float(pos.quantity * pos.price) / float(self.equity)


@dataclass
class TargetAllocation:
    """Target portfolio allocation expressed as weights per symbol in [-1, 1].

    Positive weights are long exposure, negative weights are short exposure.
    """

    weights: Dict[Symbol, float]


@dataclass
class RebalanceInstruction:
    """Domain-level rebalance instruction expressed in weights, not broker
    volumes. Conversion to concrete order sizes happens in the application
    layer (order planner).
    """

    symbol: Symbol
    from_weight: float
    to_weight: float
    delta_weight: float
    side: str  # "BUY" | "SELL"

--- src\tycherion\domain\portfolio\entities.py:END ---

--- src\tycherion\domain\portfolio\__init__.py:START ---

--- src\tycherion\domain\portfolio\__init__.py:END ---

--- src\tycherion\domain\portfolio\allocators\base.py:START ---
from __future__ import annotations

from abc import ABC, abstractmethod

from tycherion.domain.portfolio.entities import SignalsBySymbol, TargetAllocation


class BaseAllocator(ABC):
    """Abstract base class for portfolio allocator plugins."""

    # Set by decorator
    name: str = ""
    tags: set[str] = set()

    @abstractmethod
    def allocate(self, signals: SignalsBySymbol) -> TargetAllocation:
        raise NotImplementedError

--- src\tycherion\domain\portfolio\allocators\base.py:END ---

--- src\tycherion\domain\portfolio\allocators\equal_weight.py:START ---
from __future__ import annotations

from tycherion.domain.portfolio.allocators.base import BaseAllocator
from tycherion.application.plugins.registry import register_allocator
from tycherion.domain.portfolio.entities import SignalsBySymbol, TargetAllocation


@register_allocator(name="equal_weight", tags={"default"})
class EqualWeightAllocator(BaseAllocator):
    """
    Simple allocator: gives the same absolute weight to all symbols that have
    a non-zero signal. Longs get +w, shorts get -w, holds get 0.
    """
    def allocate(self, signals: SignalsBySymbol) -> TargetAllocation:
        nonzero = [s for s in signals.values() if abs(float(s.signed)) > 1e-6]
        if not nonzero:
            # nothing to do
            return TargetAllocation(weights={})

        w = 1.0 / float(len(nonzero))
        weights: dict[str, float] = {}
        for sig in signals.values():
            if sig.signed > 0:
                weights[sig.symbol] = w
            elif sig.signed < 0:
                weights[sig.symbol] = -w
            else:
                weights[sig.symbol] = 0.0
        return TargetAllocation(weights=weights)

--- src\tycherion\domain\portfolio\allocators\equal_weight.py:END ---

--- src\tycherion\domain\portfolio\allocators\proportional.py:START ---
from __future__ import annotations

from tycherion.domain.portfolio.allocators.base import BaseAllocator
from tycherion.application.plugins.registry import register_allocator
from tycherion.domain.portfolio.entities import SignalsBySymbol, TargetAllocation


@register_allocator(name="proportional", tags={"default"})
class ProportionalAllocator(BaseAllocator):
    """
    Allocator that gives each symbol a weight proportional to the absolute
    value of its signal. Signals are normalised so that the sum of absolute
    weights is 1. Longs get +w, shorts get -w.
    """
    def allocate(self, signals: SignalsBySymbol) -> TargetAllocation:
        total = sum(abs(float(s.signed)) for s in signals.values())
        if total <= 1e-9:
            return TargetAllocation(weights={})

        weights: dict[str, float] = {}
        for sig in signals.values():
            if sig.signed == 0:
                weights[sig.symbol] = 0.0
            else:
                frac = abs(float(sig.signed)) / total
                weights[sig.symbol] = frac if sig.signed > 0 else -frac
        return TargetAllocation(weights=weights)

--- src\tycherion\domain\portfolio\allocators\proportional.py:END ---

--- src\tycherion\domain\portfolio\allocators\__init__.py:START ---

--- src\tycherion\domain\portfolio\allocators\__init__.py:END ---

--- src\tycherion\domain\portfolio\balancers\base.py:START ---
from __future__ import annotations

from abc import ABC, abstractmethod

from tycherion.domain.portfolio.entities import (
    PortfolioSnapshot,
    TargetAllocation,
    RebalanceInstruction,
)


class BaseBalancer(ABC):
    """Abstract base class for portfolio balancer / rebalancer plugins."""

    # Set by decorator
    name: str = ""
    tags: set[str] = set()

    @abstractmethod
    def plan(
        self,
        portfolio: PortfolioSnapshot,
        target: TargetAllocation,
        threshold: float = 0.25,
    ) -> list[RebalanceInstruction]:
        raise NotImplementedError

--- src\tycherion\domain\portfolio\balancers\base.py:END ---

--- src\tycherion\domain\portfolio\balancers\threshold.py:START ---
from __future__ import annotations

from tycherion.domain.portfolio.balancers.base import BaseBalancer
from tycherion.application.plugins.registry import register_balancer
from tycherion.domain.portfolio.entities import (
    PortfolioSnapshot,
    TargetAllocation,
    RebalanceInstruction,
)


@register_balancer(name="threshold", tags={"default"})
class ThresholdBalancer(BaseBalancer):
    """
    Domain-level balancer: generates rebalance instructions whenever the
    difference between current and target weight is greater than or equal
    to a configured threshold.
    """
    def plan(
        self,
        portfolio: PortfolioSnapshot,
        target: TargetAllocation,
        threshold: float = 0.25,
    ) -> list[RebalanceInstruction]:
        threshold = max(0.0, min(1.0, float(threshold)))
        instructions: list[RebalanceInstruction] = []

        symbols = set(target.weights.keys()) | set(portfolio.positions.keys())
        for sym in sorted(symbols):
            current_w = float(portfolio.weight_of(sym))
            target_w = float(target.weights.get(sym, 0.0))
            delta = target_w - current_w
            if abs(delta) < threshold:
                continue
            side = "BUY" if delta > 0 else "SELL"
            instructions.append(
                RebalanceInstruction(
                    symbol=sym,
                    from_weight=current_w,
                    to_weight=target_w,
                    delta_weight=delta,
                    side=side,
                )
            )
        return instructions

--- src\tycherion\domain\portfolio\balancers\threshold.py:END ---

--- src\tycherion\domain\portfolio\balancers\__init__.py:START ---

--- src\tycherion\domain\portfolio\balancers\__init__.py:END ---

--- src\tycherion\domain\signals\entities.py:START ---
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, List


@dataclass
class IndicatorOutput:
    """Standard output of an indicator for a single symbol.

    - score: aggregated metric in [-1, 1] (by convention in this project)
    - features: extra numeric features that models may consume.
    """

    score: float
    features: Dict[str, float]


@dataclass
class ModelDecision:
    """Per-model decision for a single symbol.

    side: "BUY" | "SELL" | "HOLD"
    weight: relative intensity (usually in [0, 1])
    confidence: confidence level in [0, 1]
    """

    side: str
    weight: float
    confidence: float

@dataclass
class AggregatedDecision:
    """
    Decisão agregada (ensemble) de todos os models para um símbolo.

    side       -> direção final ("BUY"/"SELL"/"HOLD")
    weight     -> intensidade em [0, 1]
    confidence -> confiança em [0, 1]
    signed     -> direção * intensidade em [-1, 1]
    """
    side: str
    weight: float
    confidence: float
    signed: float


@dataclass
class ModelStageResult:
    """Result for a symbol at a specific model stage in the pipeline."""

    model_name: str
    score: float


@dataclass
class SymbolState:
    """Mutable per-symbol state that flows through the analysis pipeline.

    This is intentionally generic so we can reuse it for universe filters,
    macro models and per-symbol alpha models over time.
    """
    symbol: str
    is_held: bool = False      # True if the symbol is currently in the portfolio
    alive: bool = True         # If False and not held, the symbol can be dropped from the pipeline

    base_score: float = 0.0    # Optional starting score (e.g. from simple filters)
    sanity_score: float = 0.0  # Data-quality / tradability / liquidity score
    macro_score: float = 0.0   # Macro / regime score for this symbol
    alpha_score: float = 0.0   # Final alpha-like score, typically coming from signal models

    pipeline_results: List[ModelStageResult] = field(default_factory=list)

    notes: Dict[str, float] = field(default_factory=dict)


--- src\tycherion\domain\signals\entities.py:END ---

--- src\tycherion\domain\signals\__init__.py:START ---

--- src\tycherion\domain\signals\__init__.py:END ---

--- src\tycherion\domain\signals\indicators\base.py:START ---
from __future__ import annotations

from abc import ABC, abstractmethod
import pandas as pd

from tycherion.domain.signals.entities import IndicatorOutput


class BaseIndicator(ABC):
    """Abstract base class for indicator plugins."""

    # Set by decorator
    key: str = ""
    method: str = ""
    tags: set[str] = set()

    @abstractmethod
    def compute(self, df: pd.DataFrame) -> IndicatorOutput:
        raise NotImplementedError

--- src\tycherion\domain\signals\indicators\base.py:END ---

--- src\tycherion\domain\signals\indicators\stretch_zscore.py:START ---
from __future__ import annotations

from tycherion.domain.signals.indicators.base import BaseIndicator
import pandas as pd

from tycherion.application.plugins.registry import register_indicator
from tycherion.domain.signals.entities import IndicatorOutput


@register_indicator(key="stretch", method="zscore_20", tags={"default"})
class StretchZScore20(BaseIndicator):
    period = 20

    def compute(self, df: pd.DataFrame) -> IndicatorOutput:
        if df.empty or len(df) < self.period:
            return IndicatorOutput(score=0.0, features={})
        close = df["close"].astype(float)
        ma = close.rolling(self.period).mean()
        sd = close.rolling(self.period).std(ddof=0).replace(0, 1e-9)
        z = (close - ma) / sd
        zval = float(z.iloc[-1])
        score = max(-1.0, min(1.0, -zval / 3.0))
        return IndicatorOutput(score=score, features={"z": zval})

--- src\tycherion\domain\signals\indicators\stretch_zscore.py:END ---

--- src\tycherion\domain\signals\indicators\trend_donchian.py:START ---
from __future__ import annotations

from tycherion.domain.signals.indicators.base import BaseIndicator
import pandas as pd

from tycherion.application.plugins.registry import register_indicator
from tycherion.domain.signals.entities import IndicatorOutput


@register_indicator(key="trend", method="donchian_50_50", tags={"default"})
class TrendDonchian5050(BaseIndicator):
    high_n = 50
    low_n = 50

    def compute(self, df: pd.DataFrame) -> IndicatorOutput:
        if df.empty or len(df) < max(self.high_n, self.low_n):
            return IndicatorOutput(score=0.0, features={})
        hh = df["high"].rolling(self.high_n).max()
        ll = df["low"].rolling(self.low_n).min()
        mid = (hh + ll) / 2.0
        rng = (hh - ll).replace(0, 1e-9)
        pos = (df["close"] - mid) / (rng / 2.0)
        score = float(pos.iloc[-1])
        score = max(-1.0, min(1.0, score))
        return IndicatorOutput(
            score=score,
            features={"upper": float(hh.iloc[-1]), "lower": float(ll.iloc[-1])},
        )

--- src\tycherion\domain\signals\indicators\trend_donchian.py:END ---

--- src\tycherion\domain\signals\indicators\volatility_atr.py:START ---
from __future__ import annotations

from tycherion.domain.signals.indicators.base import BaseIndicator
import pandas as pd

from tycherion.application.plugins.registry import register_indicator
from tycherion.domain.signals.entities import IndicatorOutput


@register_indicator(key="volatility", method="atr_14", tags={"default"})
class VolATR14(BaseIndicator):
    period = 14

    def compute(self, df: pd.DataFrame) -> IndicatorOutput:
        if df.empty or len(df) < self.period + 1:
            return IndicatorOutput(score=0.0, features={})
        high = df["high"].astype(float)
        low = df["low"].astype(float)
        close = df["close"].astype(float)
        prev_close = close.shift(1)
        tr = (high - low).abs()
        tr = pd.concat(
            [tr, (high - prev_close).abs(), (low - prev_close).abs()], axis=1
        ).max(axis=1)
        atr = tr.rolling(self.period).mean()
        val = float(atr.iloc[-1])
        score = 1.0 / (1.0 + val) if val > 0 else 0.0
        return IndicatorOutput(score=score, features={"atr": val})

--- src\tycherion\domain\signals\indicators\volatility_atr.py:END ---

--- src\tycherion\domain\signals\indicators\__init__.py:START ---

--- src\tycherion\domain\signals\indicators\__init__.py:END ---

--- src\tycherion\domain\signals\models\base.py:START ---
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Dict

from tycherion.domain.signals.entities import IndicatorOutput, ModelDecision


class SignalModel(ABC):
    """Abstract base class for per-symbol signal models."""

    name: str = ""
    tags: set[str] = set()

    @abstractmethod
    def requires(self) -> set[str]:
        raise NotImplementedError

    @abstractmethod
    def decide(self, indicators: Dict[str, IndicatorOutput]) -> ModelDecision:
        raise NotImplementedError

--- src\tycherion\domain\signals\models\base.py:END ---

--- src\tycherion\domain\signals\models\mean_reversion.py:START ---
from __future__ import annotations

from tycherion.domain.signals.models.base import SignalModel
from typing import Dict

from tycherion.application.plugins.registry import register_model
from tycherion.domain.signals.entities import IndicatorOutput, ModelDecision


@register_model(name="mean_reversion", tags={"default"})
class MeanReversion(SignalModel):
    def requires(self) -> set[str]:
        return {"stretch", "volatility"}

    def decide(self, indicators: Dict[str, IndicatorOutput]) -> ModelDecision:
        stretch = indicators.get("stretch") if indicators is not None else None
        z = float(stretch.features.get("z", 0.0)) if stretch else 0.0

        if z <= -2.0:
            w = min(1.0, abs(z) / 3.0)
            return ModelDecision(side="BUY", weight=w, confidence=0.6)
        if z >= 2.0:
            w = min(1.0, abs(z) / 3.0)
            return ModelDecision(side="SELL", weight=w, confidence=0.6)
        return ModelDecision(side="HOLD", weight=0.0, confidence=0.4)


--- src\tycherion\domain\signals\models\mean_reversion.py:END ---

--- src\tycherion\domain\signals\models\trend_following.py:START ---
from __future__ import annotations

from tycherion.domain.signals.models.base import SignalModel
from typing import Dict

from tycherion.application.plugins.registry import register_model
from tycherion.domain.signals.entities import IndicatorOutput, ModelDecision


@register_model(name="trend_following", tags={"default"})
class TrendFollowing(SignalModel):
    def requires(self) -> set[str]:
        return {"trend", "volatility"}

    def decide(self, indicators: Dict[str, IndicatorOutput]) -> ModelDecision:
        trend = indicators.get("trend") if indicators is not None else None
        tr = float(trend.score) if trend else 0.0

        if tr > 0.2:
            return ModelDecision(
                side="BUY",
                weight=min(1.0, 0.5 + tr * 0.5),
                confidence=0.7,
            )
        if tr < -0.2:
            return ModelDecision(
                side="SELL",
                weight=min(1.0, 0.5 + (-tr) * 0.5),
                confidence=0.7,
            )
        return ModelDecision(side="HOLD", weight=0.0, confidence=0.3)


--- src\tycherion\domain\signals\models\trend_following.py:END ---

--- src\tycherion\domain\signals\models\__init__.py:START ---

--- src\tycherion\domain\signals\models\__init__.py:END ---

--- src\tycherion\ports\account.py:START ---
from __future__ import annotations

from typing import Protocol, List

from tycherion.domain.portfolio.entities import Position


class AccountPort(Protocol):
    def is_demo(self) -> bool: ...
    def balance(self) -> float: ...
    def equity(self) -> float: ...
    def positions(self) -> List[Position]: ...

--- src\tycherion\ports\account.py:END ---

--- src\tycherion\ports\market_data.py:START ---
from __future__ import annotations
from typing import Protocol
from datetime import datetime
import pandas as pd

class MarketDataPort(Protocol):
    def get_bars(self, symbol: str, timeframe: str, start: datetime, end: datetime) -> pd.DataFrame: ...

--- src\tycherion\ports\market_data.py:END ---

--- src\tycherion\ports\telemetry.py:START ---
from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any, Mapping, Protocol


class TelemetryLevel(str, Enum):
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARN = "WARN"
    ERROR = "ERROR"

    @classmethod
    def coerce(cls, value: str | "TelemetryLevel") -> "TelemetryLevel":
        if isinstance(value, TelemetryLevel):
            return value
        v = (value or "INFO").upper().strip()
        try:
            return TelemetryLevel(v)
        except Exception:
            return TelemetryLevel.INFO

    def rank(self) -> int:
        order = {
            TelemetryLevel.DEBUG: 10,
            TelemetryLevel.INFO: 20,
            TelemetryLevel.WARN: 30,
            TelemetryLevel.ERROR: 40,
        }
        return int(order[self])


@dataclass(frozen=True, slots=True)
class TelemetryEvent:
    """Canonical, small telemetry envelope.

    Design goals:
    - stable schema (versioned)
    - append-only journaling friendly
    - deterministic ordering within a trace (event_seq)

    NOTE: `attributes` and `data` must be JSON-serialisable.
    """

    schema_version: int
    runner_id: str
    trace_id: str

    # Order within the trace (1..N). Useful for ordering + idempotent dedupe in sinks.
    event_seq: int

    # When the event happened (audit / timeline)
    ts_utc: datetime

    # Optional monotonic timestamp for precision timing (not affected by NTP).
    mono_ns: int | None

    # Hierarchy
    span_id: str | None
    parent_span_id: str | None

    # Semantic payload
    name: str
    level: TelemetryLevel
    channel: str
    attributes: Mapping[str, Any] | None
    data: Mapping[str, Any]


class TelemetrySink(Protocol):
    """Adapter-side sink.

    A sink can filter independently. The hub will call `enabled` for gating,
    then `emit` to persist/print.
    """

    def enabled(self, channel: str, level: TelemetryLevel, name: str | None = None) -> bool: ...

    def emit(self, event: TelemetryEvent) -> None: ...


class TelemetryPort(Protocol):
    """Application-facing telemetry API (fan-out hub + scoped wrappers)."""

    def emit(self, event: TelemetryEvent) -> None: ...

    def enabled(self, channel: str, level: str | TelemetryLevel) -> bool: ...

    def child(self, attributes: Mapping[str, Any]) -> "TelemetryPort": ...

    def flush(self) -> None: ...

    def close(self) -> None: ...

--- src\tycherion\ports\telemetry.py:END ---

--- src\tycherion\ports\trading.py:START ---
from __future__ import annotations
from dataclasses import dataclass
from typing import Protocol, Optional

@dataclass
class TradeResult:
    ok: bool
    retcode: int
    order: Optional[int]
    message: str

class TradingPort(Protocol):
    def market_buy(self, symbol: str, volume: Optional[float] = None) -> TradeResult: ...
    def market_sell(self, symbol: str, volume: Optional[float] = None) -> TradeResult: ...

--- src\tycherion\ports\trading.py:END ---

--- src\tycherion\ports\universe.py:START ---
from __future__ import annotations
from typing import Protocol, List

class UniversePort(Protocol):
    def visible_symbols(self) -> List[str]: ...
    def by_pattern(self, pattern: str) -> List[str]: ...

--- src\tycherion\ports\universe.py:END ---

--- src\tycherion\ports\observability\logs.py:START ---
from __future__ import annotations

from typing import Protocol, runtime_checkable

from .types import Attributes, Severity


@runtime_checkable
class LoggerPort(Protocol):
    def emit(self, body: str, severity: Severity, attributes: Attributes | None = None) -> None: ...
    def is_enabled(self, severity: Severity) -> bool: ...


@runtime_checkable
class LoggerProviderPort(Protocol):
    def get_logger(self, name: str, version: str | None = None) -> LoggerPort: ...

--- src\tycherion\ports\observability\logs.py:END ---

--- src\tycherion\ports\observability\metrics.py:START ---
from __future__ import annotations

from typing import Protocol, runtime_checkable

from .types import Attributes


@runtime_checkable
class CounterPort(Protocol):
    def add(self, amount: int, attributes: Attributes | None = None) -> None: ...


@runtime_checkable
class MeterPort(Protocol):
    def create_counter(self, name: str, unit: str | None = None, description: str | None = None) -> CounterPort: ...


@runtime_checkable
class MeterProviderPort(Protocol):
    def get_meter(self, name: str, version: str | None = None) -> MeterPort: ...

--- src\tycherion\ports\observability\metrics.py:END ---

--- src\tycherion\ports\observability\observability.py:START ---
from __future__ import annotations

from typing import Protocol, runtime_checkable

from .logs import LoggerProviderPort
from .metrics import MeterProviderPort
from .traces import TracerProviderPort


@runtime_checkable
class ObservabilityPort(Protocol):
    @property
    def traces(self) -> TracerProviderPort: ...

    @property
    def logs(self) -> LoggerProviderPort: ...

    @property
    def metrics(self) -> MeterProviderPort: ...

    def shutdown(self) -> None: ...
    def force_flush(self) -> None: ...

--- src\tycherion\ports\observability\observability.py:END ---

--- src\tycherion\ports\observability\traces.py:START ---
from __future__ import annotations

from contextlib import AbstractContextManager
from typing import Protocol, runtime_checkable

from .types import Attributes


@runtime_checkable
class SpanPort(Protocol):
    def set_attribute(self, key: str, value: object) -> None: ...
    def set_attributes(self, attributes: Attributes) -> None: ...
    def add_event(self, name: str, attributes: Attributes | None = None) -> None: ...
    def record_exception(self, exc: BaseException) -> None: ...
    def set_status_ok(self) -> None: ...
    def set_status_error(self, message: str | None = None) -> None: ...
    def is_recording(self) -> bool: ...


@runtime_checkable
class TracerPort(Protocol):
    def start_as_current_span(
        self, name: str, attributes: Attributes | None = None
    ) -> AbstractContextManager[SpanPort]:
        ...


@runtime_checkable
class TracerProviderPort(Protocol):
    def get_tracer(self, name: str, version: str | None = None) -> TracerPort: ...

--- src\tycherion\ports\observability\traces.py:END ---

--- src\tycherion\ports\observability\types.py:START ---
from __future__ import annotations

from enum import Enum
from typing import Mapping, Sequence, Union

# OpenTelemetry attribute values are limited to primitives and sequences of primitives.
AttributePrimitive = Union[bool, str, int, float]
AttributeValue = Union[AttributePrimitive, Sequence[AttributePrimitive]]
Attributes = Mapping[str, AttributeValue]

# NOTE:
# Tycherion schema version for observability payloads. Keep stable and explicit.
TYCHERION_SCHEMA_VERSION = "v3"


class Severity(str, Enum):
    TRACE = "TRACE"
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARN = "WARN"
    ERROR = "ERROR"
    FATAL = "FATAL"

    def to_logging_level(self) -> int:
        import logging

        return {
            Severity.TRACE: 5,  # custom level (below DEBUG)
            Severity.DEBUG: logging.DEBUG,
            Severity.INFO: logging.INFO,
            Severity.WARN: logging.WARNING,
            Severity.ERROR: logging.ERROR,
            Severity.FATAL: logging.CRITICAL,
        }[self]


class SpanStatus(str, Enum):
    UNSET = "UNSET"
    OK = "OK"
    ERROR = "ERROR"

--- src\tycherion\ports\observability\types.py:END ---

--- src\tycherion\ports\observability\__init__.py:START ---

--- src\tycherion\ports\observability\__init__.py:END ---

--- src\tycherion\shared\config.py:START ---
from __future__ import annotations
from pydantic import BaseModel, field_validator
from typing import Optional, Any
import os, yaml
from dotenv import load_dotenv

class Trading(BaseModel):
    dry_run: bool = True
    require_demo: bool = True
    deviation_points: int = 10
    volume_mode: str = "min"     # 'min' | 'fixed'
    fixed_volume: float = 0.01

class Risk(BaseModel):
    risk_per_trade_pct: float = 0.5
    max_daily_loss_pct: float = 2.0

class MT5(BaseModel):
    terminal_path: Optional[str] = None
    server: Optional[str] = None
    login: Optional[int] = None
    password: Optional[str] = None

class RunMode(BaseModel):
    name: str = "live_multimodel"

class ScheduleCfg(BaseModel):
    run_forever: bool = False
    interval_seconds: int = 60

class CoverageCfg(BaseModel):
    source: str = "market_watch"
    symbols: list[str] = []
    pattern: str | None = None


class PipelineStageCfg(BaseModel):
    """Configuration of a single stage in the model pipeline."""

    name: str
    drop_threshold: float | None = None


class ModelsCfg(BaseModel):
    """Application-level model selection.

    `pipeline` defines an ordered list of models to run per symbol. The order
    is the order of execution. Each stage can optionally define a
    `drop_threshold` used to discard non-held symbols early.
    """

    pipeline: list[PipelineStageCfg] = []

    @field_validator("pipeline", mode="before")
    @classmethod
    def _coerce_pipeline(cls, v: Any):
        # Accept both:
        # - pipeline: ["trend_following", "mean_reversion"]
        # - pipeline: [{name: "...", drop_threshold: ...}, ...]
        if v is None:
            return []
        if isinstance(v, list):
            out: list[Any] = []
            for item in v:
                if isinstance(item, str):
                    out.append({"name": item})
                else:
                    out.append(item)
            return out
        return v


class PortfolioCfg(BaseModel):
    allocator: str = "proportional"     # plugin name
    balancer: str = "threshold"         # plugin name
    threshold_weight: float = 0.25      # only rebalance if |w| >= threshold

class ApplicationCfg(BaseModel):
    run_mode: RunMode = RunMode()
    playbook: str = "default"
    schedule: ScheduleCfg = ScheduleCfg()
    coverage: CoverageCfg = CoverageCfg()
    models: ModelsCfg = ModelsCfg()
    portfolio: PortfolioCfg = PortfolioCfg()


class TelemetrySinkCfg(BaseModel):
    enabled: bool = True
    channels: list[str] = ["audit", "ops"]
    min_level: str = "INFO"  # DEBUG/INFO/WARN/ERROR


class TelemetryCfg(BaseModel):
    """Telemetry configuration (bootstrap/application concern, not domain).

    Notes about the two databases we plan to have in Tycherion:
    - PostgreSQL: for *analytics* and structured datasets (models, indicators, actions, etc.).
      The schema for this is not defined yet.
    - MongoDB: for *operational health / audit journal* (telemetry events, errors, spans, etc.).

    Right now this config focuses on telemetry sinks (journal-like append-only storage).
    """

    # PostgreSQL execution journal (optional)
    db_enabled: bool = True
    # PostgreSQL DSN, e.g. "postgresql://user:pass@host:5432/dbname"
    db_dsn: Optional[str] = None
    db_channels: list[str] = ["audit", "ops"]
    db_min_level: str = "INFO"
    db_batch_size: int = 50

    # MongoDB execution journal (optional)
    mongo_enabled: bool = False
    # Mongo URI, e.g. "mongodb://user:pass@host:27017/?authSource=admin"
    mongo_uri: Optional[str] = None
    mongo_db: str = "tycherion"
    mongo_collection: str = "ops_journal"
    mongo_channels: list[str] = ["audit", "ops"]
    mongo_min_level: str = "INFO"
    mongo_batch_size: int = 200

    # Console sink
    console_enabled: bool = False
    console_channels: list[str] = ["ops"]
    console_min_level: str = "INFO"

    # OTLP export (optional; prepared for Alloy/Collector fan-out)
    otlp_enabled: bool = False
    otlp_endpoint: str = "http://localhost:4317"



class AppConfig(BaseModel):
    timeframe: str
    lookback_days: int
    trading: Trading = Trading()
    risk: Risk = Risk()
    mt5: MT5 = MT5()
    application: ApplicationCfg = ApplicationCfg()
    telemetry: TelemetryCfg = TelemetryCfg()

def load_config(path: str) -> AppConfig:
    load_dotenv(override=False)
    import pathlib
    p = pathlib.Path(path)
    if not p.exists():
        raise FileNotFoundError(f"Config not found: {path}")
    with open(path, "r", encoding="utf-8") as f:
        raw = yaml.safe_load(f) or {}
    raw.setdefault("mt5", {})
    mt5_cfg = raw["mt5"] or {}

    def coalesce(yaml_val, env_val):
        return env_val if (yaml_val in (None, "", 0) and env_val not in (None, "")) else yaml_val

    env_terminal = os.getenv("MT5_TERMINAL_PATH")
    env_server   = os.getenv("MT5_SERVER")
    env_login    = os.getenv("MT5_LOGIN")
    env_pass     = os.getenv("MT5_PASSWORD")

    mt5_cfg["terminal_path"] = coalesce(mt5_cfg.get("terminal_path"), env_terminal)
    mt5_cfg["server"]        = coalesce(mt5_cfg.get("server"),        env_server)
    mt5_cfg["login"]         = coalesce(mt5_cfg.get("login"),         int(env_login) if env_login and env_login.isdigit() else None)
    mt5_cfg["password"]      = coalesce(mt5_cfg.get("password"),      env_pass)


    # Observability/Telemetry env overrides (kept here to avoid leaking infra details into domain/application)
    raw.setdefault("telemetry", {})
    tel_cfg = raw["telemetry"] or {}

    def env_bool(name: str) -> bool | None:
        v = os.getenv(name)
        if v is None:
            return None
        v = str(v).strip().lower()
        if v in ("1", "true", "yes", "y", "on"):
            return True
        if v in ("0", "false", "no", "n", "off"):
            return False
        return None

    def env_override(yaml_val, env_val):
        return env_val if env_val is not None else yaml_val

    tel_cfg["otlp_enabled"] = env_override(tel_cfg.get("otlp_enabled"), env_bool("TYCHERION_OTLP_ENABLED"))
    tel_cfg["otlp_endpoint"] = env_override(tel_cfg.get("otlp_endpoint"), os.getenv("TYCHERION_OTLP_ENDPOINT"))

    # Mongo ops journal (audit). Env keys are intentionally explicit.
    tel_cfg["mongo_enabled"] = env_override(tel_cfg.get("mongo_enabled"), env_bool("TYCHERION_MONGO_AUDIT_ENABLED"))
    tel_cfg["mongo_uri"] = env_override(tel_cfg.get("mongo_uri"), os.getenv("TYCHERION_MONGO_URI"))
    tel_cfg["mongo_db"] = env_override(tel_cfg.get("mongo_db"), os.getenv("TYCHERION_MONGO_DB"))
    tel_cfg["mongo_collection"] = env_override(tel_cfg.get("mongo_collection"), os.getenv("TYCHERION_MONGO_COLLECTION"))

    # Console output for local dev
    tel_cfg["console_enabled"] = env_override(tel_cfg.get("console_enabled"), env_bool("TYCHERION_CONSOLE_ENABLED"))
    tel_cfg["console_min_level"] = env_override(tel_cfg.get("console_min_level"), os.getenv("TYCHERION_CONSOLE_MIN_LEVEL"))

    raw["telemetry"] = tel_cfg

    raw["mt5"] = mt5_cfg
    return AppConfig.model_validate(raw)

--- src\tycherion\shared\config.py:END ---

--- src\tycherion\shared\decorators.py:START ---
from __future__ import annotations
from functools import wraps
import logging
import MetaTrader5 as mt5

_log = logging.getLogger(__name__)

def demo_only(fn):
    @wraps(fn)
    def wrapper(self, *args, **kwargs):
        require = getattr(self, "require_demo", True)
        if require:
            ai = mt5.account_info()
            if not ai or ai.trade_mode != mt5.ACCOUNT_TRADE_MODE_DEMO:
                raise RuntimeError("Blocked: only allowed in DEMO account.")
        return fn(self, *args, **kwargs)
    return wrapper

def logged(fn):
    @wraps(fn)
    def wrapper(*args, **kwargs):
        name = fn.__qualname__
        try:
            res = fn(*args, **kwargs)
            _log.debug("%s: ok -> %s", name, res)
            return res
        except Exception as e:
            _log.exception("%s: error", name)
            raise
    return wrapper

--- src\tycherion\shared\decorators.py:END ---

--- tests\test_telemetry.py:START ---
from __future__ import annotations

import unittest
from datetime import datetime, timezone
import pathlib
import sys

ROOT = pathlib.Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT / "src"))

import pandas as pd

from tycherion.application.pipeline.config import PipelineConfig, PipelineStageConfig
from tycherion.application.pipeline.service import ModelPipelineService
from tycherion.application.telemetry import TelemetryHub, TelemetryProvider
from tycherion.adapters.telemetry.memory import InMemoryTelemetrySink
from tycherion.domain.portfolio.entities import PortfolioSnapshot
from tycherion.domain.signals.entities import ModelDecision
from tycherion.domain.signals.models.base import SignalModel
from tycherion.ports.telemetry import TelemetryLevel


class TestTelemetryHub(unittest.TestCase):
    def test_enabled_any_sink_accepts(self) -> None:
        sink = InMemoryTelemetrySink(
            enabled_flag=True,
            channels={"debug"},
            min_level=TelemetryLevel.DEBUG,
        )
        hub = TelemetryHub(sinks=[sink])

        self.assertTrue(hub.enabled("debug", "DEBUG"))
        self.assertFalse(hub.enabled("audit", "INFO"))


class TestTraceTelemetryIds(unittest.TestCase):
    def test_event_seq_monotonic_and_span_hierarchy(self) -> None:
        sink = InMemoryTelemetrySink(
            enabled_flag=True,
            channels={"ops", "audit"},
            min_level=TelemetryLevel.INFO,
        )
        hub = TelemetryHub(sinks=[sink])
        provider = TelemetryProvider(runner_id="test-runner", hub=hub)

        t = provider.new_trace(base_attributes={"component": "test"})

        with t.span("outer", channel="ops", level="INFO"):
            with t.span("inner", channel="ops", level="INFO"):
                t.emit(name="hello", channel="ops", level="INFO", data={"x": 1})

        self.assertGreaterEqual(len(sink.events), 3)

        # event_seq should be 1..N (no gaps) for emitted events in this trace
        seqs = [e.event_seq for e in sink.events]
        self.assertEqual(seqs, list(range(1, len(seqs) + 1)))

        # span ids should be 16-hex chars and inner span parent should be outer span
        outer_started = next(e for e in sink.events if e.name == "outer.started")
        inner_started = next(e for e in sink.events if e.name == "inner.started")

        self.assertIsNotNone(outer_started.span_id)
        self.assertIsNotNone(inner_started.span_id)
        self.assertEqual(len(str(outer_started.span_id)), 16)
        self.assertEqual(len(str(inner_started.span_id)), 16)

        self.assertEqual(inner_started.parent_span_id, outer_started.span_id)


class _DummyMarketData:
    def get_bars(self, symbol: str, timeframe: str, start: datetime, end: datetime) -> pd.DataFrame:
        _ = (symbol, timeframe, start, end)
        return pd.DataFrame(
            {
                "time": [datetime(2020, 1, 1, tzinfo=timezone.utc)],
                "open": [1.0],
                "high": [1.0],
                "low": [1.0],
                "close": [1.0],
                "tick_volume": [1],
            }
        )


class _DummyModel(SignalModel):
    def requires(self) -> set[str]:
        return set()

    def decide(self, indicators):  # type: ignore[override]
        _ = indicators
        return ModelDecision(side="BUY", weight=0.5, confidence=0.1)


class TestDebugGating(unittest.TestCase):
    def test_debug_events_not_emitted_when_debug_disabled(self) -> None:
        sink = InMemoryTelemetrySink(
            enabled_flag=True,
            channels={"audit", "ops"},
            min_level=TelemetryLevel.INFO,
        )
        hub = TelemetryHub(sinks=[sink])
        provider = TelemetryProvider(runner_id="test-runner", hub=hub)

        svc = ModelPipelineService(
            market_data=_DummyMarketData(),
            model_registry={"dummy": _DummyModel()},
            indicator_picker=lambda key, pb: None,  # type: ignore[return-value]
            timeframe="D1",
            lookback_days=10,
            playbook=None,
        )

        pipeline_config = PipelineConfig(stages=[PipelineStageConfig(name="dummy", drop_threshold=None)])
        portfolio = PortfolioSnapshot(equity=1000.0, positions={})

        tracer = provider.new_trace(base_attributes={"component": "test"})
        svc.run(
            universe_symbols=["AAA"],
            portfolio_snapshot=portfolio,
            pipeline_config=pipeline_config,
            tracer=tracer,
        )

        self.assertTrue(len(sink.events) > 0)
        self.assertEqual(0, sum(1 for e in sink.events if e.channel == "debug"))


if __name__ == "__main__":
    unittest.main()

--- tests\test_telemetry.py:END ---
